source,source_name,target,target_name,source_affiliation,target_affiliation,publications_xid,publications_title,publications_abtract
ur.012303706555.15,"Kale, Rahul W.",ur.010747712675.93,"Zhang, Justin Zuopeng",UNF,UNF,['pub.1143170828'],['Agility and Resilience in Information Systems Research'],"['<p>This research analyzes how the concepts of agility and resilience are treated in IS literature. While agility has been an object of study in IS research for many decades, resilience is a fairly new topic. Both are gaining strategic importance in a firm’s sustainability and growth, especially given the remarkable changes in the landscape that the firm operates in. It is thus important to understand what agility and resilience mean in IS research. Our paper is a first attempt to study IS literature and provide a thematic analysis of facets of each concept. In doing so, we identify aspects that are common to both agility and resilience and those that are unique to each. The results of this study can be used for an empirical examination of the two constructs and a validation of how they can be measured in firms. IS researchers and industrial practitioners can benefit from a deeper understanding of agility and resilience.</p>']"
ur.012303706555.15,"Kale, Rahul W.",ur.011416214620.82,"Goel, Lakshmi",UNF,UNF,['pub.1143170828'],['Agility and Resilience in Information Systems Research'],"['<p>This research analyzes how the concepts of agility and resilience are treated in IS literature. While agility has been an object of study in IS research for many decades, resilience is a fairly new topic. Both are gaining strategic importance in a firm’s sustainability and growth, especially given the remarkable changes in the landscape that the firm operates in. It is thus important to understand what agility and resilience mean in IS research. Our paper is a first attempt to study IS literature and provide a thematic analysis of facets of each concept. In doing so, we identify aspects that are common to both agility and resilience and those that are unique to each. The results of this study can be used for an empirical examination of the two constructs and a validation of how they can be measured in firms. IS researchers and industrial practitioners can benefit from a deeper understanding of agility and resilience.</p>']"
ur.010747712675.93,"Zhang, Justin Zuopeng",ur.012303706555.15,"Kale, Rahul W.",UNF,UNF,['pub.1143170828'],['Agility and Resilience in Information Systems Research'],"['<p>This research analyzes how the concepts of agility and resilience are treated in IS literature. While agility has been an object of study in IS research for many decades, resilience is a fairly new topic. Both are gaining strategic importance in a firm’s sustainability and growth, especially given the remarkable changes in the landscape that the firm operates in. It is thus important to understand what agility and resilience mean in IS research. Our paper is a first attempt to study IS literature and provide a thematic analysis of facets of each concept. In doing so, we identify aspects that are common to both agility and resilience and those that are unique to each. The results of this study can be used for an empirical examination of the two constructs and a validation of how they can be measured in firms. IS researchers and industrial practitioners can benefit from a deeper understanding of agility and resilience.</p>']"
ur.010747712675.93,"Zhang, Justin Zuopeng",ur.011416214620.82,"Goel, Lakshmi",UNF,UNF,"['pub.1143170828', 'pub.1146347117', 'pub.1147029387', 'pub.1151635872']","['Agility and Resilience in Information Systems Research', 'IT assimilation: construct, measurement, and implications in cybersecurity', 'Knowledge acquisition model of mobile payment based on automatic summary technology', 'Work-to-Home Cybersecurity Spillover: Construct Development and Validation']","['<p>This research analyzes how the concepts of agility and resilience are treated in IS literature. While agility has been an object of study in IS research for many decades, resilience is a fairly new topic. Both are gaining strategic importance in a firm’s sustainability and growth, especially given the remarkable changes in the landscape that the firm operates in. It is thus important to understand what agility and resilience mean in IS research. Our paper is a first attempt to study IS literature and provide a thematic analysis of facets of each concept. In doing so, we identify aspects that are common to both agility and resilience and those that are unique to each. The results of this study can be used for an empirical examination of the two constructs and a validation of how they can be measured in firms. IS researchers and industrial practitioners can benefit from a deeper understanding of agility and resilience.</p>', 'Unintentional employee behaviours can be a major cause of security breaches. To mitigate the risk, employees need to habituate their new behaviours, implying a transfer of learning that may occur at ‘work’ to ‘personal’ contexts. Against this backdrop, we examine a new construct – IT Assimilation – grounded in situated learning theory by its definition as the incorporation of enterprise IT into an individual’s IT repertoire. We use a multi-phase methodological approach to conceptualise the construct and develop a validated scale to measure it. Our findings offer valuable insights for researchers and practitioners about transferring IT behaviours, particularly in the cybersecurity context..', 'The risks in mobile payment under Fintech have become an urgent problem to be addressed. This paper develops a research framework of knowledge acquisition and explores how automatic summarization technology helps extract knowledge of mobile payment to help managers and users reduce the financial risks. Specifically, we construct the mobile payment domain thesaurus and propose an automatic summary extraction model that integrates Bi-directional Long Short-Term Memory (BiLSTM), Attention Mechanism, and Reinforcement Learning (RL). The model is then used to extract the summary of mobile payment policy documents for knowledge acquisition. Our proposed model performs better than other basic models in Rouge-2, Rouge-4, and Rouge-SU4 indexes. Our study enriches relevant research in the existing literature, facilitates knowledge acquisition in mobile payment, and helps mobile users and managers reduce financial risks in their operations.', 'The COVID-19 pandemic has forced employees to work from home using enterprise systems outside traditional organizational boundaries. Thus, organizations cannot enforce security policies for safe IT use behaviors but depend on continuance intentions of work policies at home. We study the transfer of IT security behaviors from work to home by developing the construct of Work-to-Home Cybersecurity Spillover and empirically testing and validating it. Our findings provide practical implications for researchers and practitioners in cybersecurity.']"
ur.010747712675.93,"Zhang, Justin Zuopeng",ur.016646474730.58,"Williamson, Steven",UNF,UNF,"['pub.1146347117', 'pub.1151635872']","['IT assimilation: construct, measurement, and implications in cybersecurity', 'Work-to-Home Cybersecurity Spillover: Construct Development and Validation']","['Unintentional employee behaviours can be a major cause of security breaches. To mitigate the risk, employees need to habituate their new behaviours, implying a transfer of learning that may occur at ‘work’ to ‘personal’ contexts. Against this backdrop, we examine a new construct – IT Assimilation – grounded in situated learning theory by its definition as the incorporation of enterprise IT into an individual’s IT repertoire. We use a multi-phase methodological approach to conceptualise the construct and develop a validated scale to measure it. Our findings offer valuable insights for researchers and practitioners about transferring IT behaviours, particularly in the cybersecurity context..', 'The COVID-19 pandemic has forced employees to work from home using enterprise systems outside traditional organizational boundaries. Thus, organizations cannot enforce security policies for safe IT use behaviors but depend on continuance intentions of work policies at home. We study the transfer of IT security behaviors from work to home by developing the construct of Work-to-Home Cybersecurity Spillover and empirically testing and validating it. Our findings provide practical implications for researchers and practitioners in cybersecurity.']"
ur.011416214620.82,"Goel, Lakshmi",ur.012303706555.15,"Kale, Rahul W.",UNF,UNF,['pub.1143170828'],['Agility and Resilience in Information Systems Research'],"['<p>This research analyzes how the concepts of agility and resilience are treated in IS literature. While agility has been an object of study in IS research for many decades, resilience is a fairly new topic. Both are gaining strategic importance in a firm’s sustainability and growth, especially given the remarkable changes in the landscape that the firm operates in. It is thus important to understand what agility and resilience mean in IS research. Our paper is a first attempt to study IS literature and provide a thematic analysis of facets of each concept. In doing so, we identify aspects that are common to both agility and resilience and those that are unique to each. The results of this study can be used for an empirical examination of the two constructs and a validation of how they can be measured in firms. IS researchers and industrial practitioners can benefit from a deeper understanding of agility and resilience.</p>']"
ur.011416214620.82,"Goel, Lakshmi",ur.010747712675.93,"Zhang, Justin Zuopeng",UNF,UNF,"['pub.1143170828', 'pub.1146347117', 'pub.1147029387', 'pub.1151635872']","['Agility and Resilience in Information Systems Research', 'IT assimilation: construct, measurement, and implications in cybersecurity', 'Knowledge acquisition model of mobile payment based on automatic summary technology', 'Work-to-Home Cybersecurity Spillover: Construct Development and Validation']","['<p>This research analyzes how the concepts of agility and resilience are treated in IS literature. While agility has been an object of study in IS research for many decades, resilience is a fairly new topic. Both are gaining strategic importance in a firm’s sustainability and growth, especially given the remarkable changes in the landscape that the firm operates in. It is thus important to understand what agility and resilience mean in IS research. Our paper is a first attempt to study IS literature and provide a thematic analysis of facets of each concept. In doing so, we identify aspects that are common to both agility and resilience and those that are unique to each. The results of this study can be used for an empirical examination of the two constructs and a validation of how they can be measured in firms. IS researchers and industrial practitioners can benefit from a deeper understanding of agility and resilience.</p>', 'Unintentional employee behaviours can be a major cause of security breaches. To mitigate the risk, employees need to habituate their new behaviours, implying a transfer of learning that may occur at ‘work’ to ‘personal’ contexts. Against this backdrop, we examine a new construct – IT Assimilation – grounded in situated learning theory by its definition as the incorporation of enterprise IT into an individual’s IT repertoire. We use a multi-phase methodological approach to conceptualise the construct and develop a validated scale to measure it. Our findings offer valuable insights for researchers and practitioners about transferring IT behaviours, particularly in the cybersecurity context..', 'The risks in mobile payment under Fintech have become an urgent problem to be addressed. This paper develops a research framework of knowledge acquisition and explores how automatic summarization technology helps extract knowledge of mobile payment to help managers and users reduce the financial risks. Specifically, we construct the mobile payment domain thesaurus and propose an automatic summary extraction model that integrates Bi-directional Long Short-Term Memory (BiLSTM), Attention Mechanism, and Reinforcement Learning (RL). The model is then used to extract the summary of mobile payment policy documents for knowledge acquisition. Our proposed model performs better than other basic models in Rouge-2, Rouge-4, and Rouge-SU4 indexes. Our study enriches relevant research in the existing literature, facilitates knowledge acquisition in mobile payment, and helps mobile users and managers reduce financial risks in their operations.', 'The COVID-19 pandemic has forced employees to work from home using enterprise systems outside traditional organizational boundaries. Thus, organizations cannot enforce security policies for safe IT use behaviors but depend on continuance intentions of work policies at home. We study the transfer of IT security behaviors from work to home by developing the construct of Work-to-Home Cybersecurity Spillover and empirically testing and validating it. Our findings provide practical implications for researchers and practitioners in cybersecurity.']"
ur.011416214620.82,"Goel, Lakshmi",ur.016646474730.58,"Williamson, Steven",UNF,UNF,"['pub.1146347117', 'pub.1151635872']","['IT assimilation: construct, measurement, and implications in cybersecurity', 'Work-to-Home Cybersecurity Spillover: Construct Development and Validation']","['Unintentional employee behaviours can be a major cause of security breaches. To mitigate the risk, employees need to habituate their new behaviours, implying a transfer of learning that may occur at ‘work’ to ‘personal’ contexts. Against this backdrop, we examine a new construct – IT Assimilation – grounded in situated learning theory by its definition as the incorporation of enterprise IT into an individual’s IT repertoire. We use a multi-phase methodological approach to conceptualise the construct and develop a validated scale to measure it. Our findings offer valuable insights for researchers and practitioners about transferring IT behaviours, particularly in the cybersecurity context..', 'The COVID-19 pandemic has forced employees to work from home using enterprise systems outside traditional organizational boundaries. Thus, organizations cannot enforce security policies for safe IT use behaviors but depend on continuance intentions of work policies at home. We study the transfer of IT security behaviors from work to home by developing the construct of Work-to-Home Cybersecurity Spillover and empirically testing and validating it. Our findings provide practical implications for researchers and practitioners in cybersecurity.']"
ur.015114235656.26,"Franco, Jose A.",ur.013761230703.59,"Dumitru, Raluca",UNF,UNF,['pub.1143463753'],['Geodesic in-betweenness for means of several matrices'],"['A matrix mean σ is said to satisfy the in-betweenness property with respect to the metric d if for any pair of positive definite matrices A and B, d ( A , A σ B ) ≤ d ( A , B ) . In this article, we introduce the geodesic in-betweenness property for k-tuples of positive definite matrices with respect to any metric. Moreover, we show that in spaces of non-positive curvature, geodesic in-betweenness implies in-betweenness in the case of two matrices. We then show some examples of metrics and means for which this property is satisfied.']"
ur.013761230703.59,"Dumitru, Raluca",ur.015114235656.26,"Franco, Jose A.",UNF,UNF,['pub.1143463753'],['Geodesic in-betweenness for means of several matrices'],"['A matrix mean σ is said to satisfy the in-betweenness property with respect to the metric d if for any pair of positive definite matrices A and B, d ( A , A σ B ) ≤ d ( A , B ) . In this article, we introduce the geodesic in-betweenness property for k-tuples of positive definite matrices with respect to any metric. Moreover, we show that in spaces of non-positive curvature, geodesic in-betweenness implies in-betweenness in the case of two matrices. We then show some examples of metrics and means for which this property is satisfied.']"
ur.010207033721.05,"Heng, Fei",ur.01276621531.69,"Gilg, MR",UNF,UNF,['pub.1151970373'],['Acclimation to elevated temperatures in Acropora cervicornis: effects of host genotype and symbiont shuffling'],"[' Climate change is increasing the average surface temperatures of tropical waters, creating unfavorable conditions for corals. Some species of coral can physiologically acclimate to elevated temperatures, but the degree to which genetic variation underlies differences in this ability is currently unknown. Acclimation to elevated temperatures in coral has been hypothesized to be due to either alterations in the symbiont community or to changes in gene expression. The present study investigated the ability of Acropora cervicornis to acclimate to elevated temperatures, estimated the heritability of plasticity in upper thermal tolerance, and tested whether observed acclimation patterns could be explained by symbiont shuffling. Coral fragments from a nursery in the Florida Keys (USA) were acclimated at either ambient (27 ± 1°C) or elevated (30 ± 1°C) temperatures and then exposed to a second heat stress (32 ± 1°C) and monitored for mortality. Fragments acclimated to elevated temperatures showed significantly longer lifespans in the subsequent heat stress than did those acclimated at ambient temperature. The ability to acclimate to elevated temperatures differed significantly among coral genets, yielding low, but significant, estimates of broad-sense heritability. A subsequent experiment revealed no changes in either bacterial or dinoflagellate communities of symbionts as a result of acclimation, suggesting that symbiont shuffling did not account for the differences in lifespan between treatments. While estimates of heritability were low, the results suggest that plasticity in upper thermal tolerance significantly differs among coral genets, and that acclimation is likely a result of alterations in gene expression as opposed to symbiont shuffling. ']"
ur.012203646316.37,"Brown, D T",ur.010610705316.99,"Bryant, C T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.012203646316.37,"Brown, D T",ur.01205676640.74,"Pekarek, T M",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.012203646316.37,"Brown, D T",ur.015655350752.42,"Tavera, R Marquez",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.012203646316.37,"Brown, D T",ur.0623042577.72,"Warusawithana, M P",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.012203646316.37,"Brown, D T",ur.010013324716.10,"Payne, J A",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010610705316.99,"Bryant, C T",ur.012203646316.37,"Brown, D T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010610705316.99,"Bryant, C T",ur.01205676640.74,"Pekarek, T M",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010610705316.99,"Bryant, C T",ur.015655350752.42,"Tavera, R Marquez",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010610705316.99,"Bryant, C T",ur.0623042577.72,"Warusawithana, M P",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010610705316.99,"Bryant, C T",ur.010013324716.10,"Payne, J A",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.01205676640.74,"Pekarek, T M",ur.012203646316.37,"Brown, D T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.01205676640.74,"Pekarek, T M",ur.010610705316.99,"Bryant, C T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.01205676640.74,"Pekarek, T M",ur.015655350752.42,"Tavera, R Marquez",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.01205676640.74,"Pekarek, T M",ur.0623042577.72,"Warusawithana, M P",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.01205676640.74,"Pekarek, T M",ur.010013324716.10,"Payne, J A",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.015655350752.42,"Tavera, R Marquez",ur.012203646316.37,"Brown, D T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.015655350752.42,"Tavera, R Marquez",ur.010610705316.99,"Bryant, C T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.015655350752.42,"Tavera, R Marquez",ur.01205676640.74,"Pekarek, T M",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.015655350752.42,"Tavera, R Marquez",ur.0623042577.72,"Warusawithana, M P",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.015655350752.42,"Tavera, R Marquez",ur.010013324716.10,"Payne, J A",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.0623042577.72,"Warusawithana, M P",ur.012203646316.37,"Brown, D T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.0623042577.72,"Warusawithana, M P",ur.010610705316.99,"Bryant, C T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.0623042577.72,"Warusawithana, M P",ur.01205676640.74,"Pekarek, T M",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.0623042577.72,"Warusawithana, M P",ur.015655350752.42,"Tavera, R Marquez",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.0623042577.72,"Warusawithana, M P",ur.010013324716.10,"Payne, J A",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010013324716.10,"Payne, J A",ur.012203646316.37,"Brown, D T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010013324716.10,"Payne, J A",ur.010610705316.99,"Bryant, C T",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010013324716.10,"Payne, J A",ur.01205676640.74,"Pekarek, T M",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010013324716.10,"Payne, J A",ur.015655350752.42,"Tavera, R Marquez",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.010013324716.10,"Payne, J A",ur.0623042577.72,"Warusawithana, M P",UNF,UNF,['pub.1144322876'],['Doping dependent electronic and magnetic ordering in mixed-valent La1−x Sr x MnO3 thin films'],"['We have investigated the collective electronic and magnetic orderings of a series of La1−x Sr x MnO3 thin films grown epitaxially strained to (001) oriented strontium titanate substrates as a function of doping, x, for 0 ≤ x ≤ 0.4. We find that the ground states of these crystalline thin films are, in general, consistent with that observed in bulk crystals and thin film samples synthesized under a multitude of techniques. Our systematic study, however, reveal subtle features in the temperature dependent electronic transport and magnetization measurements, which presumably arise due to Jahn-Teller type distortions in the lattice for particular doping levels. For the parent compound LaMnO3 (x = 0), we report evidence of a strain-induced ferromagnetic ordering in contrast to the antiferromagnetic ground state found in bulk crystals.']"
ur.016543173506.26,"Smith, Hailey",ur.01302000265.19,"Hamadi, Hanadi Y.",UNF,UNF,['pub.1144643035'],['Striking a Balance between Work and Play: The Effects of Work–Life Interference and Burnout on Faculty Turnover Intentions and Career Satisfaction'],"[""BACKGROUND: The interactions between work and personal life are important for ensuring well-being, especially during COVID-19 where the lines between work and home are blurred. Work-life interference/imbalance can result in work-related burnout, which has been shown to have negative effects on faculty members' physical and psychological health. Although our understanding of burnout has advanced considerably in recent years, little is known about the effects of burnout on nursing faculty turnover intentions and career satisfaction.\r\nOBJECTIVE: To test a hypothesized model examining the effects of work-life interference on nursing faculty burnout (emotional exhaustion and cynicism), turnover intentions and, ultimately, career satisfaction.\r\nDESIGN: A predictive cross-sectional design was used.\r\nSETTINGS: An online national survey of nursing faculty members was administered throughout Canada in summer 2021.\r\nPARTICIPANTS: Nursing faculty who held full-time or part-time positions in Canadian academic settings were invited via email to participate in the study.\r\nMETHODS: Data were collected from an anonymous survey housed on Qualtrics. Descriptive statistics and reliability estimates were computed. The hypothesized model was tested using structural equation modeling.\r\nRESULTS: Data suggest that work-life interference significantly increases burnout which contributes to both higher turnover intentions and lower career satisfaction. Turnover intentions, in turn, decrease career satisfaction.\r\nCONCLUSIONS: The findings add to the growing body of literature linking burnout to turnover and dissatisfaction, highlighting key antecedents and/or drivers of burnout among nurse academics. These results provide suggestions for suitable areas for the development of interventions and policies within the organizational structure to reduce the risk of burnout during and post-COVID-19 and improve faculty retention.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.016543173506.26,"Smith, Hailey",UNF,UNF,['pub.1144643035'],['Striking a Balance between Work and Play: The Effects of Work–Life Interference and Burnout on Faculty Turnover Intentions and Career Satisfaction'],"[""BACKGROUND: The interactions between work and personal life are important for ensuring well-being, especially during COVID-19 where the lines between work and home are blurred. Work-life interference/imbalance can result in work-related burnout, which has been shown to have negative effects on faculty members' physical and psychological health. Although our understanding of burnout has advanced considerably in recent years, little is known about the effects of burnout on nursing faculty turnover intentions and career satisfaction.\r\nOBJECTIVE: To test a hypothesized model examining the effects of work-life interference on nursing faculty burnout (emotional exhaustion and cynicism), turnover intentions and, ultimately, career satisfaction.\r\nDESIGN: A predictive cross-sectional design was used.\r\nSETTINGS: An online national survey of nursing faculty members was administered throughout Canada in summer 2021.\r\nPARTICIPANTS: Nursing faculty who held full-time or part-time positions in Canadian academic settings were invited via email to participate in the study.\r\nMETHODS: Data were collected from an anonymous survey housed on Qualtrics. Descriptive statistics and reliability estimates were computed. The hypothesized model was tested using structural equation modeling.\r\nRESULTS: Data suggest that work-life interference significantly increases burnout which contributes to both higher turnover intentions and lower career satisfaction. Turnover intentions, in turn, decrease career satisfaction.\r\nCONCLUSIONS: The findings add to the growing body of literature linking burnout to turnover and dissatisfaction, highlighting key antecedents and/or drivers of burnout among nurse academics. These results provide suggestions for suitable areas for the development of interventions and policies within the organizational structure to reduce the risk of burnout during and post-COVID-19 and improve faculty retention.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.01160234637.90,"Zhao, Mei",UNF,UNF,"['pub.1145209160', 'pub.1148271696', 'pub.1151302311', 'pub.1154029201', 'pub.1163109713']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'Single-Vendor Electronic Health Record Use Is Associated With Greater Opportunities for Organizational and Clinical Care Improvements', 'The role of community‐level characteristics in comparing United States hospital performance by magnet designation: A propensity score matched study', 'COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'Objective: To compare how hospitals that use single-vendor vs best-of-breed electronic health record (EHR) vendors utilize clinical and organizational evaluation capabilities.\r\nMethods: Data from the 2018 (June 1, 2016, to December 31, 2017) American Hospital Association Information Technology Supplement Survey and Medicare Final Rule Standardizing File were used. Multinomial logistic regression analysis of hospitals (n=1902) was conducted to identify hospital characteristics associated with the use of EHRs for (1) clinical care evaluation capabilities and (2) organizational evaluation capabilities.\r\nResults: Single-vendor EHR hospitals were more likely (relative risk ratio, 3.37; 95% confidence interval, 1.97-5.76) to use EHRs for clinical care and organizational evaluation capabilities. Not-for-profit hospitals were more likely to use EHRs for all organizational evaluation capabilities than government nonfederal hospitals. For-profit hospitals were less likely to use EHRs for organizational or clinical evaluation capabilities than government nonfederal hospitals.\r\nConclusion: Hospitals using the single-vendor EHR system were more likely to engage in clinical care and organizational evaluation than hospitals using best-of-breed EHR systems.', ""AIMS: To assess the impact of community-level characteristics on the role of magnet designation in relation to hospital value-based purchasing quality scores, as health disparities associated with geographical location could confound hospitals' ability to meet outcome metrics.\r\nDESIGN: This cross-sectional study was carried out between October 2021 and March 2022 using data from 2016 to 2021.\r\nMETHODS: Propensity score analysis was used to match hospital and community-level characteristics, implementing nearest neighbour matching to adjust for pre-treatment differences between magnet and non-magnet hospitals to account for multi-level differences. Secondary data were obtained from all operational acute-care facilities in the United States that participated in the Centers for Medicare and Medicaid Services' hospital value-based purchasing (HVBP) program. Dependent variables were the four value-based purchasing domains that comprise the Total Performance Score (TPS; Clinical Care, Person and Community Engagement, Safety, and Efficiency and Cost Reduction).\r\nRESULTS: Magnet hospitals had increased odds for better scores in the HVBP domains of Clinical Care and Person and Community Engagement, and decreased odds for having better Safety. However, no statistically significant difference was found for the Efficiency domain or the TPS.\r\nCONCLUSION: Measuring performance equitably across organizations of various sizes serving diverse communities remains a key factor in ensuring distributive justice. Analysing the TPS components can identify complex influences of community-level characteristics not evident at the composite level. More research is needed where community and nurse-level factors may indirectly affect patient safety.\r\nIMPACT: This study's findings on the role of community contexts can inform policymakers designing value-based care programs and healthcare management administrators deliberating on magnet certification investments across diverse community settings.\r\nNO PATIENT OR PUBLIC CONTRIBUTION: For this study of US hospitals' organizational performance, we did not engage members of the patient population nor the general public. However, the multi-disciplinary research team does include diverse perspectives."", 'The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.016425135625.29,"Xu, Jing",UNF,UNF,"['pub.1145209160', 'pub.1146570632', 'pub.1150155453', 'pub.1150841335', 'pub.1154029201', 'pub.1163109713']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'Impact of Change in Allocation Score Methodology on Post Kidney Transplant Average Length of Stay', ""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?"", 'Examining Healthcare Professionals’ Telehealth Usability before and during COVID-19 in Saudi Arabia: A Cross-Sectional Study', 'COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'Background: In December 2014, a new Kidney Allocation System (KAS) was implemented nationwide to improve access and quality of care to historically disadvantaged patients. However, no study to date has examined the relationship between the KAS and potential changes in hospital length of stay (LOS). This study aimed to examine the relationship between the KAS implemented in December 2014 and potential changes in hospital LOS.\r\nMethods: We used data from the Florida Agency for Health Care Administration on kidney transplant surgeries completed between 2011 and 2018. A cross-sectional cohort study design included seven hospitals that performed kidney transplants for the duration of the study. A propensity score matching approach was used to examine the relationship between KAS and LOS. All acute general medical and surgical hospitals in Florida that performed kidney transplant surgery were included in the analysis.\r\nResults: We included 7,795 patients, 6,119 discharged to home, and 1,676 discharged to home with home health services after transplant. The average LOS prior to KAS was 6.52 days and 6.08 days post KAS. Propensity matched results show that patients transferred to home experienced a decrease in the LOS (coefficient (β) = -0.68; 95% confidence interval (CI): -0.95, -0.42) after the new allocation score was implemented. Similarly, patients transferred to home with home health experienced a decrease in the LOS (β = -1.90; 95% CI: -2.69, -1.11) after the new allocation was implemented.\r\nConclusion: In conclusion, results indicate that KAS implementation did not add a burden on the health system by increasing LOS when considering patients with similar characteristics before and after KAS implementation. KAS is an important policy change that appears to not negatively affect the LOS when sicker patients could receive a kidney transplant. Our findings improve our understanding of the KAS policy and its influence on the health system.', ""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains."", ""COVID-19 has placed substantial stress on healthcare providers in Saudi Arabia as they struggle to avoid contracting the virus, provide continued care for their patients, and protect their own families at home from possible exposure. The demand for care has increased due to the need to treat COVID-19. This pandemic has created a surge in the need for care in select healthcare delivery specialties, forcing other nonurgent or elective care to halt or transition to telehealth. This study provides a timely description of how COVID-19 affected employment, telehealth usage, and interprofessional collaboration. The STROBE checklist was used. We developed a cross-sectional online survey design that is rooted and grounded in the Technology Acceptance Model (TAM). The TAM model allows us to identify characteristics that affect the use of telehealth technologies. The survey was deployed in November 2021 to local healthcare providers in Saudi Arabia. There were 66 individuals in the final sample. Both interprofessional satisfaction on frequency and quality were positively correlated with the frequency of interactions. The odds for satisfaction of frequency and quality were about 12 times (OR = 12.27) and 8 times 110 (OR = 8.24) more, respectively, for the participants with more than three times of interaction than the participants with no interaction at all. We also found that change in telehealth usage during the pandemic was positively associated with the Telehealth Usability Questionnaire (TUQ) scores. The estimated score for the participants who reported an increase in telehealth usage was 5.37, while the scores were lower for the participants reporting 'no change' and 'decreased usage'. Additional training on telehealth use and integration to improve interprofessionalism is needed."", 'The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.010254173030.88,"Park, Sinyoung",UNF,UNF,['pub.1145209160'],['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach'],"[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.013004000573.89,"Martinez, Dayana",UNF,UNF,['pub.1146570632'],['Impact of Change in Allocation Score Methodology on Post Kidney Transplant Average Length of Stay'],"['Background: In December 2014, a new Kidney Allocation System (KAS) was implemented nationwide to improve access and quality of care to historically disadvantaged patients. However, no study to date has examined the relationship between the KAS and potential changes in hospital length of stay (LOS). This study aimed to examine the relationship between the KAS implemented in December 2014 and potential changes in hospital LOS.\r\nMethods: We used data from the Florida Agency for Health Care Administration on kidney transplant surgeries completed between 2011 and 2018. A cross-sectional cohort study design included seven hospitals that performed kidney transplants for the duration of the study. A propensity score matching approach was used to examine the relationship between KAS and LOS. All acute general medical and surgical hospitals in Florida that performed kidney transplant surgery were included in the analysis.\r\nResults: We included 7,795 patients, 6,119 discharged to home, and 1,676 discharged to home with home health services after transplant. The average LOS prior to KAS was 6.52 days and 6.08 days post KAS. Propensity matched results show that patients transferred to home experienced a decrease in the LOS (coefficient (β) = -0.68; 95% confidence interval (CI): -0.95, -0.42) after the new allocation score was implemented. Similarly, patients transferred to home with home health experienced a decrease in the LOS (β = -1.90; 95% CI: -2.69, -1.11) after the new allocation was implemented.\r\nConclusion: In conclusion, results indicate that KAS implementation did not add a burden on the health system by increasing LOS when considering patients with similar characteristics before and after KAS implementation. KAS is an important policy change that appears to not negatively affect the LOS when sicker patients could receive a kidney transplant. Our findings improve our understanding of the KAS policy and its influence on the health system.']"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.014472453705.75,"Quinn, Nathan",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.0640163054.49,"Merten, Julie W",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.011732133420.33,"Terrell, Kassie R",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.012156464142.80,"LeBlanc, Kelly",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.015332726033.17,"Walker, Krystal",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.010763032160.65,"Marsh, Tamara",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.01025441731.01,"Wright, Lauri",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.015043716424.49,"Van Horn, Leslie Thompson",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.01213347266.19,"Sealey‐Potts, Claudia",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.01273574321.58,"Mainous, Arch G",UNF,UF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.016254065343.32,"Xie, Zhigang",UNF,UNF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.012241605737.25,"Hong, Young-Rock",UNF,UF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.01302000265.19,"Hamadi, Hanadi Y.",ur.01241754627.12,"Haley, D. Rob",UNF,UNF,"['pub.1154029201', 'pub.1163109713']","['COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","['The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.01017745776.03,"Barnes, Amber N.",ur.010134425154.29,"Jenkins, Meg",UNF,UNF,['pub.1144701423'],"['A Systematic Review of Zoonotic Enteric Parasites Carried by Flies, Cockroaches, and Dung Beetles']","['Filth flies, cockroaches, and dung beetles have been close neighbors with humans and animals throughout our joint histories. However, these insects can also serve as vectors for many zoonotic enteric parasites (ZEPs). Zoonoses by ZEPs remain a paramount public health threat due to our close contact with animals, combined with poor water, sanitation, and hygiene access, services, and behaviors in many global regions. Our objective in this systematic review was to determine which ZEPs have been documented in these vectors, to identify risk factors associated with their transmission, and to provide effectual One Health recommendations for curbing their spread. Using PRISMA guidelines, a total of 85 articles published from 1926 to 2021 were reviewed and included in this study. Qualitative analysis revealed that the most common parasites associated with these insects included, but were not limited to: <i>Ascaris</i> spp., <i>Trichuris</i> spp., <i>Entamoeba</i> spp., and <i>Cryptosporidium</i> spp. Additionally, prominent risk factors discovered in the review, such as poor household and community WASH services, unsafe food handling, and exposure to domestic animals and wildlife, significantly increase parasitic transmission and zoonoses. The risk of insect vector transmission in our shared environments makes it critically important to implement a One Health approach in reducing ZEP transmission.']"
ur.010134425154.29,"Jenkins, Meg",ur.01017745776.03,"Barnes, Amber N.",UNF,UNF,['pub.1144701423'],"['A Systematic Review of Zoonotic Enteric Parasites Carried by Flies, Cockroaches, and Dung Beetles']","['Filth flies, cockroaches, and dung beetles have been close neighbors with humans and animals throughout our joint histories. However, these insects can also serve as vectors for many zoonotic enteric parasites (ZEPs). Zoonoses by ZEPs remain a paramount public health threat due to our close contact with animals, combined with poor water, sanitation, and hygiene access, services, and behaviors in many global regions. Our objective in this systematic review was to determine which ZEPs have been documented in these vectors, to identify risk factors associated with their transmission, and to provide effectual One Health recommendations for curbing their spread. Using PRISMA guidelines, a total of 85 articles published from 1926 to 2021 were reviewed and included in this study. Qualitative analysis revealed that the most common parasites associated with these insects included, but were not limited to: <i>Ascaris</i> spp., <i>Trichuris</i> spp., <i>Entamoeba</i> spp., and <i>Cryptosporidium</i> spp. Additionally, prominent risk factors discovered in the review, such as poor household and community WASH services, unsafe food handling, and exposure to domestic animals and wildlife, significantly increase parasitic transmission and zoonoses. The risk of insect vector transmission in our shared environments makes it critically important to implement a One Health approach in reducing ZEP transmission.']"
ur.01365551077.18,"Churilla, James R",ur.01337755147.18,"Johnson, Tammie M.",UNF,UNF,"['pub.1144857579', 'pub.1146012123']","['Mode of Physical Activity Participation in US Adults: A Regional Perspective', 'Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis']","['OBJECTIVE: Examine the mode of physical activity (PA) participation in United States adults by US Census region.\r\nMETHODS: The study sample (N = 323,435) included adult (18 years of age and older) participants from the 2019 Behavioral Risk Factor Surveillance System. Participants reported meeting both aerobic and muscle strengthening activity (MSA) guidelines, the aerobic-only guideline, the MSA only guideline, or neither.\r\nRESULTS: The greatest prevalence estimate of meeting the mutually exclusive aerobic and MSA guideline was found in the West Census Region (24.3, 95% confidence interval 23.6-24.9) and the greatest prevalence estimate of meeting neither aerobic nor MSA guideline was found in the South Census Region (38.1%, 95% confidence interval 37.5-38.7). Physical and mental health were found to be positively associated with PA and non-Hispanic Blacks and Hispanics reported the greatest levels of meeting neither federal PA recommendation.\r\nCONCLUSIONS: These data suggest that mode of PA participation varies by demographics and census region in US adults. State and local health departments should communicate between and within regions and disseminate information to raise awareness of the health benefits of meeting the federal PA guidelines.', nan]"
ur.01365551077.18,"Churilla, James R",ur.01253770701.51,"Richardson, Michael R.",UNF,UNF,"['pub.1144857579', 'pub.1146012123', 'pub.1149596712', 'pub.1150782593', 'pub.1150787329']","['Mode of Physical Activity Participation in US Adults: A Regional Perspective', 'Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis', 'Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Sedentary Time And Prescription Medication Use In United States Adults: 2017-2018 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['OBJECTIVE: Examine the mode of physical activity (PA) participation in United States adults by US Census region.\r\nMETHODS: The study sample (N = 323,435) included adult (18 years of age and older) participants from the 2019 Behavioral Risk Factor Surveillance System. Participants reported meeting both aerobic and muscle strengthening activity (MSA) guidelines, the aerobic-only guideline, the MSA only guideline, or neither.\r\nRESULTS: The greatest prevalence estimate of meeting the mutually exclusive aerobic and MSA guideline was found in the West Census Region (24.3, 95% confidence interval 23.6-24.9) and the greatest prevalence estimate of meeting neither aerobic nor MSA guideline was found in the South Census Region (38.1%, 95% confidence interval 37.5-38.7). Physical and mental health were found to be positively associated with PA and non-Hispanic Blacks and Hispanics reported the greatest levels of meeting neither federal PA recommendation.\r\nCONCLUSIONS: These data suggest that mode of PA participation varies by demographics and census region in US adults. State and local health departments should communicate between and within regions and disseminate information to raise awareness of the health benefits of meeting the federal PA guidelines.', nan, 'PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan, nan]"
ur.01365551077.18,"Churilla, James R",ur.016477543151.92,"Cosentino, Ralph G.",UNF,UNF,['pub.1146012123'],['Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis'],[nan]
ur.01365551077.18,"Churilla, James R",ur.016706625772.77,"Stapleton, Jessica N",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.01365551077.18,"Churilla, James R",ur.011027471255.87,"Zipperer, Madeline B",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.01365551077.18,"Churilla, James R",ur.015307320476.16,"Boyne, Ciarra A.",UNF,UNF,['pub.1150782593'],['Sedentary Time And Prescription Medication Use In United States Adults: 2017-2018 NHANES'],[nan]
ur.010420777413.09,"Beasley, Berrin",ur.016211170323.08,"Parmelee, John H.",UNF,UNF,['pub.1144757339'],['Personalization of politicians on Instagram: what Generation Z wants to see in political posts'],"['While research has examined how politicians engage in personalized presentations of themselves and their policies, and how journalists personalize political coverage, few studies have asked voters what types of personalization they wish to see from politicians. In-depth interviews with Generation Z, who heavily use Instagram, identify visual and verbal elements of politicians’ Instagram posts that they consider influential on their beliefs and actions. Responses reveal several elements, including two that expand the contours of what constitutes persuasive political personalization. Successful personalization includes politicians participating in two-way conversations with followers and adopting a backstage pass aesthetic when showing what politicians do on the job and how they feel about it. Both tactics can make politicians seem credible and personally relatable, which is the foundation for effective electronic word-of-mouth communication.']"
ur.010420777413.09,"Beasley, Berrin",ur.015421253027.94,"Perkins, Stephynie C.",UNF,UNF,['pub.1144757339'],['Personalization of politicians on Instagram: what Generation Z wants to see in political posts'],"['While research has examined how politicians engage in personalized presentations of themselves and their policies, and how journalists personalize political coverage, few studies have asked voters what types of personalization they wish to see from politicians. In-depth interviews with Generation Z, who heavily use Instagram, identify visual and verbal elements of politicians’ Instagram posts that they consider influential on their beliefs and actions. Responses reveal several elements, including two that expand the contours of what constitutes persuasive political personalization. Successful personalization includes politicians participating in two-way conversations with followers and adopting a backstage pass aesthetic when showing what politicians do on the job and how they feel about it. Both tactics can make politicians seem credible and personally relatable, which is the foundation for effective electronic word-of-mouth communication.']"
ur.016211170323.08,"Parmelee, John H.",ur.010420777413.09,"Beasley, Berrin",UNF,UNF,['pub.1144757339'],['Personalization of politicians on Instagram: what Generation Z wants to see in political posts'],"['While research has examined how politicians engage in personalized presentations of themselves and their policies, and how journalists personalize political coverage, few studies have asked voters what types of personalization they wish to see from politicians. In-depth interviews with Generation Z, who heavily use Instagram, identify visual and verbal elements of politicians’ Instagram posts that they consider influential on their beliefs and actions. Responses reveal several elements, including two that expand the contours of what constitutes persuasive political personalization. Successful personalization includes politicians participating in two-way conversations with followers and adopting a backstage pass aesthetic when showing what politicians do on the job and how they feel about it. Both tactics can make politicians seem credible and personally relatable, which is the foundation for effective electronic word-of-mouth communication.']"
ur.016211170323.08,"Parmelee, John H.",ur.015421253027.94,"Perkins, Stephynie C.",UNF,UNF,['pub.1144757339'],['Personalization of politicians on Instagram: what Generation Z wants to see in political posts'],"['While research has examined how politicians engage in personalized presentations of themselves and their policies, and how journalists personalize political coverage, few studies have asked voters what types of personalization they wish to see from politicians. In-depth interviews with Generation Z, who heavily use Instagram, identify visual and verbal elements of politicians’ Instagram posts that they consider influential on their beliefs and actions. Responses reveal several elements, including two that expand the contours of what constitutes persuasive political personalization. Successful personalization includes politicians participating in two-way conversations with followers and adopting a backstage pass aesthetic when showing what politicians do on the job and how they feel about it. Both tactics can make politicians seem credible and personally relatable, which is the foundation for effective electronic word-of-mouth communication.']"
ur.015421253027.94,"Perkins, Stephynie C.",ur.010420777413.09,"Beasley, Berrin",UNF,UNF,['pub.1144757339'],['Personalization of politicians on Instagram: what Generation Z wants to see in political posts'],"['While research has examined how politicians engage in personalized presentations of themselves and their policies, and how journalists personalize political coverage, few studies have asked voters what types of personalization they wish to see from politicians. In-depth interviews with Generation Z, who heavily use Instagram, identify visual and verbal elements of politicians’ Instagram posts that they consider influential on their beliefs and actions. Responses reveal several elements, including two that expand the contours of what constitutes persuasive political personalization. Successful personalization includes politicians participating in two-way conversations with followers and adopting a backstage pass aesthetic when showing what politicians do on the job and how they feel about it. Both tactics can make politicians seem credible and personally relatable, which is the foundation for effective electronic word-of-mouth communication.']"
ur.015421253027.94,"Perkins, Stephynie C.",ur.016211170323.08,"Parmelee, John H.",UNF,UNF,['pub.1144757339'],['Personalization of politicians on Instagram: what Generation Z wants to see in political posts'],"['While research has examined how politicians engage in personalized presentations of themselves and their policies, and how journalists personalize political coverage, few studies have asked voters what types of personalization they wish to see from politicians. In-depth interviews with Generation Z, who heavily use Instagram, identify visual and verbal elements of politicians’ Instagram posts that they consider influential on their beliefs and actions. Responses reveal several elements, including two that expand the contours of what constitutes persuasive political personalization. Successful personalization includes politicians participating in two-way conversations with followers and adopting a backstage pass aesthetic when showing what politicians do on the job and how they feel about it. Both tactics can make politicians seem credible and personally relatable, which is the foundation for effective electronic word-of-mouth communication.']"
ur.0644663313.57,"Chatterjee, Chiradip",ur.01164760644.22,"Loh, Chung-Ping A.",UNF,UNF,['pub.1148514912'],"['Consumer perception and information in a model of household water usage: The case of jacksonville, FL']","['Consumption of bottled water in the U.S. continues to grow despite the higher user price and greater environmental cost relative to municipal tap water. Convenience is surely one reason for this trend, but it is less relevant for in-home consumption of bottled water. The existing literature highlights perceptions of quality, access to information and personal experience as important factors influencing water usage in the home. In this paper we report the results of a 2018 survey of water customers of Jacksonville Electric Authority (JEA), the primary municipal water utility in Northeast Florida. The survey includes detailed questions regarding self-reported household water usage, information availability, information processing, trust in institutions and demographic characteristics. In addition, in cooperation with JEA, we matched the survey results with administrative data on geographic location within the system. Using a bivariate probit regression method, we estimate the determinants of water usage in the home. The results show that concern for drinking water safety is the principal contributor of bottled water consumption. Moreover, the evidence illustrates how information from water quality reports and objective measures of water hardness translate into the drinking water choice. We also show that greater transaction costs of bottled water due to low access to retail suppliers is associated with a substitution of water filtration for bottled water.']"
ur.0644663313.57,"Chatterjee, Chiradip",ur.07577325317.14,"Triplett, Russell",UNF,UNF,['pub.1148514912'],"['Consumer perception and information in a model of household water usage: The case of jacksonville, FL']","['Consumption of bottled water in the U.S. continues to grow despite the higher user price and greater environmental cost relative to municipal tap water. Convenience is surely one reason for this trend, but it is less relevant for in-home consumption of bottled water. The existing literature highlights perceptions of quality, access to information and personal experience as important factors influencing water usage in the home. In this paper we report the results of a 2018 survey of water customers of Jacksonville Electric Authority (JEA), the primary municipal water utility in Northeast Florida. The survey includes detailed questions regarding self-reported household water usage, information availability, information processing, trust in institutions and demographic characteristics. In addition, in cooperation with JEA, we matched the survey results with administrative data on geographic location within the system. Using a bivariate probit regression method, we estimate the determinants of water usage in the home. The results show that concern for drinking water safety is the principal contributor of bottled water consumption. Moreover, the evidence illustrates how information from water quality reports and objective measures of water hardness translate into the drinking water choice. We also show that greater transaction costs of bottled water due to low access to retail suppliers is associated with a substitution of water filtration for bottled water.']"
ur.01337755147.18,"Johnson, Tammie M.",ur.01253770701.51,"Richardson, Michael R.",UNF,UNF,"['pub.1144857579', 'pub.1146012123']","['Mode of Physical Activity Participation in US Adults: A Regional Perspective', 'Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis']","['OBJECTIVE: Examine the mode of physical activity (PA) participation in United States adults by US Census region.\r\nMETHODS: The study sample (N = 323,435) included adult (18 years of age and older) participants from the 2019 Behavioral Risk Factor Surveillance System. Participants reported meeting both aerobic and muscle strengthening activity (MSA) guidelines, the aerobic-only guideline, the MSA only guideline, or neither.\r\nRESULTS: The greatest prevalence estimate of meeting the mutually exclusive aerobic and MSA guideline was found in the West Census Region (24.3, 95% confidence interval 23.6-24.9) and the greatest prevalence estimate of meeting neither aerobic nor MSA guideline was found in the South Census Region (38.1%, 95% confidence interval 37.5-38.7). Physical and mental health were found to be positively associated with PA and non-Hispanic Blacks and Hispanics reported the greatest levels of meeting neither federal PA recommendation.\r\nCONCLUSIONS: These data suggest that mode of PA participation varies by demographics and census region in US adults. State and local health departments should communicate between and within regions and disseminate information to raise awareness of the health benefits of meeting the federal PA guidelines.', nan]"
ur.01337755147.18,"Johnson, Tammie M.",ur.01365551077.18,"Churilla, James R.",UNF,UNF,"['pub.1144857579', 'pub.1146012123']","['Mode of Physical Activity Participation in US Adults: A Regional Perspective', 'Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis']","['OBJECTIVE: Examine the mode of physical activity (PA) participation in United States adults by US Census region.\r\nMETHODS: The study sample (N = 323,435) included adult (18 years of age and older) participants from the 2019 Behavioral Risk Factor Surveillance System. Participants reported meeting both aerobic and muscle strengthening activity (MSA) guidelines, the aerobic-only guideline, the MSA only guideline, or neither.\r\nRESULTS: The greatest prevalence estimate of meeting the mutually exclusive aerobic and MSA guideline was found in the West Census Region (24.3, 95% confidence interval 23.6-24.9) and the greatest prevalence estimate of meeting neither aerobic nor MSA guideline was found in the South Census Region (38.1%, 95% confidence interval 37.5-38.7). Physical and mental health were found to be positively associated with PA and non-Hispanic Blacks and Hispanics reported the greatest levels of meeting neither federal PA recommendation.\r\nCONCLUSIONS: These data suggest that mode of PA participation varies by demographics and census region in US adults. State and local health departments should communicate between and within regions and disseminate information to raise awareness of the health benefits of meeting the federal PA guidelines.', nan]"
ur.01337755147.18,"Johnson, Tammie M.",ur.016477543151.92,"Cosentino, Ralph G.",UNF,UNF,['pub.1146012123'],['Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis'],[nan]
ur.01253770701.51,"Richardson, Michael R.",ur.01337755147.18,"Johnson, Tammie M.",UNF,UNF,"['pub.1144857579', 'pub.1146012123']","['Mode of Physical Activity Participation in US Adults: A Regional Perspective', 'Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis']","['OBJECTIVE: Examine the mode of physical activity (PA) participation in United States adults by US Census region.\r\nMETHODS: The study sample (N = 323,435) included adult (18 years of age and older) participants from the 2019 Behavioral Risk Factor Surveillance System. Participants reported meeting both aerobic and muscle strengthening activity (MSA) guidelines, the aerobic-only guideline, the MSA only guideline, or neither.\r\nRESULTS: The greatest prevalence estimate of meeting the mutually exclusive aerobic and MSA guideline was found in the West Census Region (24.3, 95% confidence interval 23.6-24.9) and the greatest prevalence estimate of meeting neither aerobic nor MSA guideline was found in the South Census Region (38.1%, 95% confidence interval 37.5-38.7). Physical and mental health were found to be positively associated with PA and non-Hispanic Blacks and Hispanics reported the greatest levels of meeting neither federal PA recommendation.\r\nCONCLUSIONS: These data suggest that mode of PA participation varies by demographics and census region in US adults. State and local health departments should communicate between and within regions and disseminate information to raise awareness of the health benefits of meeting the federal PA guidelines.', nan]"
ur.01253770701.51,"Richardson, Michael R.",ur.01365551077.18,"Churilla, James R.",UNF,UNF,"['pub.1144857579', 'pub.1146012123', 'pub.1149596712', 'pub.1150782593', 'pub.1150787329']","['Mode of Physical Activity Participation in US Adults: A Regional Perspective', 'Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis', 'Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Sedentary Time And Prescription Medication Use In United States Adults: 2017-2018 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['OBJECTIVE: Examine the mode of physical activity (PA) participation in United States adults by US Census region.\r\nMETHODS: The study sample (N = 323,435) included adult (18 years of age and older) participants from the 2019 Behavioral Risk Factor Surveillance System. Participants reported meeting both aerobic and muscle strengthening activity (MSA) guidelines, the aerobic-only guideline, the MSA only guideline, or neither.\r\nRESULTS: The greatest prevalence estimate of meeting the mutually exclusive aerobic and MSA guideline was found in the West Census Region (24.3, 95% confidence interval 23.6-24.9) and the greatest prevalence estimate of meeting neither aerobic nor MSA guideline was found in the South Census Region (38.1%, 95% confidence interval 37.5-38.7). Physical and mental health were found to be positively associated with PA and non-Hispanic Blacks and Hispanics reported the greatest levels of meeting neither federal PA recommendation.\r\nCONCLUSIONS: These data suggest that mode of PA participation varies by demographics and census region in US adults. State and local health departments should communicate between and within regions and disseminate information to raise awareness of the health benefits of meeting the federal PA guidelines.', nan, 'PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan, nan]"
ur.01253770701.51,"Richardson, Michael R.",ur.016477543151.92,"Cosentino, Ralph G.",UNF,UNF,['pub.1146012123'],['Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis'],[nan]
ur.01253770701.51,"Richardson, Michael R.",ur.016706625772.77,"Stapleton, Jessica N",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.01253770701.51,"Richardson, Michael R.",ur.011027471255.87,"Zipperer, Madeline B",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.01253770701.51,"Richardson, Michael R.",ur.015307320476.16,"Boyne, Ciarra A.",UNF,UNF,['pub.1150782593'],['Sedentary Time And Prescription Medication Use In United States Adults: 2017-2018 NHANES'],[nan]
ur.010676073760.64,"Pascale, Amanda Blakewood",ur.011252000536.58,"Hicks‐Roof, Kristen",UNF,UNF,['pub.1152156533'],['Faculty fathers: Understanding the experiences of faculty men with children during the COVID‐19 pandemic'],"['Abstract To effectively cultivate systems of equity, it is important to not only understand the experiences of those who are most affected by inequity but also of those who are in positions of power and privilege. This study explores how faculty men with children navigated the pandemic experience. Two major thematic findings emerged including ‘Not (as) Interrupted’ and ‘“I” feel supported’. These findings suggest that academic fathers may be emerging from the pandemic positioned differently than academic mothers. This research has implications for higher education administrators and faculty in the shaping of practice and policies aimed at equitably supporting faculty post‐pandemic.']"
ur.010676073760.64,"Pascale, Amanda Blakewood",ur.014211702264.47,"Ehrlich, Suzanne",UNF,UNF,['pub.1152156533'],['Faculty fathers: Understanding the experiences of faculty men with children during the COVID‐19 pandemic'],"['Abstract To effectively cultivate systems of equity, it is important to not only understand the experiences of those who are most affected by inequity but also of those who are in positions of power and privilege. This study explores how faculty men with children navigated the pandemic experience. Two major thematic findings emerged including ‘Not (as) Interrupted’ and ‘“I” feel supported’. These findings suggest that academic fathers may be emerging from the pandemic positioned differently than academic mothers. This research has implications for higher education administrators and faculty in the shaping of practice and policies aimed at equitably supporting faculty post‐pandemic.']"
ur.0746107425.44,"Alloway, Tracy Packiam",ur.016003073441.55,"Horton, John C.",UNF,UNF,['pub.1148910234'],"['Time Perspective, Working Memory, and Depression in Non-Clinical Samples: Is There a Link?']","['Non-clinical depression is a major issue on college campuses, with some surveys estimating that 30% of college students have experienced a major depressive episode. One theoretical framework of depression is Zimbardo and Boyd (1999) time perspective model, which posits that our perspectives on time impact different aspects of life including our emotions, judgments, and decision making. The current study seeks to determine the role of this time perspectives model and a range of cognitive constructs including hope, rumination, and working memory on their influence in depression. Currently enrolled college students and participants not currently enrolled in college completed the Center for Epidemiologic Studies Depression Scale, the Zimbardo Time Perspective Inventory, the Adult Hope Scale, the Rumination Reflection Questionnaire, and the Automated Working Memory Assessment. Linear regression analysis revealed that, for the college students, Rumination and Past Negative scores predicted depressive symptoms. For the non-college students, Rumination, Present Fatalism, Hope Agency and Verbal Working Memory scores predicted depressive symptoms. The current results reiterate the importance of rumination in depression symptomology and that current cognitive depression models and treatments may benefit from including time perspective measures. Further implications of the results are discussed.']"
ur.012101457103.65,"Kovalenko, Samantha",ur.016407560042.02,"Brown, Christopher James",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.012101457103.65,"Kovalenko, Samantha",ur.014500566712.21,"Akan, Cigdem",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.012101457103.65,"Kovalenko, Samantha",ur.016662074635.57,"Schonning, Alexandra",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.016407560042.02,"Brown, Christopher James",ur.012101457103.65,"Kovalenko, Samantha",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.016407560042.02,"Brown, Christopher James",ur.014500566712.21,"Akan, Cigdem",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.016407560042.02,"Brown, Christopher James",ur.016662074635.57,"Schonning, Alexandra",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.014500566712.21,"Akan, Cigdem",ur.012101457103.65,"Kovalenko, Samantha",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.014500566712.21,"Akan, Cigdem",ur.016407560042.02,"Brown, Christopher James",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.014500566712.21,"Akan, Cigdem",ur.016662074635.57,"Schonning, Alexandra",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.016662074635.57,"Schonning, Alexandra",ur.012101457103.65,"Kovalenko, Samantha",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.016662074635.57,"Schonning, Alexandra",ur.016407560042.02,"Brown, Christopher James",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.016662074635.57,"Schonning, Alexandra",ur.014500566712.21,"Akan, Cigdem",UNF,UNF,['pub.1145125572'],"['An examination of extreme floods, effects on land-use change and seasonality in the lower St. Johns River Basin, Florida using HSPF and statistical methods']","['As population growth and urbanization are steadily rising, the need for dependable flood estimation techniques is crucial. This study evaluates extreme flood events in select sub-basins of the Lower St. Johns River in Florida, USA. The study considers the effect of urbanization on the natural hydrologic processes and flood magnitudes in the watershed. Additionally, the effects of varying seasonality into the hydrologic modeling procedure are also investigated. This research focuses on determining the 10-, 25-, 50-, and 100-year return frequency flood flows in Julington Creek, Ortega River, and Pablo Creek of the Lower St. Johns River Basin in Florida, USA. The major findings of this research indicate that by implementing a range of flood estimation methods one can better describe the inherent uncertainty with traditional estimates. Also, the research showed that varying seasonality in the hydrologic modeling procedure does not result in vast differences in the resulting flood estimates. However, various land-use scenarios may produce simulated flood flows of greater magnitude—especially when a more urbanized land-use scenario is modeled.']"
ur.01160234637.90,"Zhao, Mei",ur.016425135625.29,"Xu, Jing",UNF,UNF,"['pub.1145209160', 'pub.1154029201', 'pub.1163109713']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.01160234637.90,"Zhao, Mei",ur.01302000265.19,"Hamadi, Hanadi",UNF,UNF,"['pub.1145209160', 'pub.1148271696', 'pub.1151302311', 'pub.1154029201', 'pub.1163109713']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'Single-Vendor Electronic Health Record Use Is Associated With Greater Opportunities for Organizational and Clinical Care Improvements', 'The role of community‐level characteristics in comparing United States hospital performance by magnet designation: A propensity score matched study', 'COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'Objective: To compare how hospitals that use single-vendor vs best-of-breed electronic health record (EHR) vendors utilize clinical and organizational evaluation capabilities.\r\nMethods: Data from the 2018 (June 1, 2016, to December 31, 2017) American Hospital Association Information Technology Supplement Survey and Medicare Final Rule Standardizing File were used. Multinomial logistic regression analysis of hospitals (n=1902) was conducted to identify hospital characteristics associated with the use of EHRs for (1) clinical care evaluation capabilities and (2) organizational evaluation capabilities.\r\nResults: Single-vendor EHR hospitals were more likely (relative risk ratio, 3.37; 95% confidence interval, 1.97-5.76) to use EHRs for clinical care and organizational evaluation capabilities. Not-for-profit hospitals were more likely to use EHRs for all organizational evaluation capabilities than government nonfederal hospitals. For-profit hospitals were less likely to use EHRs for organizational or clinical evaluation capabilities than government nonfederal hospitals.\r\nConclusion: Hospitals using the single-vendor EHR system were more likely to engage in clinical care and organizational evaluation than hospitals using best-of-breed EHR systems.', ""AIMS: To assess the impact of community-level characteristics on the role of magnet designation in relation to hospital value-based purchasing quality scores, as health disparities associated with geographical location could confound hospitals' ability to meet outcome metrics.\r\nDESIGN: This cross-sectional study was carried out between October 2021 and March 2022 using data from 2016 to 2021.\r\nMETHODS: Propensity score analysis was used to match hospital and community-level characteristics, implementing nearest neighbour matching to adjust for pre-treatment differences between magnet and non-magnet hospitals to account for multi-level differences. Secondary data were obtained from all operational acute-care facilities in the United States that participated in the Centers for Medicare and Medicaid Services' hospital value-based purchasing (HVBP) program. Dependent variables were the four value-based purchasing domains that comprise the Total Performance Score (TPS; Clinical Care, Person and Community Engagement, Safety, and Efficiency and Cost Reduction).\r\nRESULTS: Magnet hospitals had increased odds for better scores in the HVBP domains of Clinical Care and Person and Community Engagement, and decreased odds for having better Safety. However, no statistically significant difference was found for the Efficiency domain or the TPS.\r\nCONCLUSION: Measuring performance equitably across organizations of various sizes serving diverse communities remains a key factor in ensuring distributive justice. Analysing the TPS components can identify complex influences of community-level characteristics not evident at the composite level. More research is needed where community and nurse-level factors may indirectly affect patient safety.\r\nIMPACT: This study's findings on the role of community contexts can inform policymakers designing value-based care programs and healthcare management administrators deliberating on magnet certification investments across diverse community settings.\r\nNO PATIENT OR PUBLIC CONTRIBUTION: For this study of US hospitals' organizational performance, we did not engage members of the patient population nor the general public. However, the multi-disciplinary research team does include diverse perspectives."", 'The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.01160234637.90,"Zhao, Mei",ur.010254173030.88,"Park, Sinyoung",UNF,UNF,['pub.1145209160'],['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach'],"[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts.""]"
ur.01160234637.90,"Zhao, Mei",ur.01241754627.12,"Haley, D. Rob",UNF,UNF,"['pub.1154029201', 'pub.1163109713']","['COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","['The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.016425135625.29,"Xu, Jing",ur.01160234637.90,"Zhao, Mei",UNF,UNF,"['pub.1145209160', 'pub.1154029201', 'pub.1163109713']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.016425135625.29,"Xu, Jing",ur.01302000265.19,"Hamadi, Hanadi",UNF,UNF,"['pub.1145209160', 'pub.1146570632', 'pub.1150155453', 'pub.1150841335', 'pub.1154029201', 'pub.1163109713']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'Impact of Change in Allocation Score Methodology on Post Kidney Transplant Average Length of Stay', ""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?"", 'Examining Healthcare Professionals’ Telehealth Usability before and during COVID-19 in Saudi Arabia: A Cross-Sectional Study', 'COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'Background: In December 2014, a new Kidney Allocation System (KAS) was implemented nationwide to improve access and quality of care to historically disadvantaged patients. However, no study to date has examined the relationship between the KAS and potential changes in hospital length of stay (LOS). This study aimed to examine the relationship between the KAS implemented in December 2014 and potential changes in hospital LOS.\r\nMethods: We used data from the Florida Agency for Health Care Administration on kidney transplant surgeries completed between 2011 and 2018. A cross-sectional cohort study design included seven hospitals that performed kidney transplants for the duration of the study. A propensity score matching approach was used to examine the relationship between KAS and LOS. All acute general medical and surgical hospitals in Florida that performed kidney transplant surgery were included in the analysis.\r\nResults: We included 7,795 patients, 6,119 discharged to home, and 1,676 discharged to home with home health services after transplant. The average LOS prior to KAS was 6.52 days and 6.08 days post KAS. Propensity matched results show that patients transferred to home experienced a decrease in the LOS (coefficient (β) = -0.68; 95% confidence interval (CI): -0.95, -0.42) after the new allocation score was implemented. Similarly, patients transferred to home with home health experienced a decrease in the LOS (β = -1.90; 95% CI: -2.69, -1.11) after the new allocation was implemented.\r\nConclusion: In conclusion, results indicate that KAS implementation did not add a burden on the health system by increasing LOS when considering patients with similar characteristics before and after KAS implementation. KAS is an important policy change that appears to not negatively affect the LOS when sicker patients could receive a kidney transplant. Our findings improve our understanding of the KAS policy and its influence on the health system.', ""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains."", ""COVID-19 has placed substantial stress on healthcare providers in Saudi Arabia as they struggle to avoid contracting the virus, provide continued care for their patients, and protect their own families at home from possible exposure. The demand for care has increased due to the need to treat COVID-19. This pandemic has created a surge in the need for care in select healthcare delivery specialties, forcing other nonurgent or elective care to halt or transition to telehealth. This study provides a timely description of how COVID-19 affected employment, telehealth usage, and interprofessional collaboration. The STROBE checklist was used. We developed a cross-sectional online survey design that is rooted and grounded in the Technology Acceptance Model (TAM). The TAM model allows us to identify characteristics that affect the use of telehealth technologies. The survey was deployed in November 2021 to local healthcare providers in Saudi Arabia. There were 66 individuals in the final sample. Both interprofessional satisfaction on frequency and quality were positively correlated with the frequency of interactions. The odds for satisfaction of frequency and quality were about 12 times (OR = 12.27) and 8 times 110 (OR = 8.24) more, respectively, for the participants with more than three times of interaction than the participants with no interaction at all. We also found that change in telehealth usage during the pandemic was positively associated with the Telehealth Usability Questionnaire (TUQ) scores. The estimated score for the participants who reported an increase in telehealth usage was 5.37, while the scores were lower for the participants reporting 'no change' and 'decreased usage'. Additional training on telehealth use and integration to improve interprofessionalism is needed."", 'The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.016425135625.29,"Xu, Jing",ur.010254173030.88,"Park, Sinyoung",UNF,UNF,"['pub.1145209160', 'pub.1150570791']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'What Determinants Affect Inpatient Satisfaction in a Post-Acute Care Rehabilitation Hospital?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'OBJECTIVE: To examine how specific hospital service domains (personal issues domain, discharge domain, rehabilitation doctor domain, nursing domain, physical therapist domain, occupational therapist domain, and food domain) influence final patient satisfaction scores, the overall quality of care, and willingness to recommend the hospital to others among patients in an inpatient rehabilitation hospital.\r\nDESIGN: Longitudinal study.\r\nSETTING: Patient-level data from electronic medical records were joined with Press Ganey (www.pressganey.com) satisfaction data for a single post-acute care inpatient rehabilitation facility in northeast Florida.\r\nPARTICIPANTS: Patients who participated in the inpatient rehabilitation survey (N=4,785).\r\nINTERVENTIONS: Not applicable.\r\nMAIN OUTCOME MEASURES: Main outcome measures included final patient satisfaction scores, overall rating of care during the stay, and willingness to recommend the hospital to others.\r\nRESULTS: This study found the personal issues domain to be the most important factor in determining the final patient satisfaction score, overall rating of care, and likelihood to recommend the hospital to others, followed by the physical therapist, nurse, discharge, and food domains (P<.0001). Within the personal issues domain score, staff promptness and explanation upon arrival were areas identified as opportunities to make improvements that would result in the greatest positive effect.\r\nCONCLUSIONS: This work represents novel findings by investigating the major determinants of positive patient experience in a rehabilitation hospital setting. These findings provide actionable information to improve patient experience as well as where to focus improvement efforts using limited resources.']"
ur.016425135625.29,"Xu, Jing",ur.013004000573.89,"Martinez, Dayana",UNF,UNF,['pub.1146570632'],['Impact of Change in Allocation Score Methodology on Post Kidney Transplant Average Length of Stay'],"['Background: In December 2014, a new Kidney Allocation System (KAS) was implemented nationwide to improve access and quality of care to historically disadvantaged patients. However, no study to date has examined the relationship between the KAS and potential changes in hospital length of stay (LOS). This study aimed to examine the relationship between the KAS implemented in December 2014 and potential changes in hospital LOS.\r\nMethods: We used data from the Florida Agency for Health Care Administration on kidney transplant surgeries completed between 2011 and 2018. A cross-sectional cohort study design included seven hospitals that performed kidney transplants for the duration of the study. A propensity score matching approach was used to examine the relationship between KAS and LOS. All acute general medical and surgical hospitals in Florida that performed kidney transplant surgery were included in the analysis.\r\nResults: We included 7,795 patients, 6,119 discharged to home, and 1,676 discharged to home with home health services after transplant. The average LOS prior to KAS was 6.52 days and 6.08 days post KAS. Propensity matched results show that patients transferred to home experienced a decrease in the LOS (coefficient (β) = -0.68; 95% confidence interval (CI): -0.95, -0.42) after the new allocation score was implemented. Similarly, patients transferred to home with home health experienced a decrease in the LOS (β = -1.90; 95% CI: -2.69, -1.11) after the new allocation was implemented.\r\nConclusion: In conclusion, results indicate that KAS implementation did not add a burden on the health system by increasing LOS when considering patients with similar characteristics before and after KAS implementation. KAS is an important policy change that appears to not negatively affect the LOS when sicker patients could receive a kidney transplant. Our findings improve our understanding of the KAS policy and its influence on the health system.']"
ur.016425135625.29,"Xu, Jing",ur.012156464142.80,"LeBlanc, Kelly",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.016425135625.29,"Xu, Jing",ur.015332726033.17,"Walker, Krystal",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.016425135625.29,"Xu, Jing",ur.010763032160.65,"Marsh, Tamara",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.016425135625.29,"Xu, Jing",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.016425135625.29,"Xu, Jing",ur.01241754627.12,"Haley, D. Rob",UNF,UNF,"['pub.1154029201', 'pub.1163109713']","['COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","['The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.010254173030.88,"Park, Sinyoung",ur.01160234637.90,"Zhao, Mei",UNF,UNF,['pub.1145209160'],['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach'],"[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts.""]"
ur.010254173030.88,"Park, Sinyoung",ur.016425135625.29,"Xu, Jing",UNF,UNF,"['pub.1145209160', 'pub.1150570791']","['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach', 'What Determinants Affect Inpatient Satisfaction in a Post-Acute Care Rehabilitation Hospital?']","[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts."", 'OBJECTIVE: To examine how specific hospital service domains (personal issues domain, discharge domain, rehabilitation doctor domain, nursing domain, physical therapist domain, occupational therapist domain, and food domain) influence final patient satisfaction scores, the overall quality of care, and willingness to recommend the hospital to others among patients in an inpatient rehabilitation hospital.\r\nDESIGN: Longitudinal study.\r\nSETTING: Patient-level data from electronic medical records were joined with Press Ganey (www.pressganey.com) satisfaction data for a single post-acute care inpatient rehabilitation facility in northeast Florida.\r\nPARTICIPANTS: Patients who participated in the inpatient rehabilitation survey (N=4,785).\r\nINTERVENTIONS: Not applicable.\r\nMAIN OUTCOME MEASURES: Main outcome measures included final patient satisfaction scores, overall rating of care during the stay, and willingness to recommend the hospital to others.\r\nRESULTS: This study found the personal issues domain to be the most important factor in determining the final patient satisfaction score, overall rating of care, and likelihood to recommend the hospital to others, followed by the physical therapist, nurse, discharge, and food domains (P<.0001). Within the personal issues domain score, staff promptness and explanation upon arrival were areas identified as opportunities to make improvements that would result in the greatest positive effect.\r\nCONCLUSIONS: This work represents novel findings by investigating the major determinants of positive patient experience in a rehabilitation hospital setting. These findings provide actionable information to improve patient experience as well as where to focus improvement efforts using limited resources.']"
ur.010254173030.88,"Park, Sinyoung",ur.01302000265.19,"Hamadi, Hanadi",UNF,UNF,['pub.1145209160'],['Factors Impacting Patients’ Willingness to Recommend: A Structural Equation Modeling Approach'],"[""Patient ratings of inpatient stay have been the focus of prior research since better patient satisfaction results in a financial benefit to hospitals and are associated with better patient health care outcomes. However, studies that simultaneously account for within- and between-hospital effects are uncommon. We constructed a multilevel structural equation model to identify predictors of patients' willingness to recommend a hospital at both within-hospital and between-hospital levels. We used data from 60 U.S. general medical and surgical hospitals and 12,115 patients. Multilevel structural equation modeling reported that patient ratings on the overall quality of care significantly affect the willingness to recommend within hospitals. Also, patients' perspectives on the hospital environment and nursing are the significant factors that predict the patient ratings on the overall quality of care. Overall patient satisfaction significantly predicts the willingness to recommend at the between-hospital level, whereas hospital size and location have marginal impacts.""]"
ur.01012437430.14,"Aceros, Juan",ur.016556614717.31,"Lundy, Mary",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.01012437430.14,"Aceros, Juan",ur.016436340061.73,"Fraser, Abbey M",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.01012437430.14,"Aceros, Juan",ur.014627324770.26,"Bevill, Grant",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.016556614717.31,"Lundy, Mary",ur.01012437430.14,"Aceros, Juan",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.016556614717.31,"Lundy, Mary",ur.016436340061.73,"Fraser, Abbey M",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.016556614717.31,"Lundy, Mary",ur.014627324770.26,"Bevill, Grant",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.016436340061.73,"Fraser, Abbey M",ur.01012437430.14,"Aceros, Juan",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.016436340061.73,"Fraser, Abbey M",ur.016556614717.31,"Lundy, Mary",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.016436340061.73,"Fraser, Abbey M",ur.014627324770.26,"Bevill, Grant",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.014627324770.26,"Bevill, Grant",ur.01012437430.14,"Aceros, Juan",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.014627324770.26,"Bevill, Grant",ur.016556614717.31,"Lundy, Mary",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.014627324770.26,"Bevill, Grant",ur.016436340061.73,"Fraser, Abbey M",UNF,UNF,['pub.1145238857'],['Safety analysis of adapted battery-powered ride-on toy car for children with disabilities using a modified test dummy with varying joint stiffness.'],"['Adaptive ride-on toy programs have increased in popularity in recent years and provide novel rehabilitation tools as developmental aids for children with disabilities. While the adaptations made to these toys are intended to provide a safer experience for children with disabilities, safety concerns still exist. Within this context, the purpose of this study was to use a model with varying joint stiffness as a first-order approximation of a child with disabilities and to investigate whether modifications to ride-on toys are sufficient to prevent common injuries. Because the population of children with disabilities who are receiving adaptive ride-on toys have a wide range of musculoskeletal disorders, those with both decreased and increased muscle stiffness were considered in this safety study. A 5-point harness reduced movement regardless of change in joint stiffness and therefore, results from this study indicate that the use of these harnesses is effective regardless of joint stiffness. Furthermore, as excursion-related injuries are considered more critical to the user than injuries relating to kinetic variables and no known injury thresholds were exceeded, the addition of a belt is considered a necessary trade-off with little-to-no added risk.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.01110613476.83,"Arikawa, Andrea",UNF,UNF,['pub.1146384767'],['Instructor and Student Perceptions of Teacher Empathy in Higher Education'],"['Empathy is recognized as the ability to relate emotionally to an experience or another person’s emotions. Evidence supports the notion that students learn better and have greater positive perceptions when instructors display empathetic values and seek more meaningful relationships with their students. The purpose of this study was to assess instructor self-reported empathy and students’ perceptions of instructor empathy, using the Toronto Empathy Questionnaire (TEQ) and a newly developed Instructor Empathy Practices questionnaire (IEP7), among a convenience sample from a public university. This convergent mixed-methods study consisted of a cross-sectional survey sent to both students and instructors and focus groups conducted with students. A total of 168 students and 68 faculty members completed the survey and 19 students participated in the focus groups. Contextual factors such as race (p = 0.036), classification (p = 0.003) and GPA (p = 0.028) played significant roles in student empathy scores. Among instructors, only total student enrollment (p = 0.028) had an association with teacher empathy. Focus groups revealed themes related to recognizing empathy including understanding its definition, instructional techniques, perception in relation to class motivation, and importance. This study was the first to employ a convergent study design to assess and better characterize empathy from both instructors’ and students’ perspectives.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.014243412173.03,"Ross, Jenifer",UNF,UNF,['pub.1146384767'],['Instructor and Student Perceptions of Teacher Empathy in Higher Education'],"['Empathy is recognized as the ability to relate emotionally to an experience or another person’s emotions. Evidence supports the notion that students learn better and have greater positive perceptions when instructors display empathetic values and seek more meaningful relationships with their students. The purpose of this study was to assess instructor self-reported empathy and students’ perceptions of instructor empathy, using the Toronto Empathy Questionnaire (TEQ) and a newly developed Instructor Empathy Practices questionnaire (IEP7), among a convenience sample from a public university. This convergent mixed-methods study consisted of a cross-sectional survey sent to both students and instructors and focus groups conducted with students. A total of 168 students and 68 faculty members completed the survey and 19 students participated in the focus groups. Contextual factors such as race (p = 0.036), classification (p = 0.003) and GPA (p = 0.028) played significant roles in student empathy scores. Among instructors, only total student enrollment (p = 0.028) had an association with teacher empathy. Focus groups revealed themes related to recognizing empathy including understanding its definition, instructional techniques, perception in relation to class motivation, and importance. This study was the first to employ a convergent study design to assess and better characterize empathy from both instructors’ and students’ perspectives.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.012171662703.42,"Ninya, Leila",UNF,UNF,['pub.1146475296'],['Nutrition and Dietetics Faculty Providing Experiential Learning: Opportunities Amid the Pandemic and Beyond'],[nan]
ur.011252000536.58,"Hicks-Roof, Kristen",ur.012156464142.80,"LeBlanc, Kelly",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.015332726033.17,"Walker, Krystal",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.010763032160.65,"Marsh, Tamara",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.016425135625.29,"Xu, Jing",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.01302000265.19,"Hamadi, Hanadi Y.",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.014472453705.75,"Quinn, N.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.015534510336.20,"Boggs, C.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.011732133420.33,"Terrell, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.07627017536.46,"Rule, M.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.0634752264.04,"Zeglin, R.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.010676073760.64,"Pascale, Amanda Blakewood",UNF,UNF,['pub.1152156533'],['Faculty fathers: Understanding the experiences of faculty men with children during the COVID‐19 pandemic'],"['Abstract To effectively cultivate systems of equity, it is important to not only understand the experiences of those who are most affected by inequity but also of those who are in positions of power and privilege. This study explores how faculty men with children navigated the pandemic experience. Two major thematic findings emerged including ‘Not (as) Interrupted’ and ‘“I” feel supported’. These findings suggest that academic fathers may be emerging from the pandemic positioned differently than academic mothers. This research has implications for higher education administrators and faculty in the shaping of practice and policies aimed at equitably supporting faculty post‐pandemic.']"
ur.011252000536.58,"Hicks-Roof, Kristen",ur.014211702264.47,"Ehrlich, Suzanne",UNF,UNF,['pub.1152156533'],['Faculty fathers: Understanding the experiences of faculty men with children during the COVID‐19 pandemic'],"['Abstract To effectively cultivate systems of equity, it is important to not only understand the experiences of those who are most affected by inequity but also of those who are in positions of power and privilege. This study explores how faculty men with children navigated the pandemic experience. Two major thematic findings emerged including ‘Not (as) Interrupted’ and ‘“I” feel supported’. These findings suggest that academic fathers may be emerging from the pandemic positioned differently than academic mothers. This research has implications for higher education administrators and faculty in the shaping of practice and policies aimed at equitably supporting faculty post‐pandemic.']"
ur.0730275122.15,"Osterbrink, Jürgen",ur.0675707375.80,"PARRY, Selina M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0730275122.15,"Osterbrink, Jürgen",ur.016105651565.67,"EGGMANN, Sabrina",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0730275122.15,"Osterbrink, Jürgen",ur.01016254223.10,"SCHALLER, Stefan J.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0730275122.15,"Osterbrink, Jürgen",ur.01050046744.47,"NYDAHL, Peter",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0730275122.15,"Osterbrink, Jürgen",ur.01263513533.79,"NEEDHAM, Dale M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.014243412173.03,"Ross, Jenifer M.",ur.01025441731.01,"Wright, Lauri",UNF,UNF,"['pub.1145697312', 'pub.1151885993']","['The Impact of a Food Recovery-Meal Delivery Program on Homebound Seniors’ Food Security, Nutrition, and Well-Being', 'Barriers to Implementing Weight Management Recommendations']","[""Food insecurity is a growing problem among seniors. A novel program was established to help mitigate the problem of food insecurity among seniors who are homebound. Volunteers recover unused prepared food donated by area hospitals, repack it into healthy meals which are delivered to program participants. To evaluate the impact of our intervention, seniors' nutritional health and social well-being were measured at enrollment and after three to five months using the following: Mini Nutritional Assessment Short Form (MNA-SF), 24-hour recall, USDA 6-Item Food Security Survey, WHO-5 Well-Being Index, and the 3-Item Loneliness Scale. Statistical analysis indicated a significant improvement in nutritional health, well-being, and loneliness; participants also increased their consumption of protein and calories. Semi-structured interviews were conducted to investigate the self-perceived impact of the program. Thematic analysis of the interviews revealed that meal recipients perceive that food recovery-meal delivery programs may improve their nutrition health, food security, and well-being."", 'Dietitians are responsible for using evidence-based practice to mitigate the effects of obesity; however, it is unclear how dietitians use research to guide weight management interventions. The aim of this pilot study was to identify the barriers of research utilization and implementation of evidence-based practice in adult weight management. A survey was disseminated to dietitians working at least part-time with people with obesity. Dietitians seem to value research and evidence-based practice; however, implementation may be an issue. The pilot study found that workplace setting may provide a barrier to research utilization, but dietitian opinion of current screening and referral guidelines may also be a significant barrier to implementing best practices in adult weight management.']"
ur.014243412173.03,"Ross, Jenifer M.",ur.01110613476.83,"Arikawa, Andrea",UNF,UNF,"['pub.1145697312', 'pub.1146384767', 'pub.1151885993', 'pub.1152002766']","['The Impact of a Food Recovery-Meal Delivery Program on Homebound Seniors’ Food Security, Nutrition, and Well-Being', 'Instructor and Student Perceptions of Teacher Empathy in Higher Education', 'Barriers to Implementing Weight Management Recommendations', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""Food insecurity is a growing problem among seniors. A novel program was established to help mitigate the problem of food insecurity among seniors who are homebound. Volunteers recover unused prepared food donated by area hospitals, repack it into healthy meals which are delivered to program participants. To evaluate the impact of our intervention, seniors' nutritional health and social well-being were measured at enrollment and after three to five months using the following: Mini Nutritional Assessment Short Form (MNA-SF), 24-hour recall, USDA 6-Item Food Security Survey, WHO-5 Well-Being Index, and the 3-Item Loneliness Scale. Statistical analysis indicated a significant improvement in nutritional health, well-being, and loneliness; participants also increased their consumption of protein and calories. Semi-structured interviews were conducted to investigate the self-perceived impact of the program. Thematic analysis of the interviews revealed that meal recipients perceive that food recovery-meal delivery programs may improve their nutrition health, food security, and well-being."", 'Empathy is recognized as the ability to relate emotionally to an experience or another person’s emotions. Evidence supports the notion that students learn better and have greater positive perceptions when instructors display empathetic values and seek more meaningful relationships with their students. The purpose of this study was to assess instructor self-reported empathy and students’ perceptions of instructor empathy, using the Toronto Empathy Questionnaire (TEQ) and a newly developed Instructor Empathy Practices questionnaire (IEP7), among a convenience sample from a public university. This convergent mixed-methods study consisted of a cross-sectional survey sent to both students and instructors and focus groups conducted with students. A total of 168 students and 68 faculty members completed the survey and 19 students participated in the focus groups. Contextual factors such as race (p = 0.036), classification (p = 0.003) and GPA (p = 0.028) played significant roles in student empathy scores. Among instructors, only total student enrollment (p = 0.028) had an association with teacher empathy. Focus groups revealed themes related to recognizing empathy including understanding its definition, instructional techniques, perception in relation to class motivation, and importance. This study was the first to employ a convergent study design to assess and better characterize empathy from both instructors’ and students’ perspectives.', 'Dietitians are responsible for using evidence-based practice to mitigate the effects of obesity; however, it is unclear how dietitians use research to guide weight management interventions. The aim of this pilot study was to identify the barriers of research utilization and implementation of evidence-based practice in adult weight management. A survey was disseminated to dietitians working at least part-time with people with obesity. Dietitians seem to value research and evidence-based practice; however, implementation may be an issue. The pilot study found that workplace setting may provide a barrier to research utilization, but dietitian opinion of current screening and referral guidelines may also be a significant barrier to implementing best practices in adult weight management.', 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.014243412173.03,"Ross, Jenifer M.",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1146384767'],['Instructor and Student Perceptions of Teacher Empathy in Higher Education'],"['Empathy is recognized as the ability to relate emotionally to an experience or another person’s emotions. Evidence supports the notion that students learn better and have greater positive perceptions when instructors display empathetic values and seek more meaningful relationships with their students. The purpose of this study was to assess instructor self-reported empathy and students’ perceptions of instructor empathy, using the Toronto Empathy Questionnaire (TEQ) and a newly developed Instructor Empathy Practices questionnaire (IEP7), among a convenience sample from a public university. This convergent mixed-methods study consisted of a cross-sectional survey sent to both students and instructors and focus groups conducted with students. A total of 168 students and 68 faculty members completed the survey and 19 students participated in the focus groups. Contextual factors such as race (p = 0.036), classification (p = 0.003) and GPA (p = 0.028) played significant roles in student empathy scores. Among instructors, only total student enrollment (p = 0.028) had an association with teacher empathy. Focus groups revealed themes related to recognizing empathy including understanding its definition, instructional techniques, perception in relation to class motivation, and importance. This study was the first to employ a convergent study design to assess and better characterize empathy from both instructors’ and students’ perspectives.']"
ur.014243412173.03,"Ross, Jenifer M.",ur.014613704314.11,"Perez, Doreen",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.014243412173.03,"Ross, Jenifer M.",ur.0636665472.55,"Bednarzyk, Michele",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.014243412173.03,"Ross, Jenifer M.",ur.010776677330.88,"Harris, Michel",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.01025441731.01,"Wright, Lauri",ur.014243412173.03,"Ross, Jenifer M.",UNF,UNF,"['pub.1145697312', 'pub.1151885993']","['The Impact of a Food Recovery-Meal Delivery Program on Homebound Seniors’ Food Security, Nutrition, and Well-Being', 'Barriers to Implementing Weight Management Recommendations']","[""Food insecurity is a growing problem among seniors. A novel program was established to help mitigate the problem of food insecurity among seniors who are homebound. Volunteers recover unused prepared food donated by area hospitals, repack it into healthy meals which are delivered to program participants. To evaluate the impact of our intervention, seniors' nutritional health and social well-being were measured at enrollment and after three to five months using the following: Mini Nutritional Assessment Short Form (MNA-SF), 24-hour recall, USDA 6-Item Food Security Survey, WHO-5 Well-Being Index, and the 3-Item Loneliness Scale. Statistical analysis indicated a significant improvement in nutritional health, well-being, and loneliness; participants also increased their consumption of protein and calories. Semi-structured interviews were conducted to investigate the self-perceived impact of the program. Thematic analysis of the interviews revealed that meal recipients perceive that food recovery-meal delivery programs may improve their nutrition health, food security, and well-being."", 'Dietitians are responsible for using evidence-based practice to mitigate the effects of obesity; however, it is unclear how dietitians use research to guide weight management interventions. The aim of this pilot study was to identify the barriers of research utilization and implementation of evidence-based practice in adult weight management. A survey was disseminated to dietitians working at least part-time with people with obesity. Dietitians seem to value research and evidence-based practice; however, implementation may be an issue. The pilot study found that workplace setting may provide a barrier to research utilization, but dietitian opinion of current screening and referral guidelines may also be a significant barrier to implementing best practices in adult weight management.']"
ur.01025441731.01,"Wright, Lauri",ur.01110613476.83,"Arikawa, Andrea",UNF,UNF,"['pub.1145697312', 'pub.1147495410', 'pub.1151572301', 'pub.1151885993', 'pub.1152312344']","['The Impact of a Food Recovery-Meal Delivery Program on Homebound Seniors’ Food Security, Nutrition, and Well-Being', 'Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists', 'Barriers to Implementing Weight Management Recommendations', 'Food insecurity among older adults living in low- and middle-income countries: a scoping review.']","[""Food insecurity is a growing problem among seniors. A novel program was established to help mitigate the problem of food insecurity among seniors who are homebound. Volunteers recover unused prepared food donated by area hospitals, repack it into healthy meals which are delivered to program participants. To evaluate the impact of our intervention, seniors' nutritional health and social well-being were measured at enrollment and after three to five months using the following: Mini Nutritional Assessment Short Form (MNA-SF), 24-hour recall, USDA 6-Item Food Security Survey, WHO-5 Well-Being Index, and the 3-Item Loneliness Scale. Statistical analysis indicated a significant improvement in nutritional health, well-being, and loneliness; participants also increased their consumption of protein and calories. Semi-structured interviews were conducted to investigate the self-perceived impact of the program. Thematic analysis of the interviews revealed that meal recipients perceive that food recovery-meal delivery programs may improve their nutrition health, food security, and well-being."", ""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree."", 'Dietitians are responsible for using evidence-based practice to mitigate the effects of obesity; however, it is unclear how dietitians use research to guide weight management interventions. The aim of this pilot study was to identify the barriers of research utilization and implementation of evidence-based practice in adult weight management. A survey was disseminated to dietitians working at least part-time with people with obesity. Dietitians seem to value research and evidence-based practice; however, implementation may be an issue. The pilot study found that workplace setting may provide a barrier to research utilization, but dietitian opinion of current screening and referral guidelines may also be a significant barrier to implementing best practices in adult weight management.', 'BACKGROUND AND OBJECTIVES: Food insecurity is considered an increasing public health problem worldwide with adverse effects, especially among older adults. Although the literature related to food insecurity among older adults in low- and middle-income countries (LMIC) is expanding, little is known about existing patterns and knowledge gaps in these settings. This scoping review aims to provide a comprehensive overview of the current research related to food insecurity among older adults in LMIC.\r\nRESEARCH DESIGN AND METHODS: A systematic search was conducted in November 2021 and revised in July 2022 on six databases using terms related to food insecurity and older adults. Data were extracted and the emerging themes from the main findings were summarized using a Social-Ecological Model (SEM).\r\nRESULTS: Forty-one studies met the inclusion criteria. Almost half (48.8%) were published in the last two years and utilized a quantitative approach (n=38). Only one study was conducted in a low-income country. Using the SEM, most studies included in this review focused on addressing the relationship between food insecurity with intrapersonal factors.\r\nDISCUSSION AND IMPLICATIONS: Several gaps in the current literature were identified. There is a lack of longitudinal and qualitative studies available on this topic. Also, only fifteen LMIC were represented in the literature. A critical point in this review is that only a few studies addressed the relationship between food insecurity and the policy/social structure, institutional, community and interpersonal levels. These identified gaps can serve as a guide for future research on this topic.']"
ur.01025441731.01,"Wright, Lauri",ur.015043716424.49,"Van Horn, Leslie Thompson",UNF,UNF,"['pub.1146517805', 'pub.1147495410', 'pub.1151572301', 'pub.1152312344']","['Trends Related to Evidence-Based Dietetic Practice Among Dietitians With Varying Levels of Education and Experience', 'Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists', 'Food insecurity among older adults living in low- and middle-income countries: a scoping review.']","[""This qualitative study evaluates registered dietitian nutritionists' (RDNs) use of evidence-based dietetic practices (EBDPs) by education level and years of experience. Interviews were conducted with bachelor's and master's-trained RDNs (n = 9). The following themes regarding EBDP were identified: (1) resources utilized, (2) personal utilization and perceptions, (3) barriers and areas for improvement, and (4) collaboration. It was noted that master's-trained RDNs were more knowledgeable regarding EBDP resources. While it was reported that EBDP had improved with experience, those further from their didactic training had less awareness of EBDP resources. Continuing education should aim to bridge this gap in knowledge among RDNs."", ""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree."", 'BACKGROUND AND OBJECTIVES: Food insecurity is considered an increasing public health problem worldwide with adverse effects, especially among older adults. Although the literature related to food insecurity among older adults in low- and middle-income countries (LMIC) is expanding, little is known about existing patterns and knowledge gaps in these settings. This scoping review aims to provide a comprehensive overview of the current research related to food insecurity among older adults in LMIC.\r\nRESEARCH DESIGN AND METHODS: A systematic search was conducted in November 2021 and revised in July 2022 on six databases using terms related to food insecurity and older adults. Data were extracted and the emerging themes from the main findings were summarized using a Social-Ecological Model (SEM).\r\nRESULTS: Forty-one studies met the inclusion criteria. Almost half (48.8%) were published in the last two years and utilized a quantitative approach (n=38). Only one study was conducted in a low-income country. Using the SEM, most studies included in this review focused on addressing the relationship between food insecurity with intrapersonal factors.\r\nDISCUSSION AND IMPLICATIONS: Several gaps in the current literature were identified. There is a lack of longitudinal and qualitative studies available on this topic. Also, only fifteen LMIC were represented in the literature. A critical point in this review is that only a few studies addressed the relationship between food insecurity and the policy/social structure, institutional, community and interpersonal levels. These identified gaps can serve as a guide for future research on this topic.']"
ur.01025441731.01,"Wright, Lauri",ur.01213347266.19,"Sealey‐Potts, Claudia",UNF,UNF,"['pub.1147495410', 'pub.1150304586', 'pub.1151572301']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Nutrition Interventions Are Needed for Dementia Informal Caregivers', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", nan, ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01025441731.01,"Wright, Lauri",ur.012121456246.61,"Waterman, A.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.01025441731.01,"Wright, Lauri",ur.010732040045.21,"Labyak, C.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.01025441731.01,"Wright, Lauri",ur.01302000265.19,"Hamadi, Hanadi",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01110613476.83,"Arikawa, Andrea",ur.014243412173.03,"Ross, Jenifer M.",UNF,UNF,"['pub.1145697312', 'pub.1146384767', 'pub.1151885993', 'pub.1152002766']","['The Impact of a Food Recovery-Meal Delivery Program on Homebound Seniors’ Food Security, Nutrition, and Well-Being', 'Instructor and Student Perceptions of Teacher Empathy in Higher Education', 'Barriers to Implementing Weight Management Recommendations', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""Food insecurity is a growing problem among seniors. A novel program was established to help mitigate the problem of food insecurity among seniors who are homebound. Volunteers recover unused prepared food donated by area hospitals, repack it into healthy meals which are delivered to program participants. To evaluate the impact of our intervention, seniors' nutritional health and social well-being were measured at enrollment and after three to five months using the following: Mini Nutritional Assessment Short Form (MNA-SF), 24-hour recall, USDA 6-Item Food Security Survey, WHO-5 Well-Being Index, and the 3-Item Loneliness Scale. Statistical analysis indicated a significant improvement in nutritional health, well-being, and loneliness; participants also increased their consumption of protein and calories. Semi-structured interviews were conducted to investigate the self-perceived impact of the program. Thematic analysis of the interviews revealed that meal recipients perceive that food recovery-meal delivery programs may improve their nutrition health, food security, and well-being."", 'Empathy is recognized as the ability to relate emotionally to an experience or another person’s emotions. Evidence supports the notion that students learn better and have greater positive perceptions when instructors display empathetic values and seek more meaningful relationships with their students. The purpose of this study was to assess instructor self-reported empathy and students’ perceptions of instructor empathy, using the Toronto Empathy Questionnaire (TEQ) and a newly developed Instructor Empathy Practices questionnaire (IEP7), among a convenience sample from a public university. This convergent mixed-methods study consisted of a cross-sectional survey sent to both students and instructors and focus groups conducted with students. A total of 168 students and 68 faculty members completed the survey and 19 students participated in the focus groups. Contextual factors such as race (p = 0.036), classification (p = 0.003) and GPA (p = 0.028) played significant roles in student empathy scores. Among instructors, only total student enrollment (p = 0.028) had an association with teacher empathy. Focus groups revealed themes related to recognizing empathy including understanding its definition, instructional techniques, perception in relation to class motivation, and importance. This study was the first to employ a convergent study design to assess and better characterize empathy from both instructors’ and students’ perspectives.', 'Dietitians are responsible for using evidence-based practice to mitigate the effects of obesity; however, it is unclear how dietitians use research to guide weight management interventions. The aim of this pilot study was to identify the barriers of research utilization and implementation of evidence-based practice in adult weight management. A survey was disseminated to dietitians working at least part-time with people with obesity. Dietitians seem to value research and evidence-based practice; however, implementation may be an issue. The pilot study found that workplace setting may provide a barrier to research utilization, but dietitian opinion of current screening and referral guidelines may also be a significant barrier to implementing best practices in adult weight management.', 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.01110613476.83,"Arikawa, Andrea",ur.01025441731.01,"Wright, Lauri",UNF,UNF,"['pub.1145697312', 'pub.1147495410', 'pub.1151572301', 'pub.1151885993', 'pub.1152312344']","['The Impact of a Food Recovery-Meal Delivery Program on Homebound Seniors’ Food Security, Nutrition, and Well-Being', 'Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists', 'Barriers to Implementing Weight Management Recommendations', 'Food insecurity among older adults living in low- and middle-income countries: a scoping review.']","[""Food insecurity is a growing problem among seniors. A novel program was established to help mitigate the problem of food insecurity among seniors who are homebound. Volunteers recover unused prepared food donated by area hospitals, repack it into healthy meals which are delivered to program participants. To evaluate the impact of our intervention, seniors' nutritional health and social well-being were measured at enrollment and after three to five months using the following: Mini Nutritional Assessment Short Form (MNA-SF), 24-hour recall, USDA 6-Item Food Security Survey, WHO-5 Well-Being Index, and the 3-Item Loneliness Scale. Statistical analysis indicated a significant improvement in nutritional health, well-being, and loneliness; participants also increased their consumption of protein and calories. Semi-structured interviews were conducted to investigate the self-perceived impact of the program. Thematic analysis of the interviews revealed that meal recipients perceive that food recovery-meal delivery programs may improve their nutrition health, food security, and well-being."", ""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree."", 'Dietitians are responsible for using evidence-based practice to mitigate the effects of obesity; however, it is unclear how dietitians use research to guide weight management interventions. The aim of this pilot study was to identify the barriers of research utilization and implementation of evidence-based practice in adult weight management. A survey was disseminated to dietitians working at least part-time with people with obesity. Dietitians seem to value research and evidence-based practice; however, implementation may be an issue. The pilot study found that workplace setting may provide a barrier to research utilization, but dietitian opinion of current screening and referral guidelines may also be a significant barrier to implementing best practices in adult weight management.', 'BACKGROUND AND OBJECTIVES: Food insecurity is considered an increasing public health problem worldwide with adverse effects, especially among older adults. Although the literature related to food insecurity among older adults in low- and middle-income countries (LMIC) is expanding, little is known about existing patterns and knowledge gaps in these settings. This scoping review aims to provide a comprehensive overview of the current research related to food insecurity among older adults in LMIC.\r\nRESEARCH DESIGN AND METHODS: A systematic search was conducted in November 2021 and revised in July 2022 on six databases using terms related to food insecurity and older adults. Data were extracted and the emerging themes from the main findings were summarized using a Social-Ecological Model (SEM).\r\nRESULTS: Forty-one studies met the inclusion criteria. Almost half (48.8%) were published in the last two years and utilized a quantitative approach (n=38). Only one study was conducted in a low-income country. Using the SEM, most studies included in this review focused on addressing the relationship between food insecurity with intrapersonal factors.\r\nDISCUSSION AND IMPLICATIONS: Several gaps in the current literature were identified. There is a lack of longitudinal and qualitative studies available on this topic. Also, only fifteen LMIC were represented in the literature. A critical point in this review is that only a few studies addressed the relationship between food insecurity and the policy/social structure, institutional, community and interpersonal levels. These identified gaps can serve as a guide for future research on this topic.']"
ur.01110613476.83,"Arikawa, Andrea",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1146384767'],['Instructor and Student Perceptions of Teacher Empathy in Higher Education'],"['Empathy is recognized as the ability to relate emotionally to an experience or another person’s emotions. Evidence supports the notion that students learn better and have greater positive perceptions when instructors display empathetic values and seek more meaningful relationships with their students. The purpose of this study was to assess instructor self-reported empathy and students’ perceptions of instructor empathy, using the Toronto Empathy Questionnaire (TEQ) and a newly developed Instructor Empathy Practices questionnaire (IEP7), among a convenience sample from a public university. This convergent mixed-methods study consisted of a cross-sectional survey sent to both students and instructors and focus groups conducted with students. A total of 168 students and 68 faculty members completed the survey and 19 students participated in the focus groups. Contextual factors such as race (p = 0.036), classification (p = 0.003) and GPA (p = 0.028) played significant roles in student empathy scores. Among instructors, only total student enrollment (p = 0.028) had an association with teacher empathy. Focus groups revealed themes related to recognizing empathy including understanding its definition, instructional techniques, perception in relation to class motivation, and importance. This study was the first to employ a convergent study design to assess and better characterize empathy from both instructors’ and students’ perspectives.']"
ur.01110613476.83,"Arikawa, Andrea",ur.015043716424.49,"Van Horn, Leslie Thompson",UNF,UNF,"['pub.1147495410', 'pub.1151572301', 'pub.1152312344']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists', 'Food insecurity among older adults living in low- and middle-income countries: a scoping review.']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree."", 'BACKGROUND AND OBJECTIVES: Food insecurity is considered an increasing public health problem worldwide with adverse effects, especially among older adults. Although the literature related to food insecurity among older adults in low- and middle-income countries (LMIC) is expanding, little is known about existing patterns and knowledge gaps in these settings. This scoping review aims to provide a comprehensive overview of the current research related to food insecurity among older adults in LMIC.\r\nRESEARCH DESIGN AND METHODS: A systematic search was conducted in November 2021 and revised in July 2022 on six databases using terms related to food insecurity and older adults. Data were extracted and the emerging themes from the main findings were summarized using a Social-Ecological Model (SEM).\r\nRESULTS: Forty-one studies met the inclusion criteria. Almost half (48.8%) were published in the last two years and utilized a quantitative approach (n=38). Only one study was conducted in a low-income country. Using the SEM, most studies included in this review focused on addressing the relationship between food insecurity with intrapersonal factors.\r\nDISCUSSION AND IMPLICATIONS: Several gaps in the current literature were identified. There is a lack of longitudinal and qualitative studies available on this topic. Also, only fifteen LMIC were represented in the literature. A critical point in this review is that only a few studies addressed the relationship between food insecurity and the policy/social structure, institutional, community and interpersonal levels. These identified gaps can serve as a guide for future research on this topic.']"
ur.01110613476.83,"Arikawa, Andrea",ur.01213347266.19,"Sealey‐Potts, Claudia",UNF,UNF,"['pub.1147495410', 'pub.1151572301']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01110613476.83,"Arikawa, Andrea",ur.01302000265.19,"Hamadi, Hanadi",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01110613476.83,"Arikawa, Andrea",ur.013621327420.63,"Galena, Amy E.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01110613476.83,"Arikawa, Andrea",ur.01134664434.25,"Jahan-Mihan, Alireza",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01110613476.83,"Arikawa, Andrea",ur.014613704314.11,"Perez, Doreen",UNF,UNF,"['pub.1151600442', 'pub.1152002766']","['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis."", 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.01110613476.83,"Arikawa, Andrea",ur.0636665472.55,"Bednarzyk, Michele",UNF,UNF,"['pub.1151600442', 'pub.1152002766']","['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis."", 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.01110613476.83,"Arikawa, Andrea",ur.01226756046.26,"Ochrietor, Judith D.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01110613476.83,"Arikawa, Andrea",ur.010776677330.88,"Harris, Michel",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.013357234557.87,"Dedrick, Ashley",ur.0640163054.49,"Merten, Julie",UNF,UNF,['pub.1145770394'],['Content Analysis of Skin Cancer Screenings on Pinterest: An Exploratory Study'],"['Skin cancer rates are rising in the United States, yet screening rates remain low. Meanwhile, social media has evolved to become a primary source of health information, with 40% of daily users of Pinterest reporting the platform as a ""go-to"" source. The objective of this research paper is to examine how skin cancer screenings were portrayed on Pinterest. Using the search terms ""skin cancer screening"" and ""skin cancer exam"", researchers sampled every fifth pin to collect 274 relevant pins. Two researchers coded the pins, and interrater agreement was established at 94%. The results showed that twenty-two percent of the sample depicted skin cancer screening in a negative way, yet 41.5% noted that early detection leads to better outcomes. The pins were geared toward younger, white women with minimal depiction of people of color. Few pins included comprehensive information about skin cancer risk factors, importance of routine self-screenings, or what to expect with a medical provider. Fifty-eight percent of pins included links to personal blogs. In conclusion, social media has become a powerful source of health information, yet much of the posted information is incomplete. These findings present public health experts with an opportunity to disseminate more comprehensive skin cancer screening information on social media.']"
ur.0640163054.49,"Merten, Julie",ur.013357234557.87,"Dedrick, Ashley",UNF,UNF,['pub.1145770394'],['Content Analysis of Skin Cancer Screenings on Pinterest: An Exploratory Study'],"['Skin cancer rates are rising in the United States, yet screening rates remain low. Meanwhile, social media has evolved to become a primary source of health information, with 40% of daily users of Pinterest reporting the platform as a ""go-to"" source. The objective of this research paper is to examine how skin cancer screenings were portrayed on Pinterest. Using the search terms ""skin cancer screening"" and ""skin cancer exam"", researchers sampled every fifth pin to collect 274 relevant pins. Two researchers coded the pins, and interrater agreement was established at 94%. The results showed that twenty-two percent of the sample depicted skin cancer screening in a negative way, yet 41.5% noted that early detection leads to better outcomes. The pins were geared toward younger, white women with minimal depiction of people of color. Few pins included comprehensive information about skin cancer risk factors, importance of routine self-screenings, or what to expect with a medical provider. Fifty-eight percent of pins included links to personal blogs. In conclusion, social media has become a powerful source of health information, yet much of the posted information is incomplete. These findings present public health experts with an opportunity to disseminate more comprehensive skin cancer screening information on social media.']"
ur.0640163054.49,"Merten, Julie",ur.01302000265.19,"Hamadi, Hanadi Y",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.0640163054.49,"Merten, Julie",ur.014472453705.75,"Quinn, Nathan",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.0640163054.49,"Merten, Julie",ur.011732133420.33,"Terrell, Kassie R",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.010425511525.83,"Williams, Cynthia",ur.012500267315.13,"DeDeo, Michelle",UNF,UNF,['pub.1145814923'],['Demographics and surgery-related complications lead to 30-day readmission rates among knee arthroscopic procedures'],"['PurposeThe study objectives were (1) to evaluate risk factors related to 30-day hospital readmissions after arthroscopic knee surgeries and (2) to determine the complications that may arise from surgery.MethodsThe American College of Surgeons National Surgical Quality Improvement Program database data from 2012 to 2017 were researched. Patients were identified using Current Procedural Terminology codes for knee arthroscopic procedures. Ordinal logistic fit regression and decision tree analysis were used to examine study objectives.ResultsThere were 83,083 knee arthroscopic procedures between 2012 and 2017 obtained from the National Surgical Quality Improvement Program database. The overall readmission rate was 0.87%. The complication rates were highest for synovectomy and cartilage procedures, 1.6% and 1.3% respectively. A majority of readmissions were related to the procedure (71.1%) with wound complications being the primary reason (28.2%) followed by pulmonary embolism and deep vein thrombosis, 12.7% and 10.6%, respectively. Gender and body mass index were not significant factors and age over 65\xa0years was an independent risk factor. Wound infection, deep vein thrombosis, and pulmonary embolism were the most prevalent complications.ConclusionHealthcare professionals have a unique opportunity to modify treatment plans based on patient risk factors. For patients who are at higher risk of inferior surgical outcomes, clinicians should carefully weigh risk factors when considering surgical and non-surgical approaches.Level of evidenceIII.']"
ur.012500267315.13,"DeDeo, Michelle",ur.010425511525.83,"Williams, Cynthia",UNF,UNF,['pub.1145814923'],['Demographics and surgery-related complications lead to 30-day readmission rates among knee arthroscopic procedures'],"['PurposeThe study objectives were (1) to evaluate risk factors related to 30-day hospital readmissions after arthroscopic knee surgeries and (2) to determine the complications that may arise from surgery.MethodsThe American College of Surgeons National Surgical Quality Improvement Program database data from 2012 to 2017 were researched. Patients were identified using Current Procedural Terminology codes for knee arthroscopic procedures. Ordinal logistic fit regression and decision tree analysis were used to examine study objectives.ResultsThere were 83,083 knee arthroscopic procedures between 2012 and 2017 obtained from the National Surgical Quality Improvement Program database. The overall readmission rate was 0.87%. The complication rates were highest for synovectomy and cartilage procedures, 1.6% and 1.3% respectively. A majority of readmissions were related to the procedure (71.1%) with wound complications being the primary reason (28.2%) followed by pulmonary embolism and deep vein thrombosis, 12.7% and 10.6%, respectively. Gender and body mass index were not significant factors and age over 65\xa0years was an independent risk factor. Wound infection, deep vein thrombosis, and pulmonary embolism were the most prevalent complications.ConclusionHealthcare professionals have a unique opportunity to modify treatment plans based on patient risk factors. For patients who are at higher risk of inferior surgical outcomes, clinicians should carefully weigh risk factors when considering surgical and non-surgical approaches.Level of evidenceIII.']"
ur.012500267315.13,"DeDeo, Michelle",ur.011610640333.18,"Lambert, J. David",UNF,UNF,['pub.1146281239'],"['Population and Nesting Site Evidence for Diamondback Terrapins, Malaclemys terrapin, in Northeast Florida']","['Diamondback terrapins (Malaclemys terrapin) are listed as Vulnerable by the IUCN Red List Index of Threatened Species. Among the challenges terrapins encounter are habitat loss due to coastal development and sea level rise, mortality at all life stages by mammalian and avian predators, road mortality, boat strikes, harvest for the pet trade, and drowning in crab traps. The primary objective of this study was to locate populations and nesting areas of diamondback terrapins in the four northeastern-most counties of Florida (Nassau, Duval, St. Johns, and Flagler). We conducted head counts and performed land surveys of shorelines and high spots for evidence of terrapin presence. During the land surveys we searched for crawls, intact and depredated nests, dead terrapins, and terrapin bones. To evaluate whether woody plant presence affected nest site choices, we recorded the occurrence of 10 common woody plant species during each land survey and compared areas where nesting did and did not occur. We collected 404 records of terrapin activity in 2013 and 2014. Most were from Nassau County (277) and only one was from Flagler County. Most data were in the form of depredated nests (205) and terrapin remains (147). The woody plant data suggest that terrapins were significantly more likely to nest when Christmas berry (Lycium carolinianum) was present, and nesting was less likely when either wax myrtle (Myrica cerifera) or oak (Quercus spp.) were present.']"
ur.012500267315.13,"DeDeo, Michelle",ur.07510646561.80,"Butler, Joseph A.",UNF,UNF,['pub.1146281239'],"['Population and Nesting Site Evidence for Diamondback Terrapins, Malaclemys terrapin, in Northeast Florida']","['Diamondback terrapins (Malaclemys terrapin) are listed as Vulnerable by the IUCN Red List Index of Threatened Species. Among the challenges terrapins encounter are habitat loss due to coastal development and sea level rise, mortality at all life stages by mammalian and avian predators, road mortality, boat strikes, harvest for the pet trade, and drowning in crab traps. The primary objective of this study was to locate populations and nesting areas of diamondback terrapins in the four northeastern-most counties of Florida (Nassau, Duval, St. Johns, and Flagler). We conducted head counts and performed land surveys of shorelines and high spots for evidence of terrapin presence. During the land surveys we searched for crawls, intact and depredated nests, dead terrapins, and terrapin bones. To evaluate whether woody plant presence affected nest site choices, we recorded the occurrence of 10 common woody plant species during each land survey and compared areas where nesting did and did not occur. We collected 404 records of terrapin activity in 2013 and 2014. Most were from Nassau County (277) and only one was from Flagler County. Most data were in the form of depredated nests (205) and terrapin remains (147). The woody plant data suggest that terrapins were significantly more likely to nest when Christmas berry (Lycium carolinianum) was present, and nesting was less likely when either wax myrtle (Myrica cerifera) or oak (Quercus spp.) were present.']"
ur.01164760644.22,"Loh, Chung-Ping A.",ur.0644663313.57,"Chatterjee, Chiradip",UNF,UNF,['pub.1148514912'],"['Consumer perception and information in a model of household water usage: The case of jacksonville, FL']","['Consumption of bottled water in the U.S. continues to grow despite the higher user price and greater environmental cost relative to municipal tap water. Convenience is surely one reason for this trend, but it is less relevant for in-home consumption of bottled water. The existing literature highlights perceptions of quality, access to information and personal experience as important factors influencing water usage in the home. In this paper we report the results of a 2018 survey of water customers of Jacksonville Electric Authority (JEA), the primary municipal water utility in Northeast Florida. The survey includes detailed questions regarding self-reported household water usage, information availability, information processing, trust in institutions and demographic characteristics. In addition, in cooperation with JEA, we matched the survey results with administrative data on geographic location within the system. Using a bivariate probit regression method, we estimate the determinants of water usage in the home. The results show that concern for drinking water safety is the principal contributor of bottled water consumption. Moreover, the evidence illustrates how information from water quality reports and objective measures of water hardness translate into the drinking water choice. We also show that greater transaction costs of bottled water due to low access to retail suppliers is associated with a substitution of water filtration for bottled water.']"
ur.01164760644.22,"Loh, Chung-Ping A.",ur.07577325317.14,"Triplett, Russell",UNF,UNF,['pub.1148514912'],"['Consumer perception and information in a model of household water usage: The case of jacksonville, FL']","['Consumption of bottled water in the U.S. continues to grow despite the higher user price and greater environmental cost relative to municipal tap water. Convenience is surely one reason for this trend, but it is less relevant for in-home consumption of bottled water. The existing literature highlights perceptions of quality, access to information and personal experience as important factors influencing water usage in the home. In this paper we report the results of a 2018 survey of water customers of Jacksonville Electric Authority (JEA), the primary municipal water utility in Northeast Florida. The survey includes detailed questions regarding self-reported household water usage, information availability, information processing, trust in institutions and demographic characteristics. In addition, in cooperation with JEA, we matched the survey results with administrative data on geographic location within the system. Using a bivariate probit regression method, we estimate the determinants of water usage in the home. The results show that concern for drinking water safety is the principal contributor of bottled water consumption. Moreover, the evidence illustrates how information from water quality reports and objective measures of water hardness translate into the drinking water choice. We also show that greater transaction costs of bottled water due to low access to retail suppliers is associated with a substitution of water filtration for bottled water.']"
ur.012043417064.50,"Gray, Eve",ur.01343071750.63,"Aspinwall, Michael J.",UNF,UNF,['pub.1145978256'],['Warming impacts on photosynthetic processes in dominant plant species in a subtropical forest'],"['Climate warming could shift some subtropical regions to a tropical climate in the next 30\u2009years. Yet, climate warming impacts on subtropical species and ecosystems remain unclear. We conducted a passive warming experiment in a subtropical forest in Florida, USA, to determine warming impacts on four species differing in their climatic distribution, growth form, and functional type: Serenoa repens (palm), Andropogon glomeratus (C<sub>4</sub> grass), Pinus palustris (needled evergreen tree), and Quercus laevis (broadleaved deciduous tree). We hypothesized that warming would have neutral-positive effects on photosynthetic processes in monocot species with warmer climatic distributions or adaptations to warmer temperatures, but negative effects on photosynthesis in tree species. We also hypothesized that periods of low soil moisture would alter photosynthetic responses to warming. In both monocot species, warming had no significant effect on net photosynthesis (A) or stomatal conductance (g<sub>s</sub> ) measured at prevailing temperatures, or photosynthetic capacity measured at a common temperature. In P. palustris, warming reduced A (-15%) and g<sub>s</sub> (-28%), and caused small reductions in Rubisco carboxylation and RuBP regeneration. Warming had little effect on photosynthetic processes in Q. laevis. Interestingly, A. glomeratus showed little sensitivity to reduced soil moisture, and all C<sub>3</sub> species reduced A and g<sub>s</sub> as soil moisture declined and did so consistently across temperature treatments. In subtropical forests of the southeastern US, we conclude that climate warming may have neutral or slightly positive effects on the performance of grasses and broadleaved species but negative effects on P. palustris seedlings, foreshadowing possible changes in community and ecosystem properties.']"
ur.012043417064.50,"Gray, Eve",ur.011201414666.36,"Chieppa, Jeff",UNF,UNF,['pub.1145978256'],['Warming impacts on photosynthetic processes in dominant plant species in a subtropical forest'],"['Climate warming could shift some subtropical regions to a tropical climate in the next 30\u2009years. Yet, climate warming impacts on subtropical species and ecosystems remain unclear. We conducted a passive warming experiment in a subtropical forest in Florida, USA, to determine warming impacts on four species differing in their climatic distribution, growth form, and functional type: Serenoa repens (palm), Andropogon glomeratus (C<sub>4</sub> grass), Pinus palustris (needled evergreen tree), and Quercus laevis (broadleaved deciduous tree). We hypothesized that warming would have neutral-positive effects on photosynthetic processes in monocot species with warmer climatic distributions or adaptations to warmer temperatures, but negative effects on photosynthesis in tree species. We also hypothesized that periods of low soil moisture would alter photosynthetic responses to warming. In both monocot species, warming had no significant effect on net photosynthesis (A) or stomatal conductance (g<sub>s</sub> ) measured at prevailing temperatures, or photosynthetic capacity measured at a common temperature. In P. palustris, warming reduced A (-15%) and g<sub>s</sub> (-28%), and caused small reductions in Rubisco carboxylation and RuBP regeneration. Warming had little effect on photosynthetic processes in Q. laevis. Interestingly, A. glomeratus showed little sensitivity to reduced soil moisture, and all C<sub>3</sub> species reduced A and g<sub>s</sub> as soil moisture declined and did so consistently across temperature treatments. In subtropical forests of the southeastern US, we conclude that climate warming may have neutral or slightly positive effects on the performance of grasses and broadleaved species but negative effects on P. palustris seedlings, foreshadowing possible changes in community and ecosystem properties.']"
ur.01343071750.63,"Aspinwall, Michael J.",ur.012043417064.50,"Gray, Eve",UNF,UNF,['pub.1145978256'],['Warming impacts on photosynthetic processes in dominant plant species in a subtropical forest'],"['Climate warming could shift some subtropical regions to a tropical climate in the next 30\u2009years. Yet, climate warming impacts on subtropical species and ecosystems remain unclear. We conducted a passive warming experiment in a subtropical forest in Florida, USA, to determine warming impacts on four species differing in their climatic distribution, growth form, and functional type: Serenoa repens (palm), Andropogon glomeratus (C<sub>4</sub> grass), Pinus palustris (needled evergreen tree), and Quercus laevis (broadleaved deciduous tree). We hypothesized that warming would have neutral-positive effects on photosynthetic processes in monocot species with warmer climatic distributions or adaptations to warmer temperatures, but negative effects on photosynthesis in tree species. We also hypothesized that periods of low soil moisture would alter photosynthetic responses to warming. In both monocot species, warming had no significant effect on net photosynthesis (A) or stomatal conductance (g<sub>s</sub> ) measured at prevailing temperatures, or photosynthetic capacity measured at a common temperature. In P. palustris, warming reduced A (-15%) and g<sub>s</sub> (-28%), and caused small reductions in Rubisco carboxylation and RuBP regeneration. Warming had little effect on photosynthetic processes in Q. laevis. Interestingly, A. glomeratus showed little sensitivity to reduced soil moisture, and all C<sub>3</sub> species reduced A and g<sub>s</sub> as soil moisture declined and did so consistently across temperature treatments. In subtropical forests of the southeastern US, we conclude that climate warming may have neutral or slightly positive effects on the performance of grasses and broadleaved species but negative effects on P. palustris seedlings, foreshadowing possible changes in community and ecosystem properties.']"
ur.01343071750.63,"Aspinwall, Michael J.",ur.011201414666.36,"Chieppa, Jeff",UNF,UNF,"['pub.1145978256', 'pub.1151125604']","['Warming impacts on photosynthetic processes in dominant plant species in a subtropical forest', 'Contrasting Effects of Nitrogen Addition on Leaf Photosynthesis and Respiration in Black Mangrove in North Florida']","['Climate warming could shift some subtropical regions to a tropical climate in the next 30\u2009years. Yet, climate warming impacts on subtropical species and ecosystems remain unclear. We conducted a passive warming experiment in a subtropical forest in Florida, USA, to determine warming impacts on four species differing in their climatic distribution, growth form, and functional type: Serenoa repens (palm), Andropogon glomeratus (C<sub>4</sub> grass), Pinus palustris (needled evergreen tree), and Quercus laevis (broadleaved deciduous tree). We hypothesized that warming would have neutral-positive effects on photosynthetic processes in monocot species with warmer climatic distributions or adaptations to warmer temperatures, but negative effects on photosynthesis in tree species. We also hypothesized that periods of low soil moisture would alter photosynthetic responses to warming. In both monocot species, warming had no significant effect on net photosynthesis (A) or stomatal conductance (g<sub>s</sub> ) measured at prevailing temperatures, or photosynthetic capacity measured at a common temperature. In P. palustris, warming reduced A (-15%) and g<sub>s</sub> (-28%), and caused small reductions in Rubisco carboxylation and RuBP regeneration. Warming had little effect on photosynthetic processes in Q. laevis. Interestingly, A. glomeratus showed little sensitivity to reduced soil moisture, and all C<sub>3</sub> species reduced A and g<sub>s</sub> as soil moisture declined and did so consistently across temperature treatments. In subtropical forests of the southeastern US, we conclude that climate warming may have neutral or slightly positive effects on the performance of grasses and broadleaved species but negative effects on P. palustris seedlings, foreshadowing possible changes in community and ecosystem properties.', 'Nutrient enrichment is a major driver of environmental change in mangrove ecosystems. Yet, nutrient enrichment impacts on physiological processes that regulate CO2 and water fluxes between mangrove vegetation and the atmosphere remain unclear. We measured peak growing season photosynthesis (A) and respiration (R) in black mangrove (Avicennia germinans) leaves that had been subjected to long-term (8-year) nutrient enrichment (added N, added P, control) in north Florida. Previous results from this site indicated that Avicennia productivity was N-limited, but not P-limited. Thus, we expected that N addition would increase light saturated net photosynthesis at ambient CO2 (Anet), intrinsic water-use efficiency (iWUE), maximum rate of Rubisco carboxylation (Vcmax), and leaf dark respiration (R), while P addition would have little effect on any aspect of photosynthesis or respiration. We expected that increased photosynthesis and respiration would be most apparent immediately after N addition and in newly formed leaves. Indeed, Anet and Vcmax increased just after N addition in the N addition treatment; these increases were limited to leaves formed just after N addition. Nonetheless, over time, photosynthetic parameters and iWUE were similar across treatments. Interestingly, R measured at 25\xa0°C increased with N addition; this effect was consistent across time points. P addition had little effect on R. Across treatments and time points, Vcmax,25 (Vcmax standardized to 25\xa0°C) showed no relationship with R at 25\xa0°C, but the maximum rate of electron transport for RuBP regeneration standardized to 25\xa0°C (Jmax,25) increased with R at 25\xa0°C. We conclude that N addition may have small, short-lived effects on photosynthetic processes, but sustained effects on leaf R in N-limited mangrove ecosystems.']"
ur.01343071750.63,"Aspinwall, Michael J.",ur.016132335454.74,"Sturchio, Matthew A.",UNF,UNF,['pub.1151125604'],['Contrasting Effects of Nitrogen Addition on Leaf Photosynthesis and Respiration in Black Mangrove in North Florida'],"['Nutrient enrichment is a major driver of environmental change in mangrove ecosystems. Yet, nutrient enrichment impacts on physiological processes that regulate CO2 and water fluxes between mangrove vegetation and the atmosphere remain unclear. We measured peak growing season photosynthesis (A) and respiration (R) in black mangrove (Avicennia germinans) leaves that had been subjected to long-term (8-year) nutrient enrichment (added N, added P, control) in north Florida. Previous results from this site indicated that Avicennia productivity was N-limited, but not P-limited. Thus, we expected that N addition would increase light saturated net photosynthesis at ambient CO2 (Anet), intrinsic water-use efficiency (iWUE), maximum rate of Rubisco carboxylation (Vcmax), and leaf dark respiration (R), while P addition would have little effect on any aspect of photosynthesis or respiration. We expected that increased photosynthesis and respiration would be most apparent immediately after N addition and in newly formed leaves. Indeed, Anet and Vcmax increased just after N addition in the N addition treatment; these increases were limited to leaves formed just after N addition. Nonetheless, over time, photosynthetic parameters and iWUE were similar across treatments. Interestingly, R measured at 25\xa0°C increased with N addition; this effect was consistent across time points. P addition had little effect on R. Across treatments and time points, Vcmax,25 (Vcmax standardized to 25\xa0°C) showed no relationship with R at 25\xa0°C, but the maximum rate of electron transport for RuBP regeneration standardized to 25\xa0°C (Jmax,25) increased with R at 25\xa0°C. We conclude that N addition may have small, short-lived effects on photosynthetic processes, but sustained effects on leaf R in N-limited mangrove ecosystems.']"
ur.011201414666.36,"Chieppa, Jeff",ur.012043417064.50,"Gray, Eve",UNF,UNF,['pub.1145978256'],['Warming impacts on photosynthetic processes in dominant plant species in a subtropical forest'],"['Climate warming could shift some subtropical regions to a tropical climate in the next 30\u2009years. Yet, climate warming impacts on subtropical species and ecosystems remain unclear. We conducted a passive warming experiment in a subtropical forest in Florida, USA, to determine warming impacts on four species differing in their climatic distribution, growth form, and functional type: Serenoa repens (palm), Andropogon glomeratus (C<sub>4</sub> grass), Pinus palustris (needled evergreen tree), and Quercus laevis (broadleaved deciduous tree). We hypothesized that warming would have neutral-positive effects on photosynthetic processes in monocot species with warmer climatic distributions or adaptations to warmer temperatures, but negative effects on photosynthesis in tree species. We also hypothesized that periods of low soil moisture would alter photosynthetic responses to warming. In both monocot species, warming had no significant effect on net photosynthesis (A) or stomatal conductance (g<sub>s</sub> ) measured at prevailing temperatures, or photosynthetic capacity measured at a common temperature. In P. palustris, warming reduced A (-15%) and g<sub>s</sub> (-28%), and caused small reductions in Rubisco carboxylation and RuBP regeneration. Warming had little effect on photosynthetic processes in Q. laevis. Interestingly, A. glomeratus showed little sensitivity to reduced soil moisture, and all C<sub>3</sub> species reduced A and g<sub>s</sub> as soil moisture declined and did so consistently across temperature treatments. In subtropical forests of the southeastern US, we conclude that climate warming may have neutral or slightly positive effects on the performance of grasses and broadleaved species but negative effects on P. palustris seedlings, foreshadowing possible changes in community and ecosystem properties.']"
ur.011201414666.36,"Chieppa, Jeff",ur.01343071750.63,"Aspinwall, Michael J.",UNF,UNF,"['pub.1145978256', 'pub.1151125604']","['Warming impacts on photosynthetic processes in dominant plant species in a subtropical forest', 'Contrasting Effects of Nitrogen Addition on Leaf Photosynthesis and Respiration in Black Mangrove in North Florida']","['Climate warming could shift some subtropical regions to a tropical climate in the next 30\u2009years. Yet, climate warming impacts on subtropical species and ecosystems remain unclear. We conducted a passive warming experiment in a subtropical forest in Florida, USA, to determine warming impacts on four species differing in their climatic distribution, growth form, and functional type: Serenoa repens (palm), Andropogon glomeratus (C<sub>4</sub> grass), Pinus palustris (needled evergreen tree), and Quercus laevis (broadleaved deciduous tree). We hypothesized that warming would have neutral-positive effects on photosynthetic processes in monocot species with warmer climatic distributions or adaptations to warmer temperatures, but negative effects on photosynthesis in tree species. We also hypothesized that periods of low soil moisture would alter photosynthetic responses to warming. In both monocot species, warming had no significant effect on net photosynthesis (A) or stomatal conductance (g<sub>s</sub> ) measured at prevailing temperatures, or photosynthetic capacity measured at a common temperature. In P. palustris, warming reduced A (-15%) and g<sub>s</sub> (-28%), and caused small reductions in Rubisco carboxylation and RuBP regeneration. Warming had little effect on photosynthetic processes in Q. laevis. Interestingly, A. glomeratus showed little sensitivity to reduced soil moisture, and all C<sub>3</sub> species reduced A and g<sub>s</sub> as soil moisture declined and did so consistently across temperature treatments. In subtropical forests of the southeastern US, we conclude that climate warming may have neutral or slightly positive effects on the performance of grasses and broadleaved species but negative effects on P. palustris seedlings, foreshadowing possible changes in community and ecosystem properties.', 'Nutrient enrichment is a major driver of environmental change in mangrove ecosystems. Yet, nutrient enrichment impacts on physiological processes that regulate CO2 and water fluxes between mangrove vegetation and the atmosphere remain unclear. We measured peak growing season photosynthesis (A) and respiration (R) in black mangrove (Avicennia germinans) leaves that had been subjected to long-term (8-year) nutrient enrichment (added N, added P, control) in north Florida. Previous results from this site indicated that Avicennia productivity was N-limited, but not P-limited. Thus, we expected that N addition would increase light saturated net photosynthesis at ambient CO2 (Anet), intrinsic water-use efficiency (iWUE), maximum rate of Rubisco carboxylation (Vcmax), and leaf dark respiration (R), while P addition would have little effect on any aspect of photosynthesis or respiration. We expected that increased photosynthesis and respiration would be most apparent immediately after N addition and in newly formed leaves. Indeed, Anet and Vcmax increased just after N addition in the N addition treatment; these increases were limited to leaves formed just after N addition. Nonetheless, over time, photosynthetic parameters and iWUE were similar across treatments. Interestingly, R measured at 25\xa0°C increased with N addition; this effect was consistent across time points. P addition had little effect on R. Across treatments and time points, Vcmax,25 (Vcmax standardized to 25\xa0°C) showed no relationship with R at 25\xa0°C, but the maximum rate of electron transport for RuBP regeneration standardized to 25\xa0°C (Jmax,25) increased with R at 25\xa0°C. We conclude that N addition may have small, short-lived effects on photosynthetic processes, but sustained effects on leaf R in N-limited mangrove ecosystems.']"
ur.011201414666.36,"Chieppa, Jeff",ur.016132335454.74,"Sturchio, Matthew A.",UNF,UNF,['pub.1151125604'],['Contrasting Effects of Nitrogen Addition on Leaf Photosynthesis and Respiration in Black Mangrove in North Florida'],"['Nutrient enrichment is a major driver of environmental change in mangrove ecosystems. Yet, nutrient enrichment impacts on physiological processes that regulate CO2 and water fluxes between mangrove vegetation and the atmosphere remain unclear. We measured peak growing season photosynthesis (A) and respiration (R) in black mangrove (Avicennia germinans) leaves that had been subjected to long-term (8-year) nutrient enrichment (added N, added P, control) in north Florida. Previous results from this site indicated that Avicennia productivity was N-limited, but not P-limited. Thus, we expected that N addition would increase light saturated net photosynthesis at ambient CO2 (Anet), intrinsic water-use efficiency (iWUE), maximum rate of Rubisco carboxylation (Vcmax), and leaf dark respiration (R), while P addition would have little effect on any aspect of photosynthesis or respiration. We expected that increased photosynthesis and respiration would be most apparent immediately after N addition and in newly formed leaves. Indeed, Anet and Vcmax increased just after N addition in the N addition treatment; these increases were limited to leaves formed just after N addition. Nonetheless, over time, photosynthetic parameters and iWUE were similar across treatments. Interestingly, R measured at 25\xa0°C increased with N addition; this effect was consistent across time points. P addition had little effect on R. Across treatments and time points, Vcmax,25 (Vcmax standardized to 25\xa0°C) showed no relationship with R at 25\xa0°C, but the maximum rate of electron transport for RuBP regeneration standardized to 25\xa0°C (Jmax,25) increased with R at 25\xa0°C. We conclude that N addition may have small, short-lived effects on photosynthetic processes, but sustained effects on leaf R in N-limited mangrove ecosystems.']"
ur.016477543151.92,"Cosentino, Ralph G.",ur.01365551077.18,"Churilla, James R.",UNF,UNF,['pub.1146012123'],['Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis'],[nan]
ur.016477543151.92,"Cosentino, Ralph G.",ur.01337755147.18,"Johnson, Tammie M.",UNF,UNF,['pub.1146012123'],['Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis'],[nan]
ur.016477543151.92,"Cosentino, Ralph G.",ur.01253770701.51,"Richardson, Michael R.",UNF,UNF,['pub.1146012123'],['Trends in Hyperinsulinemia Among U.S. Adults without Diabetes: A Joinpoint Analysis'],[nan]
ur.011610640333.18,"Lambert, J. David",ur.07510646561.80,"Butler, Joseph A.",UNF,UNF,['pub.1146281239'],"['Population and Nesting Site Evidence for Diamondback Terrapins, Malaclemys terrapin, in Northeast Florida']","['Diamondback terrapins (Malaclemys terrapin) are listed as Vulnerable by the IUCN Red List Index of Threatened Species. Among the challenges terrapins encounter are habitat loss due to coastal development and sea level rise, mortality at all life stages by mammalian and avian predators, road mortality, boat strikes, harvest for the pet trade, and drowning in crab traps. The primary objective of this study was to locate populations and nesting areas of diamondback terrapins in the four northeastern-most counties of Florida (Nassau, Duval, St. Johns, and Flagler). We conducted head counts and performed land surveys of shorelines and high spots for evidence of terrapin presence. During the land surveys we searched for crawls, intact and depredated nests, dead terrapins, and terrapin bones. To evaluate whether woody plant presence affected nest site choices, we recorded the occurrence of 10 common woody plant species during each land survey and compared areas where nesting did and did not occur. We collected 404 records of terrapin activity in 2013 and 2014. Most were from Nassau County (277) and only one was from Flagler County. Most data were in the form of depredated nests (205) and terrapin remains (147). The woody plant data suggest that terrapins were significantly more likely to nest when Christmas berry (Lycium carolinianum) was present, and nesting was less likely when either wax myrtle (Myrica cerifera) or oak (Quercus spp.) were present.']"
ur.011610640333.18,"Lambert, J. David",ur.012500267315.13,"DeDeo, Michelle",UNF,UNF,['pub.1146281239'],"['Population and Nesting Site Evidence for Diamondback Terrapins, Malaclemys terrapin, in Northeast Florida']","['Diamondback terrapins (Malaclemys terrapin) are listed as Vulnerable by the IUCN Red List Index of Threatened Species. Among the challenges terrapins encounter are habitat loss due to coastal development and sea level rise, mortality at all life stages by mammalian and avian predators, road mortality, boat strikes, harvest for the pet trade, and drowning in crab traps. The primary objective of this study was to locate populations and nesting areas of diamondback terrapins in the four northeastern-most counties of Florida (Nassau, Duval, St. Johns, and Flagler). We conducted head counts and performed land surveys of shorelines and high spots for evidence of terrapin presence. During the land surveys we searched for crawls, intact and depredated nests, dead terrapins, and terrapin bones. To evaluate whether woody plant presence affected nest site choices, we recorded the occurrence of 10 common woody plant species during each land survey and compared areas where nesting did and did not occur. We collected 404 records of terrapin activity in 2013 and 2014. Most were from Nassau County (277) and only one was from Flagler County. Most data were in the form of depredated nests (205) and terrapin remains (147). The woody plant data suggest that terrapins were significantly more likely to nest when Christmas berry (Lycium carolinianum) was present, and nesting was less likely when either wax myrtle (Myrica cerifera) or oak (Quercus spp.) were present.']"
ur.07510646561.80,"Butler, Joseph A.",ur.011610640333.18,"Lambert, J. David",UNF,UNF,['pub.1146281239'],"['Population and Nesting Site Evidence for Diamondback Terrapins, Malaclemys terrapin, in Northeast Florida']","['Diamondback terrapins (Malaclemys terrapin) are listed as Vulnerable by the IUCN Red List Index of Threatened Species. Among the challenges terrapins encounter are habitat loss due to coastal development and sea level rise, mortality at all life stages by mammalian and avian predators, road mortality, boat strikes, harvest for the pet trade, and drowning in crab traps. The primary objective of this study was to locate populations and nesting areas of diamondback terrapins in the four northeastern-most counties of Florida (Nassau, Duval, St. Johns, and Flagler). We conducted head counts and performed land surveys of shorelines and high spots for evidence of terrapin presence. During the land surveys we searched for crawls, intact and depredated nests, dead terrapins, and terrapin bones. To evaluate whether woody plant presence affected nest site choices, we recorded the occurrence of 10 common woody plant species during each land survey and compared areas where nesting did and did not occur. We collected 404 records of terrapin activity in 2013 and 2014. Most were from Nassau County (277) and only one was from Flagler County. Most data were in the form of depredated nests (205) and terrapin remains (147). The woody plant data suggest that terrapins were significantly more likely to nest when Christmas berry (Lycium carolinianum) was present, and nesting was less likely when either wax myrtle (Myrica cerifera) or oak (Quercus spp.) were present.']"
ur.07510646561.80,"Butler, Joseph A.",ur.012500267315.13,"DeDeo, Michelle",UNF,UNF,['pub.1146281239'],"['Population and Nesting Site Evidence for Diamondback Terrapins, Malaclemys terrapin, in Northeast Florida']","['Diamondback terrapins (Malaclemys terrapin) are listed as Vulnerable by the IUCN Red List Index of Threatened Species. Among the challenges terrapins encounter are habitat loss due to coastal development and sea level rise, mortality at all life stages by mammalian and avian predators, road mortality, boat strikes, harvest for the pet trade, and drowning in crab traps. The primary objective of this study was to locate populations and nesting areas of diamondback terrapins in the four northeastern-most counties of Florida (Nassau, Duval, St. Johns, and Flagler). We conducted head counts and performed land surveys of shorelines and high spots for evidence of terrapin presence. During the land surveys we searched for crawls, intact and depredated nests, dead terrapins, and terrapin bones. To evaluate whether woody plant presence affected nest site choices, we recorded the occurrence of 10 common woody plant species during each land survey and compared areas where nesting did and did not occur. We collected 404 records of terrapin activity in 2013 and 2014. Most were from Nassau County (277) and only one was from Flagler County. Most data were in the form of depredated nests (205) and terrapin remains (147). The woody plant data suggest that terrapins were significantly more likely to nest when Christmas berry (Lycium carolinianum) was present, and nesting was less likely when either wax myrtle (Myrica cerifera) or oak (Quercus spp.) were present.']"
ur.0634752264.04,"Zeglin, Robert J.",ur.07500421537.27,"Niemela, Danielle R. M.",UNF,UNF,['pub.1149209947'],['Parents Talking to Middle School Children about Sex: A Protective Factor against Suicide in Sexually Active Teens'],"['Suicide continues to be a considerable health risk among adolescents and is the second leading cause of death among adolescents between the ages of 10 and 24. Middle school-aged adolescents may be at a heightened risk for suicide, as middle school can be rife with intrapersonal and interpersonal stressors. One such stressor may be sexual activity and navigating sexual relationships. Past research has shown that parents can play an important role in helping adolescents cope with stressors at this age, but there is no identified research assessing whether parental conversations about sex are associated with suicidality among adolescents. The current study addressed this gap via secondary data analysis using logistic regression with a sample of 3,568 middle school students (mean age = 12.74; SD = 1.08). Results suggest that parental conversations about sex are a significant protective factor against suicidality, but only among adolescents who report having engaged in sexual activity.']"
ur.0634752264.04,"Zeglin, Robert J.",ur.014472453705.75,"Quinn, N.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.0634752264.04,"Zeglin, Robert J.",ur.015534510336.20,"Boggs, C.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.0634752264.04,"Zeglin, Robert J.",ur.011732133420.33,"Terrell, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.0634752264.04,"Zeglin, Robert J.",ur.07627017536.46,"Rule, M.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.0634752264.04,"Zeglin, Robert J.",ur.011252000536.58,"Hicks-Roof, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.01365060341.83,"Kahanda, Indika",ur.014015716703.00,"Kazi, Nazmul",UNF,UNF,['pub.1150887389'],['Challenges and opportunities in current vaccine technology and administration: A comprehensive survey examining oral vaccine potential in the United States'],"[""This study provides a snapshot of the current vaccine business ecosystem, including practices, challenges, beliefs, and expectations of vaccine providers. Our team focused on providers' firsthand experience with administering vaccines to determine if an oral vaccine (e.g. pill or oral-drop) would be well-received. We interviewed 135 healthcare providers and vaccine specialists across the US, focusing questions on routine vaccinations, not COVID-19 vaccines. Improving workflow efficiency is a top concern among vaccine providers due to shrinking reimbursement rates-determined by pharmacy benefit managers (PBMs)-and the time-intensiveness of injectable vaccines. Administering injectable vaccines takes 23\u2009minutes/patient on average, while dispensing pills takes only 5\u2009minutes/patient. An average of 24% of patients express needle-fear, which further lengthens the processing time. Misaligned incentives between providers and PBMs could reduce the quality and availability of vaccine-related care. The unavailability of single-dose orders prevents some rural providers from offering certain vaccines. Most interviewees (74%) believe an oral vaccine would improve patient-provider experience, patient-compliance, and workflow efficiency, while detractors (26%) worry about the taste, vaccine absorption, and efficacy. Additional research could investigate whether currently non-vaccinating pharmacies would be willing to offer oral vaccines, and the impact of oral vaccines on vaccine acceptance.""]"
ur.016646474730.58,"Williamson, Steven",ur.010747712675.93,"Zhang, Justin Zuopeng",UNF,UNF,"['pub.1146347117', 'pub.1151635872']","['IT assimilation: construct, measurement, and implications in cybersecurity', 'Work-to-Home Cybersecurity Spillover: Construct Development and Validation']","['Unintentional employee behaviours can be a major cause of security breaches. To mitigate the risk, employees need to habituate their new behaviours, implying a transfer of learning that may occur at ‘work’ to ‘personal’ contexts. Against this backdrop, we examine a new construct – IT Assimilation – grounded in situated learning theory by its definition as the incorporation of enterprise IT into an individual’s IT repertoire. We use a multi-phase methodological approach to conceptualise the construct and develop a validated scale to measure it. Our findings offer valuable insights for researchers and practitioners about transferring IT behaviours, particularly in the cybersecurity context..', 'The COVID-19 pandemic has forced employees to work from home using enterprise systems outside traditional organizational boundaries. Thus, organizations cannot enforce security policies for safe IT use behaviors but depend on continuance intentions of work policies at home. We study the transfer of IT security behaviors from work to home by developing the construct of Work-to-Home Cybersecurity Spillover and empirically testing and validating it. Our findings provide practical implications for researchers and practitioners in cybersecurity.']"
ur.016646474730.58,"Williamson, Steven",ur.011416214620.82,"Goel, Lakshmi",UNF,UNF,"['pub.1146347117', 'pub.1151635872']","['IT assimilation: construct, measurement, and implications in cybersecurity', 'Work-to-Home Cybersecurity Spillover: Construct Development and Validation']","['Unintentional employee behaviours can be a major cause of security breaches. To mitigate the risk, employees need to habituate their new behaviours, implying a transfer of learning that may occur at ‘work’ to ‘personal’ contexts. Against this backdrop, we examine a new construct – IT Assimilation – grounded in situated learning theory by its definition as the incorporation of enterprise IT into an individual’s IT repertoire. We use a multi-phase methodological approach to conceptualise the construct and develop a validated scale to measure it. Our findings offer valuable insights for researchers and practitioners about transferring IT behaviours, particularly in the cybersecurity context..', 'The COVID-19 pandemic has forced employees to work from home using enterprise systems outside traditional organizational boundaries. Thus, organizations cannot enforce security policies for safe IT use behaviors but depend on continuance intentions of work policies at home. We study the transfer of IT security behaviors from work to home by developing the construct of Work-to-Home Cybersecurity Spillover and empirically testing and validating it. Our findings provide practical implications for researchers and practitioners in cybersecurity.']"
ur.012171662703.42,"Ninya, Leila",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1146475296'],['Nutrition and Dietetics Faculty Providing Experiential Learning: Opportunities Amid the Pandemic and Beyond'],[nan]
ur.015043716424.49,"Van Horn, Leslie Thompson",ur.01025441731.01,"Wright, Lauri",UNF,UNF,"['pub.1146517805', 'pub.1147495410', 'pub.1151572301', 'pub.1152312344']","['Trends Related to Evidence-Based Dietetic Practice Among Dietitians With Varying Levels of Education and Experience', 'Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists', 'Food insecurity among older adults living in low- and middle-income countries: a scoping review.']","[""This qualitative study evaluates registered dietitian nutritionists' (RDNs) use of evidence-based dietetic practices (EBDPs) by education level and years of experience. Interviews were conducted with bachelor's and master's-trained RDNs (n = 9). The following themes regarding EBDP were identified: (1) resources utilized, (2) personal utilization and perceptions, (3) barriers and areas for improvement, and (4) collaboration. It was noted that master's-trained RDNs were more knowledgeable regarding EBDP resources. While it was reported that EBDP had improved with experience, those further from their didactic training had less awareness of EBDP resources. Continuing education should aim to bridge this gap in knowledge among RDNs."", ""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree."", 'BACKGROUND AND OBJECTIVES: Food insecurity is considered an increasing public health problem worldwide with adverse effects, especially among older adults. Although the literature related to food insecurity among older adults in low- and middle-income countries (LMIC) is expanding, little is known about existing patterns and knowledge gaps in these settings. This scoping review aims to provide a comprehensive overview of the current research related to food insecurity among older adults in LMIC.\r\nRESEARCH DESIGN AND METHODS: A systematic search was conducted in November 2021 and revised in July 2022 on six databases using terms related to food insecurity and older adults. Data were extracted and the emerging themes from the main findings were summarized using a Social-Ecological Model (SEM).\r\nRESULTS: Forty-one studies met the inclusion criteria. Almost half (48.8%) were published in the last two years and utilized a quantitative approach (n=38). Only one study was conducted in a low-income country. Using the SEM, most studies included in this review focused on addressing the relationship between food insecurity with intrapersonal factors.\r\nDISCUSSION AND IMPLICATIONS: Several gaps in the current literature were identified. There is a lack of longitudinal and qualitative studies available on this topic. Also, only fifteen LMIC were represented in the literature. A critical point in this review is that only a few studies addressed the relationship between food insecurity and the policy/social structure, institutional, community and interpersonal levels. These identified gaps can serve as a guide for future research on this topic.']"
ur.015043716424.49,"Van Horn, Leslie Thompson",ur.01213347266.19,"Sealey‐Potts, Claudia",UNF,UNF,"['pub.1147495410', 'pub.1151572301']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.015043716424.49,"Van Horn, Leslie Thompson",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,"['pub.1147495410', 'pub.1151572301', 'pub.1152312344']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists', 'Food insecurity among older adults living in low- and middle-income countries: a scoping review.']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree."", 'BACKGROUND AND OBJECTIVES: Food insecurity is considered an increasing public health problem worldwide with adverse effects, especially among older adults. Although the literature related to food insecurity among older adults in low- and middle-income countries (LMIC) is expanding, little is known about existing patterns and knowledge gaps in these settings. This scoping review aims to provide a comprehensive overview of the current research related to food insecurity among older adults in LMIC.\r\nRESEARCH DESIGN AND METHODS: A systematic search was conducted in November 2021 and revised in July 2022 on six databases using terms related to food insecurity and older adults. Data were extracted and the emerging themes from the main findings were summarized using a Social-Ecological Model (SEM).\r\nRESULTS: Forty-one studies met the inclusion criteria. Almost half (48.8%) were published in the last two years and utilized a quantitative approach (n=38). Only one study was conducted in a low-income country. Using the SEM, most studies included in this review focused on addressing the relationship between food insecurity with intrapersonal factors.\r\nDISCUSSION AND IMPLICATIONS: Several gaps in the current literature were identified. There is a lack of longitudinal and qualitative studies available on this topic. Also, only fifteen LMIC were represented in the literature. A critical point in this review is that only a few studies addressed the relationship between food insecurity and the policy/social structure, institutional, community and interpersonal levels. These identified gaps can serve as a guide for future research on this topic.']"
ur.015043716424.49,"Van Horn, Leslie Thompson",ur.01302000265.19,"Hamadi, Hanadi",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.013004000573.89,"Martinez, Dayana",ur.016425135625.29,"Xu, Jing",UNF,UNF,['pub.1146570632'],['Impact of Change in Allocation Score Methodology on Post Kidney Transplant Average Length of Stay'],"['Background: In December 2014, a new Kidney Allocation System (KAS) was implemented nationwide to improve access and quality of care to historically disadvantaged patients. However, no study to date has examined the relationship between the KAS and potential changes in hospital length of stay (LOS). This study aimed to examine the relationship between the KAS implemented in December 2014 and potential changes in hospital LOS.\r\nMethods: We used data from the Florida Agency for Health Care Administration on kidney transplant surgeries completed between 2011 and 2018. A cross-sectional cohort study design included seven hospitals that performed kidney transplants for the duration of the study. A propensity score matching approach was used to examine the relationship between KAS and LOS. All acute general medical and surgical hospitals in Florida that performed kidney transplant surgery were included in the analysis.\r\nResults: We included 7,795 patients, 6,119 discharged to home, and 1,676 discharged to home with home health services after transplant. The average LOS prior to KAS was 6.52 days and 6.08 days post KAS. Propensity matched results show that patients transferred to home experienced a decrease in the LOS (coefficient (β) = -0.68; 95% confidence interval (CI): -0.95, -0.42) after the new allocation score was implemented. Similarly, patients transferred to home with home health experienced a decrease in the LOS (β = -1.90; 95% CI: -2.69, -1.11) after the new allocation was implemented.\r\nConclusion: In conclusion, results indicate that KAS implementation did not add a burden on the health system by increasing LOS when considering patients with similar characteristics before and after KAS implementation. KAS is an important policy change that appears to not negatively affect the LOS when sicker patients could receive a kidney transplant. Our findings improve our understanding of the KAS policy and its influence on the health system.']"
ur.013004000573.89,"Martinez, Dayana",ur.01302000265.19,"Hamadi, Hanadi Y.",UNF,UNF,['pub.1146570632'],['Impact of Change in Allocation Score Methodology on Post Kidney Transplant Average Length of Stay'],"['Background: In December 2014, a new Kidney Allocation System (KAS) was implemented nationwide to improve access and quality of care to historically disadvantaged patients. However, no study to date has examined the relationship between the KAS and potential changes in hospital length of stay (LOS). This study aimed to examine the relationship between the KAS implemented in December 2014 and potential changes in hospital LOS.\r\nMethods: We used data from the Florida Agency for Health Care Administration on kidney transplant surgeries completed between 2011 and 2018. A cross-sectional cohort study design included seven hospitals that performed kidney transplants for the duration of the study. A propensity score matching approach was used to examine the relationship between KAS and LOS. All acute general medical and surgical hospitals in Florida that performed kidney transplant surgery were included in the analysis.\r\nResults: We included 7,795 patients, 6,119 discharged to home, and 1,676 discharged to home with home health services after transplant. The average LOS prior to KAS was 6.52 days and 6.08 days post KAS. Propensity matched results show that patients transferred to home experienced a decrease in the LOS (coefficient (β) = -0.68; 95% confidence interval (CI): -0.95, -0.42) after the new allocation score was implemented. Similarly, patients transferred to home with home health experienced a decrease in the LOS (β = -1.90; 95% CI: -2.69, -1.11) after the new allocation was implemented.\r\nConclusion: In conclusion, results indicate that KAS implementation did not add a burden on the health system by increasing LOS when considering patients with similar characteristics before and after KAS implementation. KAS is an important policy change that appears to not negatively affect the LOS when sicker patients could receive a kidney transplant. Our findings improve our understanding of the KAS policy and its influence on the health system.']"
ur.014104062341.33,"Martin, Daniel L.",ur.0701575222.69,"Ross, Cliff",UNF,UNF,['pub.1146587311'],"['Corrigendum: Spatial Patterns of Thalassia testudinum Immune Status and Labyrinthula spp. Load Implicate Environmental Quality and History as Modulators of Defense Strategies and Wasting Disease in Florida Bay, United States']",['[This corrects the article DOI: 10.3389/fpls.2021.612947.].']
ur.014104062341.33,"Martin, Daniel L.",ur.01230272013.16,"Duffin, Paige",UNF,UNF,['pub.1146587311'],"['Corrigendum: Spatial Patterns of Thalassia testudinum Immune Status and Labyrinthula spp. Load Implicate Environmental Quality and History as Modulators of Defense Strategies and Wasting Disease in Florida Bay, United States']",['[This corrects the article DOI: 10.3389/fpls.2021.612947.].']
ur.0701575222.69,"Ross, Cliff",ur.014104062341.33,"Martin, Daniel L.",UNF,UNF,['pub.1146587311'],"['Corrigendum: Spatial Patterns of Thalassia testudinum Immune Status and Labyrinthula spp. Load Implicate Environmental Quality and History as Modulators of Defense Strategies and Wasting Disease in Florida Bay, United States']",['[This corrects the article DOI: 10.3389/fpls.2021.612947.].']
ur.0701575222.69,"Ross, Cliff",ur.01230272013.16,"Duffin, Paige",UNF,UNF,['pub.1146587311'],"['Corrigendum: Spatial Patterns of Thalassia testudinum Immune Status and Labyrinthula spp. Load Implicate Environmental Quality and History as Modulators of Defense Strategies and Wasting Disease in Florida Bay, United States']",['[This corrects the article DOI: 10.3389/fpls.2021.612947.].']
ur.01230272013.16,"Duffin, Paige",ur.014104062341.33,"Martin, Daniel L.",UNF,UNF,['pub.1146587311'],"['Corrigendum: Spatial Patterns of Thalassia testudinum Immune Status and Labyrinthula spp. Load Implicate Environmental Quality and History as Modulators of Defense Strategies and Wasting Disease in Florida Bay, United States']",['[This corrects the article DOI: 10.3389/fpls.2021.612947.].']
ur.01230272013.16,"Duffin, Paige",ur.0701575222.69,"Ross, Cliff",UNF,UNF,['pub.1146587311'],"['Corrigendum: Spatial Patterns of Thalassia testudinum Immune Status and Labyrinthula spp. Load Implicate Environmental Quality and History as Modulators of Defense Strategies and Wasting Disease in Florida Bay, United States']",['[This corrects the article DOI: 10.3389/fpls.2021.612947.].']
ur.01241754627.12,"Haley, D. Rob",ur.01160234637.90,"Zhao, Mei",UNF,UNF,"['pub.1154029201', 'pub.1163109713']","['COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","['The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.01241754627.12,"Haley, D. Rob",ur.01302000265.19,"Hamadi, Hanadi Y.",UNF,UNF,"['pub.1154029201', 'pub.1163109713']","['COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","['The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.01241754627.12,"Haley, D. Rob",ur.016425135625.29,"Xu, Jing",UNF,UNF,"['pub.1154029201', 'pub.1163109713']","['COVID-19 Deaths and the Impact of Health Disparities, Hospital Characteristics, Community, Social Distancing, and Health System Competition', 'Hospital COVID-19 preparedness: Are (were) we ready?']","['The United States has one of the highest cumulative mortalities of coronavirus disease 2019 (COVID-19) and has reached 1 million deaths as of May 19th, 2022. Understanding which community and hospital factors contributed to disparities in COVID-19 mortality is important to inform public health strategies. This study aimed to explore the potential relationship between hospital service area (1) community (ie, health professional shortage areas, market competition, and uninsured percentage) and (2) hospital (ie, teaching, system, and ownership status) characteristics (2013-2018) on publicly available COVD-19 (February to October 2020) mortality data. The study included 2514 health service areas and used multilevel mixed-effects linear model to account for the multilevel data structure. The outcome measure was the number of COVID-19 deaths. This study found that public health, as opposed to acute care provision, was associated with community health and, ultimately, COVID-19 mortality. The study found that population characteristics including more uninsured greater proportion of those over 65 years, more diverse populations, and larger populations were all associated with a higher rate of death. In addition, communities with fewer hospitals were associated with a lower rate of death. When considering region in the United States, the west region showed a higher rate of death than all other regions. The association between some community characteristics and higher COVID-19 deaths demonstrated that access to health care, either for COVID-19 infection or worse health from higher disease burden, is strongly associated with COVID-19 deaths. Thus, to be better prepared for potential future pandemics, a greater emphasis on public health infrastructure is needed.', ""BACKGROUND: Terrorist attacks and natural disasters such as Hurricanes Katrina and Harvey have increased focus on disaster preparedness planning. Despite the attention on planning, many studies have found that hospitals in the United States are underprepared to manage extended disasters appropriately and the surge in patient volume it might bring.\r\nAIM: This study aims to profile and examine the availability of hospital capacity specifically related to COVID-19 patients, such as emergency department (ED) beds, intensive care unit (ICU) beds, temporary space setup, and ventilators.\r\nMETHOD: A cross-sectional retrospective study design was used to examine secondary data from the 2020 American Hospital Association (AHA) Annual Survey. A series of multivariate logistic analyses were conducted to investigate the strength of association between changes in ED beds, ICU beds, staffed beds, and temporary spaces setup, and the 3,655 hospitals' characteristics.\r\nRESULTS: Our results highlight that the odds of a change in ED beds are 44 percent lower for government hospitals and 54 percent for for-profit hospitals than not-for-profit hospitals. The odds of ED bed change for nonteaching hospitals were 34 percent lower compared to teaching hospitals. Small and medium hospitals have significantly lower odds (75 and 51 percent, respectively) than large hospitals. For ICU bed change, staffed bed change, and temporary spaces setup, the conclusions were consistently significant regarding the impact of hospital ownership, teaching status, and hospital size. However, temporary spaces setup differs by hospital location. The odds of change is significantly lower (OR = 0.71) in urban hospitals compared with rural hospitals, while for ED beds, the odds of change is considerably higher (OR = 1.57) in urban hospitals compared to rural hospitals.\r\nCONCLUSION: There is a need for policymakers to consider not only resource limitations that were created from supply line disruptions during the COVID-19 pandemic but also a more global assessment of the adequacy of funding and support for insurance coverage, hospital finance, and how hospitals meet the needs of the populations they serve.""]"
ur.010466517521.57,"Miller, J. Mitchell",ur.011646271766.59,"Miller, Holly Ventura",UNF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.010466517521.57,"Miller, J. Mitchell",ur.016213755653.16,"Vose, Brenda",UNF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.010466517521.57,"Miller, J. Mitchell",ur.012266061425.16,"Koskinen, Stephanie M.",UNF,UF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.010466517521.57,"Miller, J. Mitchell",ur.012123446530.06,"Jossie, McKenzie L.",UNF,UF,"['pub.1152787034', 'pub.1153668756']","['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States', 'COVID, Crime & Criminal Justice: Affirming the Call for System Reform Research']","['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.', 'Early into the COVID-19 pandemic, Miller & Blumstein (2020) outlined a theoretical research program (TRP) oriented around themes of contagion control and containment, legal amnesty, system leniency, nonenforcement, and tele-justice. Here, two and a half years later, these lingering themes are revisited to advocate for empirical research informing criminal justice system reform. The pandemic created rare natural experiment research conditions that enable unique and potentially valuable insights on necessitated innovations that may indicate future justice practices and policies. Given the sweeping effects of the shutdown, examples are numerous ranging from staffing analyses to estimate agencies’ personnel needs to ensure that basic public safety functions can be met after early retirements and resignations from virus risk and anti-police sentiment, the use of virtual communication in various legal proceedings at arrest, incarceration, and release junctures, and, especially, the risks versus benefits of early release. In addition to better identifying who should be jailed pre-trial, prioritization of calls for service, triaging of court cases, and hygiene and sanitation issues within facilities are other important examples central to a COVID and crime TRP. Attending research could demonstrate the utility of normative operations and identify shortfalls to be addressed during anomic conditions prior to another shutdown or similar event and present, through comparison of innovative and traditional derived outcomes, system reform and improvement opportunities. By seizing upon rare data made possible by natural experimental COVID generated conditions, researchers can meaningfully investigate the ongoing applicability of justice system adaptations mandated by the pandemic in terms of effectiveness and efficiency toward the interrelated goals of evidence-based practice discovery and justice reform.']"
ur.0663242321.49,"Mashanov, Vladimir",ur.012060030471.15,"Whaley, Lauren",UNF,UNF,['pub.1147049043'],['A subterminal growth zone at arm tip likely underlies life-long indeterminate growth in brittle stars'],"['BackgroundEchinoderms are a phylum of marine invertebrates with close phylogenetic relationships to chordates. Many members of the phylum Echinodermata are capable of extensive post-traumatic regeneration and life-long indeterminate growth. Different from regeneration, the life-long elongation of the main body axis in adult echinoderms has received little attention. The anatomical location and the nature of the dividing progenitor cells contributing to adults’ growth is unknown.ResultsWe show that the proliferating cells that drive the life-long growth of adult brittle star arms are mostly localized to the subterminal (second from the tip) arm segment. Each of the major anatomical structures contains dividing progenitors. These structures include: the radial nerve, water-vascular canal, and arm coelomic wall. Some of those proliferating progenitor cells are capable of multiple rounds of cell division. Within the nervous system, the progenitor cells were identified as a subset of radial glial cells that do not express Brn1/2/4, a transcription factor with a conserved role in the neuronal fate specification. In addition to characterizing the growth zone and the nature of the precursor cells, we provide a description of the microanatomy of the four distal-most arm segments contrasting the distal with the proximal segments, which are more mature.ConclusionsThe growth of the adult brittle star arms occurs via proliferation of progenitor cells in the distal segments, which are most abundant in the second segment from the tip. At least some of the progenitors are capable of multiple rounds of cell division. Within the nervous system the dividing cells were identified as Brn1/2/4-negative radial glial cells.']"
ur.0663242321.49,"Mashanov, Vladimir",ur.01132340615.34,"Hahn, Daniel A.",UNF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.0663242321.49,"Mashanov, Vladimir",ur.01032644343.06,"Short, Clancy A.",UNF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.0663242321.49,"Mashanov, Vladimir",ur.01134664434.25,"Jahan-Mihan, Alireza",UNF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.0663242321.49,"Mashanov, Vladimir",ur.0626416374.68,"Hatle, John D.",UNF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.012060030471.15,"Whaley, Lauren",ur.0663242321.49,"Mashanov, Vladimir",UNF,UNF,['pub.1147049043'],['A subterminal growth zone at arm tip likely underlies life-long indeterminate growth in brittle stars'],"['BackgroundEchinoderms are a phylum of marine invertebrates with close phylogenetic relationships to chordates. Many members of the phylum Echinodermata are capable of extensive post-traumatic regeneration and life-long indeterminate growth. Different from regeneration, the life-long elongation of the main body axis in adult echinoderms has received little attention. The anatomical location and the nature of the dividing progenitor cells contributing to adults’ growth is unknown.ResultsWe show that the proliferating cells that drive the life-long growth of adult brittle star arms are mostly localized to the subterminal (second from the tip) arm segment. Each of the major anatomical structures contains dividing progenitors. These structures include: the radial nerve, water-vascular canal, and arm coelomic wall. Some of those proliferating progenitor cells are capable of multiple rounds of cell division. Within the nervous system, the progenitor cells were identified as a subset of radial glial cells that do not express Brn1/2/4, a transcription factor with a conserved role in the neuronal fate specification. In addition to characterizing the growth zone and the nature of the precursor cells, we provide a description of the microanatomy of the four distal-most arm segments contrasting the distal with the proximal segments, which are more mature.ConclusionsThe growth of the adult brittle star arms occurs via proliferation of progenitor cells in the distal segments, which are most abundant in the second segment from the tip. At least some of the progenitors are capable of multiple rounds of cell division. Within the nervous system the dividing cells were identified as Brn1/2/4-negative radial glial cells.']"
ur.014472453705.75,"Quinn, Nathan",ur.01302000265.19,"Hamadi, Hanadi Y",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.014472453705.75,"Quinn, Nathan",ur.0640163054.49,"Merten, Julie W",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.014472453705.75,"Quinn, Nathan",ur.011732133420.33,"Terrell, Kassie R",UNF,UNF,"['pub.1147149952', 'pub.1150296139', 'pub.1152283195']","['Exploring life stressors, depression, and coping strategies in college students.', 'What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses."", nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.014472453705.75,"Quinn, Nathan",ur.015534510336.20,"Boggs, C.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.014472453705.75,"Quinn, Nathan",ur.07627017536.46,"Rule, M.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.014472453705.75,"Quinn, Nathan",ur.0634752264.04,"Zeglin, R.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.014472453705.75,"Quinn, Nathan",ur.011252000536.58,"Hicks-Roof, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011732133420.33,"Terrell, Kassie R",ur.01302000265.19,"Hamadi, Hanadi Y",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.011732133420.33,"Terrell, Kassie R",ur.014472453705.75,"Quinn, Nathan",UNF,UNF,"['pub.1147149952', 'pub.1150296139', 'pub.1152283195']","['Exploring life stressors, depression, and coping strategies in college students.', 'What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses."", nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011732133420.33,"Terrell, Kassie R",ur.0640163054.49,"Merten, Julie W",UNF,UNF,['pub.1147149952'],"['Exploring life stressors, depression, and coping strategies in college students.']","[""<b>Objective:</b> College students experience a variety of stressors that can increase the risk for mental health concerns, like depression. It is crucial for practitioners working on college campuses to understand the relationship among stressful life events, depression, and coping strategies. The purpose of this study was to explore life stressors' impact on reported depressive symptoms and how adaptive and maladaptive coping strategies moderate that relationship in college students. <b>Participants:</b> Data was used from a comprehensive health behavior survey. Participants included 969 college students. <b>Methods:</b> Multivariable logistic models were used to examine the association between stressful events, depression, and coping strategies. <b>Results:</b> Results from multiple logistic regression analyses indicated that college students who experienced life stressors and participated in more negative than positive coping strategies were 2.49 (95% CI = 1.34, 4.63) times more likely to experience depression. <b>Conclusions:</b> Implications and creative interventions are provided for mental health practitioners working on college campuses.""]"
ur.011732133420.33,"Terrell, Kassie R",ur.015534510336.20,"Boggs, C.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011732133420.33,"Terrell, Kassie R",ur.07627017536.46,"Rule, M.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011732133420.33,"Terrell, Kassie R",ur.0634752264.04,"Zeglin, R.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.011732133420.33,"Terrell, Kassie R",ur.011252000536.58,"Hicks-Roof, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.010104374107.55,"Waddell, David S",ur.07407035211.33,"Tello, Tala Shourbagi",UNF,UNF,['pub.1147552473'],['Identification and Characterization TSSK6 Activating Co‐chaperone (TSACC) in Skeletal Muscle'],"['Skeletal muscle atrophy is a physiological condition that is caused by a range of conditions, including immobilization, denervation, spinal cord injury, and aging. To better characterize the molecular genetic events of neurogenic atrophy, a previous study isolated the gastrocnemius muscle from mice following 3 and 14 days of sciatic nerve transection. The gene expression profile in the denervated muscle tissue was then analyzed by microarray and compared to control muscle tissue to identify novel neurogenic atrophy‐induced genes. The microarray data revealed for the first time that TSSK6 Activating Co‐chaperone (TSACC) is expressed in skeletal muscle and is significantly induced in response to denervation. To confirm that TSACC is expressed in muscle, the predicted TSACC cDNA was cloned from cultured muscle cells, as was a novel TSACC splice variant that contains an additional exon. Quantitative PCR (qPCR) was performed to assess TSACC expression levels of the full‐length and novel splice variants in proliferating and differentiated muscle cells. The results demonstrate that TSACC expression levels are relatively low in proliferating myoblasts but show significantly elevated expression in differentiated myotubes. In addition, characterization of the transcriptional regulation of TSACC was assessed by fusing fragments of the proximal promoter located immediately upstream of the start of transcription with a reporter gene. The reporter plasmids were then transfected into C2C12 mouse muscle cells in combination with myogenic regulatory factor (MRF) expression plasmids, which resulted in significant activation of reporter gene activity. Interestingly, there are several predicted E‐box elements in the proximal promoter region of the TSACC gene further suggesting that TSACC may be regulated by muscle‐specific transcription factors. To identify a possible role for TSACC in skeletal muscle, sub‐cellular localization was assessed by fusing TSACC with GFP and expressing the fusion protein in cultured muscle cells, revealing that TSACC is localized predominantly to the cytoplasm. The discovery that TSACC is induced in response to neurogenic atrophy helps further our understanding of the molecular genetic events of muscle wasting and may eventually lead to the identification of new therapeutic targets for the treatment of muscle atrophy.']"
ur.010104374107.55,"Waddell, David S",ur.07424362225.52,"Gjoka, Erisa",UNF,UNF,['pub.1147562878'],['Identification and Characterization of Calcium Binding and Coiled Coil Domain 1 (Calcoco1) in Skeletal Muscle'],"['Skeletal muscle atrophy results in loss of muscle mass and reduced strength and is caused by disparate physiological conditions including aging, cancer, corticosteroid use, and denervation. To better understand the molecular genetic events that lead to muscle atrophy, a previous study isolated skeletal muscle from mice following 3 and 14 days of denervation. Gene expression was then analyzed by microarray and compared to control muscle to identify novel, atrophy‐induced genes. The microarray revealed that Calcium Binding and Coiled Coil Domain 1 (Calcoco1) is expressed in skeletal muscle and induced in response to denervation. To confirm that Calcoco1 is expressed in skeletal muscle, the cDNA was successfully amplified and cloned from cultured C2C12 cells. Moreover, quantitative PCR was conducted using RNA isolated from proliferating and differentiated muscle cells to determine the expression profile of Calcoco1 at the transcriptional level. We observed moderate activation of Calcoco1 through myoblast proliferation and early differentiation, followed by a robust increase in activation by the later stages of myotube differentiation. These results were mirrored at the protein level where it was observed that Calcoco1 is expressed during early proliferation and increases as muscle cell differentiation progresses. To elucidate a functional role for Calcoco1 in muscle, we transfected cultured muscle cells with a Calcoco1 expression plasmid and then harvested the cells at timepoints ranging from proliferation through late differentiation. The cell lysates were probed by Western blot for markers of muscle cell differentiation, including Myosin Heavy Chain (MyHC) and myogenin, which both show significant repression in response to Calcoco1 overexpression. The Calcoco1 protein is predicted to contain both a calcium binding domain and a coiled‐coiled domain, suggesting that it may play a role in protein‐protein interactions or as a putative transcription factor. To investigate these possibilities, we sought to determine the sub‐cellular localization of Calcoco1 in muscle cells by fusing Calcoco1 cDNA to Green Fluorescent Protein (GFP) and expressing it in muscle cells. Visualization by confocal microscopy revealed clear nuclear‐exclusion of Calcoco1 protein in unchallenged myoblast cells, suggesting that it may not participate directly in gene regulation. The discovery that Calcoco1 is expressed in skeletal muscle and is induced in response to neurogenic atrophy helps further our understanding of the molecular and cellular events of muscle wasting and may eventually contribute to the identification of new therapeutic targets for the treatment and of muscle atrophy.']"
ur.01213347266.19,"Sealey‐Potts, Claudia",ur.015043716424.49,"Van Horn, Leslie Thompson",UNF,UNF,"['pub.1147495410', 'pub.1151572301']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01213347266.19,"Sealey‐Potts, Claudia",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,"['pub.1147495410', 'pub.1151572301']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01213347266.19,"Sealey‐Potts, Claudia",ur.01025441731.01,"Wright, Lauri",UNF,UNF,"['pub.1147495410', 'pub.1150304586', 'pub.1151572301']","['Validity and reliability of a questionnaire measuring EBDPs among registered dietitian nutritionist', 'Nutrition Interventions Are Needed for Dementia Informal Caregivers', 'Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists']","[""BACKGROUND: There is a lack of a valid and reliable instrument that measures objective and subjective knowledge of evidence-based dietetic practices (EBDP) among registered dietitian nutritionists (RDNs). The present study aimed to develop and assess the validity and reliability of an EBDP Questionnaire (EBDPQ) with objective knowledge items (i.e., quiz items) among RDNs in the USA.\r\nMETHODS: Subscales from four existing evidence-based practice (EBP) instruments were combined and modified for dietetics. Content and face validity and internal consistency were used to assess the full survey. Survey responsiveness and test-retest reliability were evaluated within the objective knowledge subscale. Content experts (n\u2009=\u200911) and nutrition professionals (n\u2009=\u200916) completed validation reviews. Doctoral students enrolled in a research course (n\u2009=\u200912) were used to analyse survey responsiveness. Internal and test-retest relability analyses utilised RDN participants (time point 1, n\u2009=\u2009482; time point 2, n\u2009=\u2009335).\r\nRESULTS: Content validation resulted in a 38-item questionnaire. Average percent agreement among face validity reviewers was 95.1%. Only the pre-validation version of the objective knowledge subscale resulted in significantly higher post-course scores (Mdn\u2009=\u200911.50) compared to the pre-course (Mdn\u2009=\u20099.75, p\u2009=\u20090.05). The validated instrument had excellent internal consistency (Cronbach's α\u2009=\u20090.91); however, the objective knowledge subscale was low (Cronbach's α\u2009=\u20090.41). A good degree of reliability was found between the two time points (intraclass correlation coefficient\u2009=\u20090.71).\r\nCONCLUSIONS: The EBDPQ demonstrated adequate validity and reliability among RDNs. Future research should assess construct validity, with the responsiveness and objective knowledge subscale requiring additional evaluation through an EBDP course."", nan, ""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.01213347266.19,"Sealey‐Potts, Claudia",ur.01035031577.44,"Witherspoon, D.",UNF,UNF,['pub.1150298573'],['Dietary and Lifestyle Practices of Individuals with Substantial Cardiovascular Risk Indicators'],[nan]
ur.01213347266.19,"Sealey‐Potts, Claudia",ur.012121456246.61,"Waterman, A.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.01213347266.19,"Sealey‐Potts, Claudia",ur.010732040045.21,"Labyak, C.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.01213347266.19,"Sealey‐Potts, Claudia",ur.01302000265.19,"Hamadi, Hanadi",UNF,UNF,['pub.1151572301'],['Determinant factors associated with the use of evidence‐based dietetics practice among registered dietitian nutritionists'],"[""AIMS: There is a lack of research assessing perceived and actual evidence-based dietetics practice knowledge among registered dietitian nutritionists. This cross-sectional research examined the association between determinant factors (i.e., education attainment) and objective knowledge (i.e., quiz items) of evidence-based dietetics practices. Other determinant factors associated with perceived knowledge and skill, attitudes, behaviours and outcomes related to evidence-based dietetics practice were also assessed. Perceived knowledge items were compared to objective knowledge items to evaluate the accuracy of self-assessed evidence-based dietetics practice knowledge among dietitians in the United States.\r\nMETHODS: A modified and adapted evidence-based practice survey was emailed to registered dietitian nutritionists in the United States via the Commission on Dietetic Registration database between November and December 2020. Analysis of variance assessed the relationship between highest degree held and actual knowledge of evidence-based dietetics practices. Multiple linear regression and ordinal logistic regression determined participant-level characteristics associated with survey subscales and survey items, respectively. Pearson's correlation coefficient evaluated the relationship between subjective and objective items.\r\nRESULTS: Four hundred and eighty-two dietitians completed the survey. Each increase in degree type was associated with an increase in objective knowledge score (all comparisons p\u2009<\u20090.001). Other determinants included recency of degree completion and peer-reviewed publications. Moderate positive associations (r\xa0=\xa00.35, p\u2009<\u20090.001) were found between subjective and objective measures.\r\nCONCLUSION: Graduate-level education, recency of degree completion and peer-reviewed publications were supportive of evidence-based dietetics practice-related outcomes. Future research should evaluate the efficacy of training, particularly among those further from their highest completed degree.""]"
ur.07407035211.33,"Tello, Tala Shourbagi",ur.010104374107.55,"Waddell, David",UNF,UNF,['pub.1147552473'],['Identification and Characterization TSSK6 Activating Co‐chaperone (TSACC) in Skeletal Muscle'],"['Skeletal muscle atrophy is a physiological condition that is caused by a range of conditions, including immobilization, denervation, spinal cord injury, and aging. To better characterize the molecular genetic events of neurogenic atrophy, a previous study isolated the gastrocnemius muscle from mice following 3 and 14 days of sciatic nerve transection. The gene expression profile in the denervated muscle tissue was then analyzed by microarray and compared to control muscle tissue to identify novel neurogenic atrophy‐induced genes. The microarray data revealed for the first time that TSSK6 Activating Co‐chaperone (TSACC) is expressed in skeletal muscle and is significantly induced in response to denervation. To confirm that TSACC is expressed in muscle, the predicted TSACC cDNA was cloned from cultured muscle cells, as was a novel TSACC splice variant that contains an additional exon. Quantitative PCR (qPCR) was performed to assess TSACC expression levels of the full‐length and novel splice variants in proliferating and differentiated muscle cells. The results demonstrate that TSACC expression levels are relatively low in proliferating myoblasts but show significantly elevated expression in differentiated myotubes. In addition, characterization of the transcriptional regulation of TSACC was assessed by fusing fragments of the proximal promoter located immediately upstream of the start of transcription with a reporter gene. The reporter plasmids were then transfected into C2C12 mouse muscle cells in combination with myogenic regulatory factor (MRF) expression plasmids, which resulted in significant activation of reporter gene activity. Interestingly, there are several predicted E‐box elements in the proximal promoter region of the TSACC gene further suggesting that TSACC may be regulated by muscle‐specific transcription factors. To identify a possible role for TSACC in skeletal muscle, sub‐cellular localization was assessed by fusing TSACC with GFP and expressing the fusion protein in cultured muscle cells, revealing that TSACC is localized predominantly to the cytoplasm. The discovery that TSACC is induced in response to neurogenic atrophy helps further our understanding of the molecular genetic events of muscle wasting and may eventually lead to the identification of new therapeutic targets for the treatment of muscle atrophy.']"
ur.07424362225.52,"Gjoka, Erisa",ur.010104374107.55,"Waddell, David",UNF,UNF,['pub.1147562878'],['Identification and Characterization of Calcium Binding and Coiled Coil Domain 1 (Calcoco1) in Skeletal Muscle'],"['Skeletal muscle atrophy results in loss of muscle mass and reduced strength and is caused by disparate physiological conditions including aging, cancer, corticosteroid use, and denervation. To better understand the molecular genetic events that lead to muscle atrophy, a previous study isolated skeletal muscle from mice following 3 and 14 days of denervation. Gene expression was then analyzed by microarray and compared to control muscle to identify novel, atrophy‐induced genes. The microarray revealed that Calcium Binding and Coiled Coil Domain 1 (Calcoco1) is expressed in skeletal muscle and induced in response to denervation. To confirm that Calcoco1 is expressed in skeletal muscle, the cDNA was successfully amplified and cloned from cultured C2C12 cells. Moreover, quantitative PCR was conducted using RNA isolated from proliferating and differentiated muscle cells to determine the expression profile of Calcoco1 at the transcriptional level. We observed moderate activation of Calcoco1 through myoblast proliferation and early differentiation, followed by a robust increase in activation by the later stages of myotube differentiation. These results were mirrored at the protein level where it was observed that Calcoco1 is expressed during early proliferation and increases as muscle cell differentiation progresses. To elucidate a functional role for Calcoco1 in muscle, we transfected cultured muscle cells with a Calcoco1 expression plasmid and then harvested the cells at timepoints ranging from proliferation through late differentiation. The cell lysates were probed by Western blot for markers of muscle cell differentiation, including Myosin Heavy Chain (MyHC) and myogenin, which both show significant repression in response to Calcoco1 overexpression. The Calcoco1 protein is predicted to contain both a calcium binding domain and a coiled‐coiled domain, suggesting that it may play a role in protein‐protein interactions or as a putative transcription factor. To investigate these possibilities, we sought to determine the sub‐cellular localization of Calcoco1 in muscle cells by fusing Calcoco1 cDNA to Green Fluorescent Protein (GFP) and expressing it in muscle cells. Visualization by confocal microscopy revealed clear nuclear‐exclusion of Calcoco1 protein in unchallenged myoblast cells, suggesting that it may not participate directly in gene regulation. The discovery that Calcoco1 is expressed in skeletal muscle and is induced in response to neurogenic atrophy helps further our understanding of the molecular and cellular events of muscle wasting and may eventually contribute to the identification of new therapeutic targets for the treatment and of muscle atrophy.']"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.01132340615.34,"Hahn, Daniel A.",UNF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.01032644343.06,"Short, Clancy A.",UNF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.0663242321.49,"Mashanov, Vladimir S.",UNF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.0626416374.68,"Hatle, John D.",UNF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.013621327420.63,"Galena, Amy E.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.014613704314.11,"Perez, Doreen",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.0636665472.55,"Bednarzyk, Michele",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.01226756046.26,"Ochrietor, Judith D.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01134664434.25,"Jahan-Mihan, Alireza",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.014574307507.32,"Stewart, Margaret",ur.01044120001.79,"Arnold, Christa",UNF,UNF,['pub.1147976632'],['Bridging the Gap for Online Deception Detection: Uncovering Methodology to Identify Deceptive Content in Mediated Communication'],"['Detecting deception online and in mediated communication is complex and has gained recognition due to misinformation. Traditionally most deception detection in communication relies on analysing nonverbal cues in body language and facial expression; mediated communication prohibits many of these cues from influencing the interpretation of message meaning. Given the ability for deceptive information to thrive online it becomes necessary to develop an effective method for digital deception data analysis. A method called Statement Analysis (SA) is commonly utilized in law enforcement and may be suitable for use as is or with modifications in mediated communication research. The goals of our ongoing research on mediated deception are three-fold: (1) uncover observations about mediated deception, specifically in social media posts from our current exploratory study, (2) to test and modify the current SA methodology within our current study for use and application within mediated communication contexts, and (3) to develop a textual deception detection methodology to apply within mediated communication. The findings of this exploratory research are presented within the content of examples that may be relevant to the creation and distribution of crisis messages, as the dissemination of misinformation in crisis events may be particularly critical.']"
ur.01044120001.79,"Arnold, Christa",ur.014574307507.32,"Stewart, Margaret",UNF,UNF,['pub.1147976632'],['Bridging the Gap for Online Deception Detection: Uncovering Methodology to Identify Deceptive Content in Mediated Communication'],"['Detecting deception online and in mediated communication is complex and has gained recognition due to misinformation. Traditionally most deception detection in communication relies on analysing nonverbal cues in body language and facial expression; mediated communication prohibits many of these cues from influencing the interpretation of message meaning. Given the ability for deceptive information to thrive online it becomes necessary to develop an effective method for digital deception data analysis. A method called Statement Analysis (SA) is commonly utilized in law enforcement and may be suitable for use as is or with modifications in mediated communication research. The goals of our ongoing research on mediated deception are three-fold: (1) uncover observations about mediated deception, specifically in social media posts from our current exploratory study, (2) to test and modify the current SA methodology within our current study for use and application within mediated communication contexts, and (3) to develop a textual deception detection methodology to apply within mediated communication. The findings of this exploratory research are presented within the content of examples that may be relevant to the creation and distribution of crisis messages, as the dissemination of misinformation in crisis events may be particularly critical.']"
ur.015327336032.09,"Bhullar, Amal",ur.013734375032.96,"Leon, Kalie G.",UF,UNF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.015327336032.09,"Bhullar, Amal",ur.0702507335.10,"Cuffe, Steven P.",UF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.015327336032.09,"Bhullar, Amal",ur.01117471340.02,"Dale, Lourdes P.",UF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.013734375032.96,"Leon, Kalie G.",ur.015327336032.09,"Bhullar, Amal",UNF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.013734375032.96,"Leon, Kalie G.",ur.0702507335.10,"Cuffe, Steven P.",UNF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.013734375032.96,"Leon, Kalie G.",ur.01117471340.02,"Dale, Lourdes P.",UNF,UF,"['pub.1148149919', 'pub.1148373848']","['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers', 'Childhood Maltreatment Influences Autonomic Regulation and Mental Health in College Students']","['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.', 'Childhood maltreatment history may influence autonomic reactivity and recovery to stressors. Hypothetically, the maltreatment history may contribute to a retuned autonomic nervous system that is reflected in a novel metric, vagal efficiency (VE), designed to assess the functional efficiency of vagal cardioinhibitory pathways on heart rate. We explored whether VE mediates the well-documented relationship between maltreatment history and psychiatric symptoms. We also investigated the relationship between measures of autonomic regulation in response to the physical and emotional challenges and psychiatric symptoms. Participants (<i>n</i> = 167) completed self-report measures of psychiatric symptoms and had continuous beat-to-beat heart rate monitored before, during, and after physical and emotional stressors. Participants with maltreatment histories exhibited lower VE, which mediated the association of maltreatment history and the psychiatric symptoms of anxiety and depression. Consistent with prior literature, there were significant associations between maltreatment history and autonomic reactivity (i.e., heart rate and respiratory sinus arrhythmia) during emotional and physical challenges; however, when VE was entered as a covariate these associations were no longer statistically significant. Blunted VE may reflect a neural pathway through which maltreatment retunes autonomic regulation and provides a neurophysiological platform that increases mental health risk.']"
ur.0702507335.10,"Cuffe, Steven P.",ur.015327336032.09,"Bhullar, Amal",UF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.0702507335.10,"Cuffe, Steven P.",ur.013734375032.96,"Leon, Kalie G.",UF,UNF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.0702507335.10,"Cuffe, Steven P.",ur.01117471340.02,"Dale, Lourdes P.",UF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.01117471340.02,"Dale, Lourdes P.",ur.015327336032.09,"Bhullar, Amal",UF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.01117471340.02,"Dale, Lourdes P.",ur.013734375032.96,"Leon, Kalie G.",UF,UNF,"['pub.1148149919', 'pub.1148373848']","['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers', 'Childhood Maltreatment Influences Autonomic Regulation and Mental Health in College Students']","['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.', 'Childhood maltreatment history may influence autonomic reactivity and recovery to stressors. Hypothetically, the maltreatment history may contribute to a retuned autonomic nervous system that is reflected in a novel metric, vagal efficiency (VE), designed to assess the functional efficiency of vagal cardioinhibitory pathways on heart rate. We explored whether VE mediates the well-documented relationship between maltreatment history and psychiatric symptoms. We also investigated the relationship between measures of autonomic regulation in response to the physical and emotional challenges and psychiatric symptoms. Participants (<i>n</i> = 167) completed self-report measures of psychiatric symptoms and had continuous beat-to-beat heart rate monitored before, during, and after physical and emotional stressors. Participants with maltreatment histories exhibited lower VE, which mediated the association of maltreatment history and the psychiatric symptoms of anxiety and depression. Consistent with prior literature, there were significant associations between maltreatment history and autonomic reactivity (i.e., heart rate and respiratory sinus arrhythmia) during emotional and physical challenges; however, when VE was entered as a covariate these associations were no longer statistically significant. Blunted VE may reflect a neural pathway through which maltreatment retunes autonomic regulation and provides a neurophysiological platform that increases mental health risk.']"
ur.01117471340.02,"Dale, Lourdes P.",ur.0702507335.10,"Cuffe, Steven P.",UF,UF,['pub.1148149919'],['Increased Autonomic Reactivity and Mental Health Difficulties in COVID-19 Survivors: Implications for Medical Providers'],"['Background: Because there is a relationship between mental health (MH) and medical adversity and autonomic dysregulation, we hypothesized that individuals infected with COVID-19 would report greater current autonomic reactivity and more MH difficulties (emotional distress, mindfulness difficulties, and posttraumatic stress). We also hypothesized that individuals diagnosed with COVID-19 who are experiencing difficulties related to their prior adversity and those providing medical care to COVID-19 patients would be more negatively impacted due to their increased stress and infection rates.\r\nMethod: US participants (<i>N</i> = 1,638; 61% female; Age <i>M</i> = 46.80) completed online self-report measures of prior adversity, current autonomic reactivity and current MH difficulties, and COVID-19 diagnosis history. Participants diagnosed with COVID-19 (<i>n</i> = 98) were more likely to be younger and providing medical care to COVID-19 patients.\r\nResults: Individuals diagnosed with COVID-19 reported increased current autonomic reactivity, being more negatively impacted by their prior MH/medical adversities, and currently experiencing more MH difficulties with an increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01 - <i>p</i> &lt; 0.001). Current autonomic reactivity mediated 58.9% to 85.2% of the relationship between prior adversity and current MH difficulties; and COVID-19 diagnosis moderated and enhanced the effect of prior adversity on current autonomic reactivity (<i>p</i> &lt; 0.01). Being a medical provider was associated with increased current autonomic reactivity (<i>p</i> &lt; 0.01), while moderating and enhancing the relationship between current autonomic reactivity and emotional distress and posttraumatic stress symptoms (<i>p</i> &lt; 0.05). Combining COVID-19 diagnosis with being a medical provider increased likelihood of clinically-significant PTSD and depression (<i>p</i> &lt; 0.01).\r\nConclusion: Individuals diagnosed with COVID-19, particularly medical providers, have increased current autonomic reactivity that is associated with their prior adversities and current MH difficulties.']"
ur.011210322067.23,"Moore, Michele J.",ur.0761527504.27,"Barr, Elissa",UNF,UNF,['pub.1148243201'],['Voter Support for Offering Sexual Health Services through School-Based Health Clinics'],"['School-based health clinics (SBHCs) offer convenient, free, and much-needed health services to adolescents who have been shown to lack access to preventative care and treatment. With high rates of risky sexual behavior and associated negative health outcomes, SBHCs are particularly effective in providing sexual health services. This article presents results of a survey assessing support for such services through SBHCs. Data were collected using computer-assisted telephone interviewing (CATI) software. With significant differences found for each demographic variable assessed, findings document that the majority of participants supported offering all four sexual health services in both middle and high schools: STI/HIV testing, STI/HIV treatment, pregnancy testing, and providing condoms. These findings may be used to reduce barriers in providing SBHCs, advocate for policy change regarding SBHCs, add or expand current SBHCs, and support others in conducting similar studies hoping to document local support to implement these and other sexual health services. Although limited research shows that using local data can impact policy and facilitate local change, more studies are needed to explore the impact of using local data to implement change and further evaluate the impact of sexual health services through SBHCs.']"
ur.0761527504.27,"Barr, Elissa",ur.011210322067.23,"Moore, Michele J.",UNF,UNF,['pub.1148243201'],['Voter Support for Offering Sexual Health Services through School-Based Health Clinics'],"['School-based health clinics (SBHCs) offer convenient, free, and much-needed health services to adolescents who have been shown to lack access to preventative care and treatment. With high rates of risky sexual behavior and associated negative health outcomes, SBHCs are particularly effective in providing sexual health services. This article presents results of a survey assessing support for such services through SBHCs. Data were collected using computer-assisted telephone interviewing (CATI) software. With significant differences found for each demographic variable assessed, findings document that the majority of participants supported offering all four sexual health services in both middle and high schools: STI/HIV testing, STI/HIV treatment, pregnancy testing, and providing condoms. These findings may be used to reduce barriers in providing SBHCs, advocate for policy change regarding SBHCs, add or expand current SBHCs, and support others in conducting similar studies hoping to document local support to implement these and other sexual health services. Although limited research shows that using local data can impact policy and facilitate local change, more studies are needed to explore the impact of using local data to implement change and further evaluate the impact of sexual health services through SBHCs.']"
ur.01312414645.39,"Clark, Kerry L",ur.010254305735.05,"Peterson, Breck",UNF,UNF,['pub.1148305228'],['Borrelia afzelii in an Ixodes ricinus encountered by a traveller in Ireland.'],[nan]
ur.01312414645.39,"Clark, Kerry L",ur.010605654236.42,"Marques, Mariana Fernandes",UNF,UNF,['pub.1152238101'],['Novel Rickettsia spp. in two common overwintering North American songbirds'],"['American robins and dark-eyed juncos migrate across North America and have been found to be competent hosts for some bacterial and viral pathogens, but their contributions to arthropod-borne diseases more broadly remain poorly characterized. Here, we sampled robins and juncos in multiple sites across North America for arthropod-borne bacterial pathogens of public health significance. We identified two novel <i>Rickettsia</i> spp. in one wintering migrant per bird species related to bellii, transitional, and spotted rickettsiae fever groups. Stable isotope analyses of feathers suggested spring migration of these common songbirds could disperse these novel rickettsiae hundreds-to-thousands of kilometers to host breeding grounds. Further work is needed to characterize zoonotic potential of these rickettsiae and host reservoir competence.']"
ur.01312414645.39,"Clark, Kerry L",ur.011403234636.91,"Nunez, Julissa Villegas",UNF,UNF,['pub.1152238101'],['Novel Rickettsia spp. in two common overwintering North American songbirds'],"['American robins and dark-eyed juncos migrate across North America and have been found to be competent hosts for some bacterial and viral pathogens, but their contributions to arthropod-borne diseases more broadly remain poorly characterized. Here, we sampled robins and juncos in multiple sites across North America for arthropod-borne bacterial pathogens of public health significance. We identified two novel <i>Rickettsia</i> spp. in one wintering migrant per bird species related to bellii, transitional, and spotted rickettsiae fever groups. Stable isotope analyses of feathers suggested spring migration of these common songbirds could disperse these novel rickettsiae hundreds-to-thousands of kilometers to host breeding grounds. Further work is needed to characterize zoonotic potential of these rickettsiae and host reservoir competence.']"
ur.010254305735.05,"Peterson, Breck",ur.01312414645.39,"Clark, Kerry L",UNF,UNF,['pub.1148305228'],['Borrelia afzelii in an Ixodes ricinus encountered by a traveller in Ireland.'],[nan]
ur.07577325317.14,"Triplett, Russell",ur.01164760644.22,"Loh, Chung-Ping A.",UNF,UNF,['pub.1148514912'],"['Consumer perception and information in a model of household water usage: The case of jacksonville, FL']","['Consumption of bottled water in the U.S. continues to grow despite the higher user price and greater environmental cost relative to municipal tap water. Convenience is surely one reason for this trend, but it is less relevant for in-home consumption of bottled water. The existing literature highlights perceptions of quality, access to information and personal experience as important factors influencing water usage in the home. In this paper we report the results of a 2018 survey of water customers of Jacksonville Electric Authority (JEA), the primary municipal water utility in Northeast Florida. The survey includes detailed questions regarding self-reported household water usage, information availability, information processing, trust in institutions and demographic characteristics. In addition, in cooperation with JEA, we matched the survey results with administrative data on geographic location within the system. Using a bivariate probit regression method, we estimate the determinants of water usage in the home. The results show that concern for drinking water safety is the principal contributor of bottled water consumption. Moreover, the evidence illustrates how information from water quality reports and objective measures of water hardness translate into the drinking water choice. We also show that greater transaction costs of bottled water due to low access to retail suppliers is associated with a substitution of water filtration for bottled water.']"
ur.07577325317.14,"Triplett, Russell",ur.0644663313.57,"Chatterjee, Chiradip",UNF,UNF,['pub.1148514912'],"['Consumer perception and information in a model of household water usage: The case of jacksonville, FL']","['Consumption of bottled water in the U.S. continues to grow despite the higher user price and greater environmental cost relative to municipal tap water. Convenience is surely one reason for this trend, but it is less relevant for in-home consumption of bottled water. The existing literature highlights perceptions of quality, access to information and personal experience as important factors influencing water usage in the home. In this paper we report the results of a 2018 survey of water customers of Jacksonville Electric Authority (JEA), the primary municipal water utility in Northeast Florida. The survey includes detailed questions regarding self-reported household water usage, information availability, information processing, trust in institutions and demographic characteristics. In addition, in cooperation with JEA, we matched the survey results with administrative data on geographic location within the system. Using a bivariate probit regression method, we estimate the determinants of water usage in the home. The results show that concern for drinking water safety is the principal contributor of bottled water consumption. Moreover, the evidence illustrates how information from water quality reports and objective measures of water hardness translate into the drinking water choice. We also show that greater transaction costs of bottled water due to low access to retail suppliers is associated with a substitution of water filtration for bottled water.']"
ur.015541421441.33,"Majidirad, AmirHossein",ur.014724677201.02,"May, Gokan",UNF,UNF,['pub.1148715395'],['A Semantic Model in the Context of Maintenance: A Predictive Maintenance Case Study'],"['Advanced technologies in modern industry collect massive volumes of data from a plethora of sources, such as processes, machines, components, and documents. This also applies to predictive maintenance. To provide access to these data in a standard and structured way, researchers and practitioners need to design and develop a semantic model of maintenance entities to build a reference ontology for maintenance. To date, there have been numerous studies combining the domain of predictive maintenance and ontology engineering. However, such earlier works, which focused on semantic interoperability to exchange data with standardized meanings, did not fully leverage the opportunities provided by data federation to elaborate these semantic technologies further. Therefore, in this paper, we fill this research gap by addressing interoperability in smart manufacturing and the issue of federating different data formats effectively by using semantic technologies in the context of maintenance. Furthermore, we introduce a semantic model in the form of an ontology for mapping relevant data. The proposed solution is validated and verified using an industrial implementation.']"
ur.015541421441.33,"Majidirad, AmirHossein",ur.07744420572.05,"Mitchell, Allyson",UNF,UNF,['pub.1149428016'],['The Effect of Lower Limb Exoskeleton Alignment on Knee Rehabilitation Efficacy'],"['This study focuses on a musculoskeletal analysis of human lower extremity and associated muscle forces during different rehabilitative tasks and exoskeleton alignment models. By changing the size and orientation of the impairment levels that could be caused by the misalignment of the exoskeleton and biological knee joint, muscle stress variations were observed. This indicates an increase in force such as that generated by the Vastus lateralis muscle up to 4.3% due to a 5 mm lateral offset from an anatomically healthy knee joint location. In another setting, while a subject moved the shank through a circular trajectory using an exoskeleton support, muscle strain due to misalignment was reflected at the rectus femoris with a variation of 44%, the biceps femoris large head with 32% and the gastrocnemius muscles with 31-33% variation. These results suggest that misalignment should be taken into account while using exoskeletons with certain trajectories for knee rehabilitation purposes. Based on the shortcomings of conventional physiotherapeutic tasks, the outcome of this study can be helpful in prescribing an impactful yet convenient configuration toward a safe and promising rehabilitation process. Assessment of exoskeleton alignment during rehabilitation is important to ensure user safety with a better therapy efficacy.']"
ur.014724677201.02,"May, Gokan",ur.015541421441.33,"Majidirad, AmirHossein",UNF,UNF,['pub.1148715395'],['A Semantic Model in the Context of Maintenance: A Predictive Maintenance Case Study'],"['Advanced technologies in modern industry collect massive volumes of data from a plethora of sources, such as processes, machines, components, and documents. This also applies to predictive maintenance. To provide access to these data in a standard and structured way, researchers and practitioners need to design and develop a semantic model of maintenance entities to build a reference ontology for maintenance. To date, there have been numerous studies combining the domain of predictive maintenance and ontology engineering. However, such earlier works, which focused on semantic interoperability to exchange data with standardized meanings, did not fully leverage the opportunities provided by data federation to elaborate these semantic technologies further. Therefore, in this paper, we fill this research gap by addressing interoperability in smart manufacturing and the issue of federating different data formats effectively by using semantic technologies in the context of maintenance. Furthermore, we introduce a semantic model in the form of an ontology for mapping relevant data. The proposed solution is validated and verified using an industrial implementation.']"
ur.016003073441.55,"Horton, John C.",ur.0746107425.44,"Alloway, Tracy Packiam",UNF,UNF,['pub.1148910234'],"['Time Perspective, Working Memory, and Depression in Non-Clinical Samples: Is There a Link?']","['Non-clinical depression is a major issue on college campuses, with some surveys estimating that 30% of college students have experienced a major depressive episode. One theoretical framework of depression is Zimbardo and Boyd (1999) time perspective model, which posits that our perspectives on time impact different aspects of life including our emotions, judgments, and decision making. The current study seeks to determine the role of this time perspectives model and a range of cognitive constructs including hope, rumination, and working memory on their influence in depression. Currently enrolled college students and participants not currently enrolled in college completed the Center for Epidemiologic Studies Depression Scale, the Zimbardo Time Perspective Inventory, the Adult Hope Scale, the Rumination Reflection Questionnaire, and the Automated Working Memory Assessment. Linear regression analysis revealed that, for the college students, Rumination and Past Negative scores predicted depressive symptoms. For the non-college students, Rumination, Present Fatalism, Hope Agency and Verbal Working Memory scores predicted depressive symptoms. The current results reiterate the importance of rumination in depression symptomology and that current cognitive depression models and treatments may benefit from including time perspective measures. Further implications of the results are discussed.']"
ur.0657343460.68,"Knuckley, Bryan",ur.01075777277.30,"Causey, Corey P.",UNF,UNF,['pub.1148971029'],['A peptoid-based inhibitor of protein arginine methyltransferase 1 (PRMT1) induces apoptosis and autophagy in cancer cells'],"['Protein arginine methyltransferases (PRMTs) are S-adenosylmethionine-dependent enzymes that transfer a methyl group to arginine residues within proteins, most notably histones. The nine characterized PRMT family members are divided into three types depending on the resulting methylated product: asymmetric dimethylarginine (Type I PRMT), symmetric dimethylarginine (Type II PRMT), or monomethylated arginine (Type III PRMT). In some cancers, the resulting product can lead to either increased or decreased transcription of cancer-related genes, suggesting PRMT family members may be valid therapeutic targets. Traditionally, peptide-based compounds have been employed to target this family of enzymes, which has resulted in multiple tool and lead compounds being developed. However, peptide-based therapeutics suffer from poor stability and short half-lives, as proteases can render them useless by hydrolytic degradation. Conversely, peptoids, which are peptide-mimetics composed of N-substituted glycine monomers, are less susceptible to hydrolysis, resulting in improved stability and longer half-lives. Herein, we report the development of a bioavailable, peptoid-based PRMT1 inhibitor that induces cell death in MDA468 and HCT116 cancer cell lines while not exhibiting any significant impact on nontumorigenic HepaRG or normal human mammary epithelial cells. Furthermore, the inhibitor described herein appears to induce both apoptosis and autophagy, suggesting it may be a less toxic cytostatic agent. In conclusion, we propose this peptoid-based inhibitor has significant anticancer and therapeutic potential by reducing cell viability, growth, and size in breast and colon cancer. Further experimentation will help determine the mechanism of action and downstream effects of this compound.']"
ur.0657343460.68,"Knuckley, Bryan",ur.015635457347.07,"Rehman, Fatima Khwaja",UNF,UNF,['pub.1148971029'],['A peptoid-based inhibitor of protein arginine methyltransferase 1 (PRMT1) induces apoptosis and autophagy in cancer cells'],"['Protein arginine methyltransferases (PRMTs) are S-adenosylmethionine-dependent enzymes that transfer a methyl group to arginine residues within proteins, most notably histones. The nine characterized PRMT family members are divided into three types depending on the resulting methylated product: asymmetric dimethylarginine (Type I PRMT), symmetric dimethylarginine (Type II PRMT), or monomethylated arginine (Type III PRMT). In some cancers, the resulting product can lead to either increased or decreased transcription of cancer-related genes, suggesting PRMT family members may be valid therapeutic targets. Traditionally, peptide-based compounds have been employed to target this family of enzymes, which has resulted in multiple tool and lead compounds being developed. However, peptide-based therapeutics suffer from poor stability and short half-lives, as proteases can render them useless by hydrolytic degradation. Conversely, peptoids, which are peptide-mimetics composed of N-substituted glycine monomers, are less susceptible to hydrolysis, resulting in improved stability and longer half-lives. Herein, we report the development of a bioavailable, peptoid-based PRMT1 inhibitor that induces cell death in MDA468 and HCT116 cancer cell lines while not exhibiting any significant impact on nontumorigenic HepaRG or normal human mammary epithelial cells. Furthermore, the inhibitor described herein appears to induce both apoptosis and autophagy, suggesting it may be a less toxic cytostatic agent. In conclusion, we propose this peptoid-based inhibitor has significant anticancer and therapeutic potential by reducing cell viability, growth, and size in breast and colon cancer. Further experimentation will help determine the mechanism of action and downstream effects of this compound.']"
ur.01075777277.30,"Causey, Corey P.",ur.0657343460.68,"Knuckley, Bryan",UNF,UNF,['pub.1148971029'],['A peptoid-based inhibitor of protein arginine methyltransferase 1 (PRMT1) induces apoptosis and autophagy in cancer cells'],"['Protein arginine methyltransferases (PRMTs) are S-adenosylmethionine-dependent enzymes that transfer a methyl group to arginine residues within proteins, most notably histones. The nine characterized PRMT family members are divided into three types depending on the resulting methylated product: asymmetric dimethylarginine (Type I PRMT), symmetric dimethylarginine (Type II PRMT), or monomethylated arginine (Type III PRMT). In some cancers, the resulting product can lead to either increased or decreased transcription of cancer-related genes, suggesting PRMT family members may be valid therapeutic targets. Traditionally, peptide-based compounds have been employed to target this family of enzymes, which has resulted in multiple tool and lead compounds being developed. However, peptide-based therapeutics suffer from poor stability and short half-lives, as proteases can render them useless by hydrolytic degradation. Conversely, peptoids, which are peptide-mimetics composed of N-substituted glycine monomers, are less susceptible to hydrolysis, resulting in improved stability and longer half-lives. Herein, we report the development of a bioavailable, peptoid-based PRMT1 inhibitor that induces cell death in MDA468 and HCT116 cancer cell lines while not exhibiting any significant impact on nontumorigenic HepaRG or normal human mammary epithelial cells. Furthermore, the inhibitor described herein appears to induce both apoptosis and autophagy, suggesting it may be a less toxic cytostatic agent. In conclusion, we propose this peptoid-based inhibitor has significant anticancer and therapeutic potential by reducing cell viability, growth, and size in breast and colon cancer. Further experimentation will help determine the mechanism of action and downstream effects of this compound.']"
ur.01075777277.30,"Causey, Corey P.",ur.015635457347.07,"Rehman, Fatima Khwaja",UNF,UNF,['pub.1148971029'],['A peptoid-based inhibitor of protein arginine methyltransferase 1 (PRMT1) induces apoptosis and autophagy in cancer cells'],"['Protein arginine methyltransferases (PRMTs) are S-adenosylmethionine-dependent enzymes that transfer a methyl group to arginine residues within proteins, most notably histones. The nine characterized PRMT family members are divided into three types depending on the resulting methylated product: asymmetric dimethylarginine (Type I PRMT), symmetric dimethylarginine (Type II PRMT), or monomethylated arginine (Type III PRMT). In some cancers, the resulting product can lead to either increased or decreased transcription of cancer-related genes, suggesting PRMT family members may be valid therapeutic targets. Traditionally, peptide-based compounds have been employed to target this family of enzymes, which has resulted in multiple tool and lead compounds being developed. However, peptide-based therapeutics suffer from poor stability and short half-lives, as proteases can render them useless by hydrolytic degradation. Conversely, peptoids, which are peptide-mimetics composed of N-substituted glycine monomers, are less susceptible to hydrolysis, resulting in improved stability and longer half-lives. Herein, we report the development of a bioavailable, peptoid-based PRMT1 inhibitor that induces cell death in MDA468 and HCT116 cancer cell lines while not exhibiting any significant impact on nontumorigenic HepaRG or normal human mammary epithelial cells. Furthermore, the inhibitor described herein appears to induce both apoptosis and autophagy, suggesting it may be a less toxic cytostatic agent. In conclusion, we propose this peptoid-based inhibitor has significant anticancer and therapeutic potential by reducing cell viability, growth, and size in breast and colon cancer. Further experimentation will help determine the mechanism of action and downstream effects of this compound.']"
ur.015635457347.07,"Rehman, Fatima Khwaja",ur.0657343460.68,"Knuckley, Bryan",UNF,UNF,['pub.1148971029'],['A peptoid-based inhibitor of protein arginine methyltransferase 1 (PRMT1) induces apoptosis and autophagy in cancer cells'],"['Protein arginine methyltransferases (PRMTs) are S-adenosylmethionine-dependent enzymes that transfer a methyl group to arginine residues within proteins, most notably histones. The nine characterized PRMT family members are divided into three types depending on the resulting methylated product: asymmetric dimethylarginine (Type I PRMT), symmetric dimethylarginine (Type II PRMT), or monomethylated arginine (Type III PRMT). In some cancers, the resulting product can lead to either increased or decreased transcription of cancer-related genes, suggesting PRMT family members may be valid therapeutic targets. Traditionally, peptide-based compounds have been employed to target this family of enzymes, which has resulted in multiple tool and lead compounds being developed. However, peptide-based therapeutics suffer from poor stability and short half-lives, as proteases can render them useless by hydrolytic degradation. Conversely, peptoids, which are peptide-mimetics composed of N-substituted glycine monomers, are less susceptible to hydrolysis, resulting in improved stability and longer half-lives. Herein, we report the development of a bioavailable, peptoid-based PRMT1 inhibitor that induces cell death in MDA468 and HCT116 cancer cell lines while not exhibiting any significant impact on nontumorigenic HepaRG or normal human mammary epithelial cells. Furthermore, the inhibitor described herein appears to induce both apoptosis and autophagy, suggesting it may be a less toxic cytostatic agent. In conclusion, we propose this peptoid-based inhibitor has significant anticancer and therapeutic potential by reducing cell viability, growth, and size in breast and colon cancer. Further experimentation will help determine the mechanism of action and downstream effects of this compound.']"
ur.015635457347.07,"Rehman, Fatima Khwaja",ur.01075777277.30,"Causey, Corey P.",UNF,UNF,['pub.1148971029'],['A peptoid-based inhibitor of protein arginine methyltransferase 1 (PRMT1) induces apoptosis and autophagy in cancer cells'],"['Protein arginine methyltransferases (PRMTs) are S-adenosylmethionine-dependent enzymes that transfer a methyl group to arginine residues within proteins, most notably histones. The nine characterized PRMT family members are divided into three types depending on the resulting methylated product: asymmetric dimethylarginine (Type I PRMT), symmetric dimethylarginine (Type II PRMT), or monomethylated arginine (Type III PRMT). In some cancers, the resulting product can lead to either increased or decreased transcription of cancer-related genes, suggesting PRMT family members may be valid therapeutic targets. Traditionally, peptide-based compounds have been employed to target this family of enzymes, which has resulted in multiple tool and lead compounds being developed. However, peptide-based therapeutics suffer from poor stability and short half-lives, as proteases can render them useless by hydrolytic degradation. Conversely, peptoids, which are peptide-mimetics composed of N-substituted glycine monomers, are less susceptible to hydrolysis, resulting in improved stability and longer half-lives. Herein, we report the development of a bioavailable, peptoid-based PRMT1 inhibitor that induces cell death in MDA468 and HCT116 cancer cell lines while not exhibiting any significant impact on nontumorigenic HepaRG or normal human mammary epithelial cells. Furthermore, the inhibitor described herein appears to induce both apoptosis and autophagy, suggesting it may be a less toxic cytostatic agent. In conclusion, we propose this peptoid-based inhibitor has significant anticancer and therapeutic potential by reducing cell viability, growth, and size in breast and colon cancer. Further experimentation will help determine the mechanism of action and downstream effects of this compound.']"
ur.07500421537.27,"Niemela, Danielle R. M.",ur.0634752264.04,"Zeglin, Robert J.",UNF,UNF,['pub.1149209947'],['Parents Talking to Middle School Children about Sex: A Protective Factor against Suicide in Sexually Active Teens'],"['Suicide continues to be a considerable health risk among adolescents and is the second leading cause of death among adolescents between the ages of 10 and 24. Middle school-aged adolescents may be at a heightened risk for suicide, as middle school can be rife with intrapersonal and interpersonal stressors. One such stressor may be sexual activity and navigating sexual relationships. Past research has shown that parents can play an important role in helping adolescents cope with stressors at this age, but there is no identified research assessing whether parental conversations about sex are associated with suicidality among adolescents. The current study addressed this gap via secondary data analysis using logistic regression with a sample of 3,568 middle school students (mean age = 12.74; SD = 1.08). Results suggest that parental conversations about sex are a significant protective factor against suicidality, but only among adolescents who report having engaged in sexual activity.']"
ur.011141050324.18,"Wesely, Jennifer K.",ur.014675651125.51,"Brown, Elizabeth R.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.011141050324.18,"Wesely, Jennifer K.",ur.012473703013.35,"Binder, Michael",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.011141050324.18,"Wesely, Jennifer K.",ur.0776454123.71,"Phills, Curtis E.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.01001151634.20,"Rosenblatt, Adam E.",ur.012703342762.52,"Spain, Marisa",UNF,UNF,"['pub.1149372342', 'pub.1152394204']","['American alligator (Alligator mississippiensis) weight gain in captivity and implications for captive reptile body condition', 'Social structure and habitat design affect the impact of a novel feeding enrichment for alligators']","['The body condition of an animal is an indicator of health status and is dependent upon many factors, some of which can vary between wild and captive settings. Despite this, there have not been many studies on how captivity affects body condition relative to wild animal populations. This study explores the body condition of captive and wild American alligators (Alligator mississippiensis) because reptiles are frequently overlooked in studies of captive animal health and because alligators are well-represented in captivity. We collected body condition data from 209 captive alligators and 935 wild alligators throughout Florida and southeastern Georgia and compared the relationships between body condition and body length for each group. We found that captive alligators exhibited significantly higher body condition values as they aged, and that this result was driven by the difference between captive and wild males. Body condition values for captive juveniles did not differ from wild juveniles, but they differed when comparing adults. Our results suggest that factors such as diet and movement rates play major roles in determining alligator body condition and that body condition may be an important metric for monitoring captive alligator health, especially for older adult males.', ""Providing enrichment that expands the range of behavioral opportunities associated with food acquisition and environmental exploration is an important contributing factor to the well-being of zoo animals. These behaviors can be difficult to promote in carnivores, given their foraging strategies and the logistical, ethical, and financial challenges of providing live prey. In this study, we introduced a novel feeding enrichment to Jacksonville Zoo and Gardens' five adult American alligators (Alligator mississippiensis) in an attempt to simulate a live prey organism within the exhibit and promote natural hunting behaviors like chasing and lunging, as well as increase daily activity levels. The enrichment promoted some behavioral goals for two of the alligators, but it did not promote behavioral goals for the other three alligators. This could have been due to a variety of factors including an existing dominance hierarchy amongst the group's females and the resulting spatial distribution of individuals across a habitat with only one water feature. Our results suggest that female alligators may carve out territories and avoid overlapping space usage with other females during the warmest months of the year. Given the outcomes and limitations of this enrichment strategy, we provide recommendations for this group specifically as well as future enrichment efforts in the general captive crocodilian population.""]"
ur.012703342762.52,"Spain, Marisa",ur.01001151634.20,"Rosenblatt, Adam E.",UNF,UNF,"['pub.1149372342', 'pub.1152394204']","['American alligator (Alligator mississippiensis) weight gain in captivity and implications for captive reptile body condition', 'Social structure and habitat design affect the impact of a novel feeding enrichment for alligators']","['The body condition of an animal is an indicator of health status and is dependent upon many factors, some of which can vary between wild and captive settings. Despite this, there have not been many studies on how captivity affects body condition relative to wild animal populations. This study explores the body condition of captive and wild American alligators (Alligator mississippiensis) because reptiles are frequently overlooked in studies of captive animal health and because alligators are well-represented in captivity. We collected body condition data from 209 captive alligators and 935 wild alligators throughout Florida and southeastern Georgia and compared the relationships between body condition and body length for each group. We found that captive alligators exhibited significantly higher body condition values as they aged, and that this result was driven by the difference between captive and wild males. Body condition values for captive juveniles did not differ from wild juveniles, but they differed when comparing adults. Our results suggest that factors such as diet and movement rates play major roles in determining alligator body condition and that body condition may be an important metric for monitoring captive alligator health, especially for older adult males.', ""Providing enrichment that expands the range of behavioral opportunities associated with food acquisition and environmental exploration is an important contributing factor to the well-being of zoo animals. These behaviors can be difficult to promote in carnivores, given their foraging strategies and the logistical, ethical, and financial challenges of providing live prey. In this study, we introduced a novel feeding enrichment to Jacksonville Zoo and Gardens' five adult American alligators (Alligator mississippiensis) in an attempt to simulate a live prey organism within the exhibit and promote natural hunting behaviors like chasing and lunging, as well as increase daily activity levels. The enrichment promoted some behavioral goals for two of the alligators, but it did not promote behavioral goals for the other three alligators. This could have been due to a variety of factors including an existing dominance hierarchy amongst the group's females and the resulting spatial distribution of individuals across a habitat with only one water feature. Our results suggest that female alligators may carve out territories and avoid overlapping space usage with other females during the warmest months of the year. Given the outcomes and limitations of this enrichment strategy, we provide recommendations for this group specifically as well as future enrichment efforts in the general captive crocodilian population.""]"
ur.07744420572.05,"Mitchell, Allyson",ur.015541421441.33,"MajidiRad, AmirHossein",UNF,UNF,['pub.1149428016'],['The Effect of Lower Limb Exoskeleton Alignment on Knee Rehabilitation Efficacy'],"['This study focuses on a musculoskeletal analysis of human lower extremity and associated muscle forces during different rehabilitative tasks and exoskeleton alignment models. By changing the size and orientation of the impairment levels that could be caused by the misalignment of the exoskeleton and biological knee joint, muscle stress variations were observed. This indicates an increase in force such as that generated by the Vastus lateralis muscle up to 4.3% due to a 5 mm lateral offset from an anatomically healthy knee joint location. In another setting, while a subject moved the shank through a circular trajectory using an exoskeleton support, muscle strain due to misalignment was reflected at the rectus femoris with a variation of 44%, the biceps femoris large head with 32% and the gastrocnemius muscles with 31-33% variation. These results suggest that misalignment should be taken into account while using exoskeletons with certain trajectories for knee rehabilitation purposes. Based on the shortcomings of conventional physiotherapeutic tasks, the outcome of this study can be helpful in prescribing an impactful yet convenient configuration toward a safe and promising rehabilitation process. Assessment of exoskeleton alignment during rehabilitation is important to ensure user safety with a better therapy efficacy.']"
ur.016706625772.77,"Stapleton, Jessica N",ur.01253770701.51,"Richardson, Michael R",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.016706625772.77,"Stapleton, Jessica N",ur.011027471255.87,"Zipperer, Madeline B",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.016706625772.77,"Stapleton, Jessica N",ur.01365551077.18,"Churilla, James R",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.011027471255.87,"Zipperer, Madeline B",ur.01253770701.51,"Richardson, Michael R",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.011027471255.87,"Zipperer, Madeline B",ur.016706625772.77,"Stapleton, Jessica N",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.011027471255.87,"Zipperer, Madeline B",ur.01365551077.18,"Churilla, James R",UNF,UNF,"['pub.1149596712', 'pub.1150787329']","['Joint effect of cognitive function and C-reactive protein on all-cause mortality risk: 1999–2002 NHANES', 'Potential Mediating Effects Of Social Support And Physical Activity On Cognitive Function And Mortality Risk']","['PURPOSE: Examine the joint effect of cognitive function and C-reactive protein (CRP) on all-cause mortality risk in older U.S. adults.\r\nMETHODS: Sample included 1335 older adult (≥60 years of age) participants in the 1999-2002 National Health and Nutrition Examination Survey. A four-level variable was created using cognitive function and CRP concentration. Mortality was assessed using National Center for Health Statistics linked death records from the National Death Index.\r\nRESULTS: Increased risk of all-cause mortality was revealed in adults with high CRP and low cognitive function and in those with low to average CRP and low cognitive function (P < .0001 for both). Sex-stratified analyses revealed increased all-cause mortality risk in males with low cognitive function, independent of CRP concentration. However, in females, a significant increase in all-cause mortality risk was only observed in those with low to average CRP and low cognitive function.\r\nCONCLUSIONS: Low cognitive function was associated with increased all-cause mortality risk independent of CRP concentration. However, the joint effect of cognitive function and CRP on all-cause mortality risk differed according to sex.', nan]"
ur.010015367551.61,"Dinsmore, Daniel L.",ur.012700413505.53,"Parkinson, Meghan M.",UNF,UNF,['pub.1149796823'],"['The learning styles hypothesis is false, but there are patterns of student characteristics that are useful']","['The learning styles hypothesis—and particularly the meshing hypothesis—state that learners’ preferences about their preferred modality of learning (i.e., visual, aural, or kinesthetic) predict learning gains on academic tasks. Despite the fact that this hypothesis is not borne out by the scientific evidence available to us, it still remains in widespread classroom use. This article begins by discussing the evidence against learning styles. Second, the article discusses why teachers might continue to believe in and use learning styles in their classroom as well as why essentialist beliefs about learning are not helpful. Finally, 3 variables that do impact student learning—knowledge, strategies, and interest are discussed. Each is defined, their development and measurements are discussed, and finally some instructional examples are given. Replacing the use of learning styles in the classroom with instructional decisions based on the development of knowledge, strategies, and interest can improve student learning outcomes across a wide range of subjects and grade levels.']"
ur.012700413505.53,"Parkinson, Meghan M.",ur.010015367551.61,"Dinsmore, Daniel L.",UNF,UNF,['pub.1149796823'],"['The learning styles hypothesis is false, but there are patterns of student characteristics that are useful']","['The learning styles hypothesis—and particularly the meshing hypothesis—state that learners’ preferences about their preferred modality of learning (i.e., visual, aural, or kinesthetic) predict learning gains on academic tasks. Despite the fact that this hypothesis is not borne out by the scientific evidence available to us, it still remains in widespread classroom use. This article begins by discussing the evidence against learning styles. Second, the article discusses why teachers might continue to believe in and use learning styles in their classroom as well as why essentialist beliefs about learning are not helpful. Finally, 3 variables that do impact student learning—knowledge, strategies, and interest are discussed. Each is defined, their development and measurements are discussed, and finally some instructional examples are given. Replacing the use of learning styles in the classroom with instructional decisions based on the development of knowledge, strategies, and interest can improve student learning outcomes across a wide range of subjects and grade levels.']"
ur.0741150741.82,"Nicholson, Jody S",ur.015444027474.43,"Villamor, Monique",UNF,UNF,['pub.1153822461'],['WHAT MOTIVATES PARTICIPATION IN ALZHEIMER’S PREVENTION RESEARCH: A MIXED-METHODS STUDY'],"['Abstract\r\n        <p>6.5 million Americans aged 65 and older are currently living with Alzheimer’s disease or related dementias (ADRD; Alzheimer’s Association, 2022). By 2060, it is estimated that this number will increase to 13.9 million (Matthews et al., 2018). Therefore, it is imperative to gain insight into participants’ personal motivations and expectations of research to advance community participation. The Preventing Alzheimer’s with Cognitive Training (PACT) study is a National Institute of Health, National Institute on Aging-funded, multi-site clinical trial examining the prevention of mild cognitive impairment and ADRD through computer-based cognitive training. Across 5 locations, data were collected from 2,360 cognitively normal participants (M=73.03 years, range=65–97, SD=5.04). The current project explores individuals’ motivations and expectations of cognitive training (CT) utilizing a mixed-method approach by coding qualitative open-ended questions about motivation to participate and comparing how motivational themes aligned with expectations about CT from the Expectations Assessment Scale (EAS; Rabipour et al., 2018). Six themes for participant motivation emerged: direct experience with the disease (26.9%), concern about brain health and aging (23.9%), general personal interest (17.9%), general interest in research (20.1%), referral to the study (5.8%), and altruism (5.4%). After completing the initial training session, motivation themes did not differentiate satisfaction with (p=.06) or perceived success of (p=.11) the CT program. Understanding participants’ motivations can further expand and optimize recruitment and retention strategies in AD prevention research. Future research will focus on how these themes influence adherence and retention and relate to participant demographic characteristics (i.e., education, gender, race, and ethnicity).</p>']"
ur.01132340615.34,"Hahn, Daniel A.",ur.01032644343.06,"Short, Clancy A.",UF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01132340615.34,"Hahn, Daniel A.",ur.0663242321.49,"Mashanov, Vladimir S.",UF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01132340615.34,"Hahn, Daniel A.",ur.01134664434.25,"Jahan-Mihan, Alireza",UF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01132340615.34,"Hahn, Daniel A.",ur.0626416374.68,"Hatle, John D.",UF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01032644343.06,"Short, Clancy A.",ur.01132340615.34,"Hahn, Daniel A.",UF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01032644343.06,"Short, Clancy A.",ur.0663242321.49,"Mashanov, Vladimir S.",UF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01032644343.06,"Short, Clancy A.",ur.01134664434.25,"Jahan-Mihan, Alireza",UF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.01032644343.06,"Short, Clancy A.",ur.0626416374.68,"Hatle, John D.",UF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.0626416374.68,"Hatle, John D.",ur.01132340615.34,"Hahn, Daniel A.",UNF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.0626416374.68,"Hatle, John D.",ur.01032644343.06,"Short, Clancy A.",UNF,UF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.0626416374.68,"Hatle, John D.",ur.0663242321.49,"Mashanov, Vladimir S.",UNF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.0626416374.68,"Hatle, John D.",ur.01134664434.25,"Jahan-Mihan, Alireza",UNF,UNF,['pub.1149933463'],['Protein storage and reproduction increase on a diet matched to the amino acids of egg yolk protein in grasshoppers'],"['The diets of animals are essential to support development, and protein is key. Accumulation of stored nutrients can support developmental events such as molting and initiation of reproduction. Agricultural studies have addressed how dietary protein quality affects growth, but few studies have addressed the effects of dietary protein quality on developmental transitions. Studies on how dietary quality may affect protein storage and development are possible in arthropods, which store proteins in the hemolymph. We hypothesized that diets with a composition of amino acids that matches the precursor of egg yolk protein (vitellogenin, Vg) will be high quality and support both egg production and accumulation of storage proteins. Grasshoppers were fed one of two isonitrogenous solutions of amino acids daily: Vg-balanced (matched to Vg) or Unbalanced (same total moles of amino acids, but not matched to egg yolk). We measured reproduction and storage protein levels in serial hemolymph samples from individuals. The Vg-balanced group had greater reproduction and greater cumulative levels of storage proteins than did the Unbalanced group. This occurred even though amino acids fed to the Vg-balanced group were not a better match to storage protein than were the amino acids fed to the Unbalanced group. Further, oviposition timing was best explained by a combination of diet, age at the maximum level of storage protein hexamerin-270 and accumulation of hexamerin-90. Our study tightens the link between storage proteins and commitment to reproduction, and shows that dietary protein quality is vital for protein storage and reproduction.']"
ur.012156464142.80,"LeBlanc, Kelly",ur.015332726033.17,"Walker, Krystal",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.012156464142.80,"LeBlanc, Kelly",ur.010763032160.65,"Marsh, Tamara",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.012156464142.80,"LeBlanc, Kelly",ur.016425135625.29,"Xu, Jing",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.012156464142.80,"LeBlanc, Kelly",ur.01302000265.19,"Hamadi, Hanadi Y.",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.012156464142.80,"LeBlanc, Kelly",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.015332726033.17,"Walker, Krystal",ur.012156464142.80,"LeBlanc, Kelly",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.015332726033.17,"Walker, Krystal",ur.010763032160.65,"Marsh, Tamara",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.015332726033.17,"Walker, Krystal",ur.016425135625.29,"Xu, Jing",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.015332726033.17,"Walker, Krystal",ur.01302000265.19,"Hamadi, Hanadi Y.",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.015332726033.17,"Walker, Krystal",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.010763032160.65,"Marsh, Tamara",ur.012156464142.80,"LeBlanc, Kelly",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.010763032160.65,"Marsh, Tamara",ur.015332726033.17,"Walker, Krystal",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.010763032160.65,"Marsh, Tamara",ur.016425135625.29,"Xu, Jing",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.010763032160.65,"Marsh, Tamara",ur.01302000265.19,"Hamadi, Hanadi Y.",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.010763032160.65,"Marsh, Tamara",ur.011252000536.58,"Hicks-Roof, Kristen",UNF,UNF,['pub.1150155453'],"[""Do Nutrition Students' and Professionals' Whole-Grain Knowledge, Perceptions, and Experiences Differ?""]","[""The purpose of this study was to investigate nutrition students' and professionals' knowledge, perceptions, and experiences with whole grains and the differences between the levels of nutrition experts. Participants completed an 18-question online survey from December 2020 to February 2021. Convenience, snowballing, and purposive recruitment strategies were used. Data were analyzed using descriptive statistics, regression, and correlation analysis. Participants (n = 472) included 2 groups: registered dietitians/registered dietitian nutritionists (n = 348) and dietetic intern and nutrition students (NUTRs) (n = 124). Both registered dietitian nutritionists and NUTRs were able to correctly identify whole-grain foods and had had high whole-grain knowledge. Registered dietitian nutritionists had a significantly positive relationship between categorizing whole grains and whole-grain health impacts, which was not seen for NUTRs. Registered dietitian nutritionists personally consumed an average of 13 different grains, whereas NUTRs consumed 11 grains (P < .001). More than 60% of all participants reported never consumed 7 unique grains (triticale, KAMUT, teff, spelt, sorghum, amaranth, and millet). Nutrition students and professionals are knowledgeable on whole-grain choices and their benefits yet are less experienced in their consumption of those grains. In order to inform and promote to the public whole-grain foods, nutrition students and professionals need to continue to boost their familiarity with diverse whole grains.""]"
ur.013434307160.44,"Headley, Cortney",ur.012636726560.65,"Lane, W. Brian",UNF,UNF,['pub.1150270992'],['Student representations of a community of practice'],"['The communities of practice (COP) framework is useful in understanding the effort to expand physics education into professional preparation. This framework prompts physics educators and physics education researchers to consider “what counts as doing physics” that we want to prepare students for and how we can model professional physics practice in our classrooms. We argue that this focus on community omits an important consideration of the student’s perceptions of the physics community, which informs how they navigate the community. We introduce the idea of a COP model to describe a student’s internal representation of the community’s goals and practices and their sense of membership within the community. The student develops their COP model in response to legitimate peripheral participation within the community and uses this model to extrapolate their experience of the local community to the global community. We describe how this construct shares similarities with other frameworks but retains distinct features that make it a helpful tool for analysis. We demonstrate the use of the COP model in the review of student interviews about the use of computational practices in the physics community. The COP model helps us interpret student responses in terms of their COP models’ alignment and misalignment with the physics community. We discuss implications for instruction and reflect on the utility of the COP-model construct.']"
ur.012636726560.65,"Lane, W. Brian",ur.013434307160.44,"Headley, Cortney",UNF,UNF,['pub.1150270992'],['Student representations of a community of practice'],"['The communities of practice (COP) framework is useful in understanding the effort to expand physics education into professional preparation. This framework prompts physics educators and physics education researchers to consider “what counts as doing physics” that we want to prepare students for and how we can model professional physics practice in our classrooms. We argue that this focus on community omits an important consideration of the student’s perceptions of the physics community, which informs how they navigate the community. We introduce the idea of a COP model to describe a student’s internal representation of the community’s goals and practices and their sense of membership within the community. The student develops their COP model in response to legitimate peripheral participation within the community and uses this model to extrapolate their experience of the local community to the global community. We describe how this construct shares similarities with other frameworks but retains distinct features that make it a helpful tool for analysis. We demonstrate the use of the COP model in the review of student interviews about the use of computational practices in the physics community. The COP model helps us interpret student responses in terms of their COP models’ alignment and misalignment with the physics community. We discuss implications for instruction and reflect on the utility of the COP-model construct.']"
ur.015534510336.20,"Boggs, C.",ur.014472453705.75,"Quinn, N.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.015534510336.20,"Boggs, C.",ur.011732133420.33,"Terrell, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.015534510336.20,"Boggs, C.",ur.07627017536.46,"Rule, M.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.015534510336.20,"Boggs, C.",ur.0634752264.04,"Zeglin, R.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.015534510336.20,"Boggs, C.",ur.011252000536.58,"Hicks-Roof, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.07627017536.46,"Rule, M.",ur.014472453705.75,"Quinn, N.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.07627017536.46,"Rule, M.",ur.015534510336.20,"Boggs, C.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.07627017536.46,"Rule, M.",ur.011732133420.33,"Terrell, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.07627017536.46,"Rule, M.",ur.0634752264.04,"Zeglin, R.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.07627017536.46,"Rule, M.",ur.011252000536.58,"Hicks-Roof, K.",UNF,UNF,"['pub.1150296139', 'pub.1152283195']","['What is the Connection Between Weight Inclusivity and Sexual Health?', 'Sex at every size: A content analysis of weight inclusivity in sexual functioning research']","[nan, 'Sexual functioning research has been slow to address sizeism within its volumes. Much of the sexual health science has adopted a weight normative model rather than a weight inclusive model. The purpose of this study was to, through systematic literature review, describe the sexual functioning research landscape with respect to weight inclusivity. The review included three databases (Google Scholar, Medline and Ebsco) between 2010 and 2020. Each article was double coded for weight inclusivity and sexual functioning. Sixty-seven articles met the criteria and were included in analysis. The articles, overall, only endorsed weight inclusive tenets 16 % of the time. Articles were most weight inclusive when discussing sexual satisfaction (22.5 %). Meanwhile, when discussing sexual arousal, the articles were the least weight inclusive (16.7 %). These findings suggest there is needs to be greater education and intention for sex educators, therapists, and clinicians, to explore and enhance body positivity and sexual health.']"
ur.01035031577.44,"Witherspoon, D.",ur.01213347266.19,"Sealey-Potts, C.",UNF,UNF,['pub.1150298573'],['Dietary and Lifestyle Practices of Individuals with Substantial Cardiovascular Risk Indicators'],[nan]
ur.012121456246.61,"Waterman, A.",ur.010732040045.21,"Labyak, C.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.012121456246.61,"Waterman, A.",ur.01213347266.19,"Sealey-Potts, C.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.012121456246.61,"Waterman, A.",ur.01025441731.01,"Wright, L.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.010732040045.21,"Labyak, C.",ur.012121456246.61,"Waterman, A.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.010732040045.21,"Labyak, C.",ur.01213347266.19,"Sealey-Potts, C.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.010732040045.21,"Labyak, C.",ur.01025441731.01,"Wright, L.",UNF,UNF,['pub.1150304586'],['Nutrition Interventions Are Needed for Dementia Informal Caregivers'],[nan]
ur.01250563337.43,"Marlow, Nicole M.",ur.012241605737.25,"Hong, Young-Rock",UF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.01250563337.43,"Marlow, Nicole M.",ur.01172411552.17,"Tanner, Rebecca",UF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.01250563337.43,"Marlow, Nicole M.",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.012241605737.25,"Hong, Young-Rock",ur.01250563337.43,"Marlow, Nicole M.",UF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.012241605737.25,"Hong, Young-Rock",ur.01172411552.17,"Tanner, Rebecca",UF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.012241605737.25,"Hong, Young-Rock",ur.016254065343.32,"Xie, Zhigang",UF,UNF,"['pub.1150599825', 'pub.1151113257', 'pub.1153940974']","['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic', 'Association Between Health Information‒Seeking Behavior on YouTube and Physical Activity Among U.S. Adults: Results From Health Information Trends Survey 2020', 'Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity']","['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.', 'Introduction Although physical activity has many health benefits, 45.8% of U.S. adults did not meet the WHO recommendation in 2018. Delivering health-related content, particularly physical activity, through YouTube may help to overcome some barriers, such as lack of access to resources. This study aimed to examine the association between watching health-related information on YouTube and increased levels of physical activity among U.S. adults. Methods Using the U.S. national cross-sectional survey—Health Information National Trends Survey 2020 (n=3,865), we conducted a multivariable logistic regression on obtaining 150 minutes of at least moderate-intensity physical activity per week (WHO guidelines) by watching health-related information on YouTube, controlling for demographics (age, sex, race/ethnicity), socioeconomics (income, education level, insurance coverage, employment), current use of cigarettes and e-cigarettes, use of electronic wearable devices (e.g., Fitbit), self-reported health status, BMI, and the presence of chronic conditions (e.g., diabetes, heart disease, cancer) and depression or anxiety disorders. Results Overall, 40.8% (weighted) of respondents reported using YouTube to watch health-related videos, and 39.2% reported meeting the WHO-recommended physical activity level. After controlling for covariates, adults who reported watching health-related videos on YouTube in the past 12 months (versus not watching) were 1.33 times more likely to do 150 minutes or more of moderate physical activity a week (AOR=1.33; 95% CI=1.01, 1.76). Conclusions This study suggests that adults who view health-related YouTube videos may be more likely to meet the WHO–recommended level of physical activity. This finding could inform future behavioral interventions using online video platforms to promote physical activity.', 'The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.012241605737.25,"Hong, Young-Rock",ur.01302000265.19,"Hamadi, Hanadi Y",UF,UNF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.012241605737.25,"Hong, Young-Rock",ur.01273574321.58,"Mainous, Arch G",UF,UF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.01172411552.17,"Tanner, Rebecca",ur.01250563337.43,"Marlow, Nicole M.",UF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.01172411552.17,"Tanner, Rebecca",ur.012241605737.25,"Hong, Young-Rock",UF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.01172411552.17,"Tanner, Rebecca",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.016254065343.32,"Xie, Zhigang",ur.01250563337.43,"Marlow, Nicole M.",UNF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.016254065343.32,"Xie, Zhigang",ur.012241605737.25,"Hong, Young-Rock",UNF,UF,"['pub.1150599825', 'pub.1151113257', 'pub.1153940974']","['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic', 'Association Between Health Information‒Seeking Behavior on YouTube and Physical Activity Among U.S. Adults: Results From Health Information Trends Survey 2020', 'Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity']","['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.', 'Introduction Although physical activity has many health benefits, 45.8% of U.S. adults did not meet the WHO recommendation in 2018. Delivering health-related content, particularly physical activity, through YouTube may help to overcome some barriers, such as lack of access to resources. This study aimed to examine the association between watching health-related information on YouTube and increased levels of physical activity among U.S. adults. Methods Using the U.S. national cross-sectional survey—Health Information National Trends Survey 2020 (n=3,865), we conducted a multivariable logistic regression on obtaining 150 minutes of at least moderate-intensity physical activity per week (WHO guidelines) by watching health-related information on YouTube, controlling for demographics (age, sex, race/ethnicity), socioeconomics (income, education level, insurance coverage, employment), current use of cigarettes and e-cigarettes, use of electronic wearable devices (e.g., Fitbit), self-reported health status, BMI, and the presence of chronic conditions (e.g., diabetes, heart disease, cancer) and depression or anxiety disorders. Results Overall, 40.8% (weighted) of respondents reported using YouTube to watch health-related videos, and 39.2% reported meeting the WHO-recommended physical activity level. After controlling for covariates, adults who reported watching health-related videos on YouTube in the past 12 months (versus not watching) were 1.33 times more likely to do 150 minutes or more of moderate physical activity a week (AOR=1.33; 95% CI=1.01, 1.76). Conclusions This study suggests that adults who view health-related YouTube videos may be more likely to meet the WHO–recommended level of physical activity. This finding could inform future behavioral interventions using online video platforms to promote physical activity.', 'The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.016254065343.32,"Xie, Zhigang",ur.01172411552.17,"Tanner, Rebecca",UNF,UF,['pub.1150599825'],['People With Functional Disability and Access to Health Care During the COVID-19 Pandemic'],"['INTRODUCTION: The COVID-19 pandemic and nationwide restriction measures have disrupted health care delivery and access for the general population. There is limited evidence about access to care issues (delayed and forgone care) due to the pandemic among people with disability (PWD).\r\nMETHODS: This study used the 2020 National Health Interview Survey data. Disability status was defined by disability severity (moderate and severe disability), type, and the number of disabling limitations. Descriptive analysis and multivariate logistic regression (adjusted for sociodemographic and health-related characteristics) were conducted to estimate delayed/forgone care (yes/no) between PWD and people without disability (PWoD).\r\nRESULTS: Among 17,528 US adults, 40.7% reported living with disability. A higher proportion of respondents with severe and moderate disability reported delaying care than PWoD (severe=33.2%; moderate=27.5%; PWoD=20.0%, P <0.001). The same was true for forgone medical care (severe=26.6%; moderate=19.0%; PWoD=12.2%, P <0.001). Respondents with a moderate disability {delayed [odds ratio (OR)=1.33, 95% confidence interval (CI)=1.19, 1.49]; forgone [OR=1.46, 95% CI=1.28, 1.67]} and a severe disability [delayed (OR=1.52, 95% CI=1.27, 1.83); forgone (OR=1.84, 95% CI=1.49, 2.27)] were more likely to report delayed medical care and forgone medical care compared with PWoD. These findings were consistent across the models using disability type and the number of limitations.\r\nCONCLUSIONS: PWD were more likely to experience COVID-19-related delays in or forgone medical care compared with PWoD. The more severe and higher frequency of disabling limitations were associated with higher degrees of delayed and forgone medical care. Policymakers need to develop disability-inclusive responses to public health emergencies and postpandemic care provision among PWD.']"
ur.016254065343.32,"Xie, Zhigang",ur.012776250411.35,"Bian, Jiang",UNF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.016254065343.32,"Xie, Zhigang",ur.01310165033.13,"Jo, Ara",UNF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.016254065343.32,"Xie, Zhigang",ur.012450366366.47,"O'Neal, LaToya J",UNF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.016254065343.32,"Xie, Zhigang",ur.01120345541.16,"George, Thomas J",UNF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.016254065343.32,"Xie, Zhigang",ur.01166236407.18,"Scarton, Lisa",UNF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.016254065343.32,"Xie, Zhigang",ur.01013553156.09,"Munoz Pena, Juan M",UNF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.016254065343.32,"Xie, Zhigang",ur.01302000265.19,"Hamadi, Hanadi Y",UNF,UNF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.016254065343.32,"Xie, Zhigang",ur.01273574321.58,"Mainous, Arch G",UNF,UF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.015307320476.16,"Boyne, Ciarra A.",ur.01253770701.51,"Richardson, Michael R.",UNF,UNF,['pub.1150782593'],['Sedentary Time And Prescription Medication Use In United States Adults: 2017-2018 NHANES'],[nan]
ur.015307320476.16,"Boyne, Ciarra A.",ur.01365551077.18,"Churilla, James R.",UNF,UNF,['pub.1150782593'],['Sedentary Time And Prescription Medication Use In United States Adults: 2017-2018 NHANES'],[nan]
ur.012230447375.04,"Gibson, Quincy",ur.013454116476.18,"Brightwell, Kristin",UNF,UNF,['pub.1150857140'],['Assessment of social mixing and spatial overlap as a pathway for disease transmission in a northeast Florida estuarine dolphin community'],"['Common bottlenose dolphins (Tursiops truncatus) in estuarine systems are often subjected to natural and anthropogenic risks, which may impact their health and behaviors. Effective management for this species should incorporate continual, updated information on behavioral patterns of the individuals within populations. Currently, the Jacksonville Estuarine System stock assessment report, which includes dolphins in the St. Johns River (SJR) in northeast Florida, is based on data from the 1990s. Since then, dolphins’ use of the SJR has shifted, and the community has been impacted by two unusual mortality events (UME). This study analyzed site fidelity, space use and overlap, and social mixing with respect to the epizootic 2013–2015 UME, whose infectious agent was cetacean morbillivirus (CeMV). Examination of residency status determined that most dolphins sighted in the river were year-round residents (x¯\\documentclass[12pt]{minimal}\r\n\t\t\t\t\\usepackage{amsmath}\r\n\t\t\t\t\\usepackage{wasysym}\r\n\t\t\t\t\\usepackage{amsfonts}\r\n\t\t\t\t\\usepackage{amssymb}\r\n\t\t\t\t\\usepackage{amsbsy}\r\n\t\t\t\t\\usepackage{mathrsfs}\r\n\t\t\t\t\\usepackage{upgreek}\r\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\r\n\t\t\t\t\\begin{document}$$\\overline{x }$$\\end{document}=45%, SD\u2009=\u20093.20). Space use and overlap of core areas occurred among residency categories but varied between cold and warm seasons and across time periods. There were key, highly connected individuals in the social network, which may have influenced disease transmission during the UME. Individuals infected by CeMV that stranded during the UME shared overlapping core areas with each other and with individuals from different residency categories. Altogether, this information on site fidelity, spatial overlap, and social mixing will help improve management plans for SJR dolphins, lead to better response efforts to future unusual mortality events, and increase the understanding of disease transmission in social species.']"
ur.013454116476.18,"Brightwell, Kristin",ur.012230447375.04,"Gibson, Quincy",UNF,UNF,['pub.1150857140'],['Assessment of social mixing and spatial overlap as a pathway for disease transmission in a northeast Florida estuarine dolphin community'],"['Common bottlenose dolphins (Tursiops truncatus) in estuarine systems are often subjected to natural and anthropogenic risks, which may impact their health and behaviors. Effective management for this species should incorporate continual, updated information on behavioral patterns of the individuals within populations. Currently, the Jacksonville Estuarine System stock assessment report, which includes dolphins in the St. Johns River (SJR) in northeast Florida, is based on data from the 1990s. Since then, dolphins’ use of the SJR has shifted, and the community has been impacted by two unusual mortality events (UME). This study analyzed site fidelity, space use and overlap, and social mixing with respect to the epizootic 2013–2015 UME, whose infectious agent was cetacean morbillivirus (CeMV). Examination of residency status determined that most dolphins sighted in the river were year-round residents (x¯\\documentclass[12pt]{minimal}\r\n\t\t\t\t\\usepackage{amsmath}\r\n\t\t\t\t\\usepackage{wasysym}\r\n\t\t\t\t\\usepackage{amsfonts}\r\n\t\t\t\t\\usepackage{amssymb}\r\n\t\t\t\t\\usepackage{amsbsy}\r\n\t\t\t\t\\usepackage{mathrsfs}\r\n\t\t\t\t\\usepackage{upgreek}\r\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\r\n\t\t\t\t\\begin{document}$$\\overline{x }$$\\end{document}=45%, SD\u2009=\u20093.20). Space use and overlap of core areas occurred among residency categories but varied between cold and warm seasons and across time periods. There were key, highly connected individuals in the social network, which may have influenced disease transmission during the UME. Individuals infected by CeMV that stranded during the UME shared overlapping core areas with each other and with individuals from different residency categories. Altogether, this information on site fidelity, spatial overlap, and social mixing will help improve management plans for SJR dolphins, lead to better response efforts to future unusual mortality events, and increase the understanding of disease transmission in social species.']"
ur.014015716703.00,"Kazi, Nazmul",ur.01365060341.83,"Kahanda, Indika",UNF,UNF,['pub.1150887389'],['Challenges and opportunities in current vaccine technology and administration: A comprehensive survey examining oral vaccine potential in the United States'],"[""This study provides a snapshot of the current vaccine business ecosystem, including practices, challenges, beliefs, and expectations of vaccine providers. Our team focused on providers' firsthand experience with administering vaccines to determine if an oral vaccine (e.g. pill or oral-drop) would be well-received. We interviewed 135 healthcare providers and vaccine specialists across the US, focusing questions on routine vaccinations, not COVID-19 vaccines. Improving workflow efficiency is a top concern among vaccine providers due to shrinking reimbursement rates-determined by pharmacy benefit managers (PBMs)-and the time-intensiveness of injectable vaccines. Administering injectable vaccines takes 23\u2009minutes/patient on average, while dispensing pills takes only 5\u2009minutes/patient. An average of 24% of patients express needle-fear, which further lengthens the processing time. Misaligned incentives between providers and PBMs could reduce the quality and availability of vaccine-related care. The unavailability of single-dose orders prevents some rural providers from offering certain vaccines. Most interviewees (74%) believe an oral vaccine would improve patient-provider experience, patient-compliance, and workflow efficiency, while detractors (26%) worry about the taste, vaccine absorption, and efficacy. Additional research could investigate whether currently non-vaccinating pharmacies would be willing to offer oral vaccines, and the impact of oral vaccines on vaccine acceptance.""]"
ur.015603434445.98,"Shang, Di",ur.014006001013.02,"White-Williams, Cynthia",UNF,UNF,['pub.1151083618'],['Use of Telehealth Among Racial and Ethnic Minority Groups in the United States Before and During the COVID-19 Pandemic'],"['OBJECTIVES: The COVID-19 pandemic has propelled the use of technology for health care services delivery. Because of inequities in health care and technology access, we investigated the use of telehealth services among racial and ethnic minority groups before and during the COVID-19 pandemic.\r\nMETHODS: For this retrospective study, we examined the electronic health records of privately insured patients in the Healthjump database, provided by the COVID-19 Research Database Consortium. We examined 17.98 million unique visit records of 2.93 million patients from March through December 2019 and 22.17 million records of 3.55 million patients from March through December 2020. We conducted a descriptive analysis and used multiple logistic regression to examine differences in the use of telehealth services among 3 racial and ethnic groups: non-Hispanic White, non-Hispanic Black, and Hispanic people.\r\nRESULTS: Telehealth visits before and during COVID-19 accounted for 8.3% and 10.9% of total visits, respectively, with a peak of 15.5% in April 2020. Pre-COVID-19, Hispanic patients had a significantly lower monthly utilization rate (5.3%) than non-Hispanic White patients (8.4%, <i>P</i> &lt; .001) and non-Hispanic Black patients (10.4%, <i>P</i> = .001). During the pandemic study period, Hispanic patients were 41% less likely than non-Hispanic White patients to have a telehealth visit, controlling for age and sex.\r\nCONCLUSIONS: The likelihood of using telehealth was lower among Hispanic patients than among non-Hispanic White and non-Hispanic Black patients during the pandemic. Culturally sensitive measures are needed to support telehealth use among the Hispanic population.']"
ur.014006001013.02,"White-Williams, Cynthia",ur.015603434445.98,"Shang, Di",UNF,UNF,['pub.1151083618'],['Use of Telehealth Among Racial and Ethnic Minority Groups in the United States Before and During the COVID-19 Pandemic'],"['OBJECTIVES: The COVID-19 pandemic has propelled the use of technology for health care services delivery. Because of inequities in health care and technology access, we investigated the use of telehealth services among racial and ethnic minority groups before and during the COVID-19 pandemic.\r\nMETHODS: For this retrospective study, we examined the electronic health records of privately insured patients in the Healthjump database, provided by the COVID-19 Research Database Consortium. We examined 17.98 million unique visit records of 2.93 million patients from March through December 2019 and 22.17 million records of 3.55 million patients from March through December 2020. We conducted a descriptive analysis and used multiple logistic regression to examine differences in the use of telehealth services among 3 racial and ethnic groups: non-Hispanic White, non-Hispanic Black, and Hispanic people.\r\nRESULTS: Telehealth visits before and during COVID-19 accounted for 8.3% and 10.9% of total visits, respectively, with a peak of 15.5% in April 2020. Pre-COVID-19, Hispanic patients had a significantly lower monthly utilization rate (5.3%) than non-Hispanic White patients (8.4%, <i>P</i> &lt; .001) and non-Hispanic Black patients (10.4%, <i>P</i> = .001). During the pandemic study period, Hispanic patients were 41% less likely than non-Hispanic White patients to have a telehealth visit, controlling for age and sex.\r\nCONCLUSIONS: The likelihood of using telehealth was lower among Hispanic patients than among non-Hispanic White and non-Hispanic Black patients during the pandemic. Culturally sensitive measures are needed to support telehealth use among the Hispanic population.']"
ur.016132335454.74,"Sturchio, Matthew A.",ur.01343071750.63,"Aspinwall, Michael J.",UNF,UNF,['pub.1151125604'],['Contrasting Effects of Nitrogen Addition on Leaf Photosynthesis and Respiration in Black Mangrove in North Florida'],"['Nutrient enrichment is a major driver of environmental change in mangrove ecosystems. Yet, nutrient enrichment impacts on physiological processes that regulate CO2 and water fluxes between mangrove vegetation and the atmosphere remain unclear. We measured peak growing season photosynthesis (A) and respiration (R) in black mangrove (Avicennia germinans) leaves that had been subjected to long-term (8-year) nutrient enrichment (added N, added P, control) in north Florida. Previous results from this site indicated that Avicennia productivity was N-limited, but not P-limited. Thus, we expected that N addition would increase light saturated net photosynthesis at ambient CO2 (Anet), intrinsic water-use efficiency (iWUE), maximum rate of Rubisco carboxylation (Vcmax), and leaf dark respiration (R), while P addition would have little effect on any aspect of photosynthesis or respiration. We expected that increased photosynthesis and respiration would be most apparent immediately after N addition and in newly formed leaves. Indeed, Anet and Vcmax increased just after N addition in the N addition treatment; these increases were limited to leaves formed just after N addition. Nonetheless, over time, photosynthetic parameters and iWUE were similar across treatments. Interestingly, R measured at 25\xa0°C increased with N addition; this effect was consistent across time points. P addition had little effect on R. Across treatments and time points, Vcmax,25 (Vcmax standardized to 25\xa0°C) showed no relationship with R at 25\xa0°C, but the maximum rate of electron transport for RuBP regeneration standardized to 25\xa0°C (Jmax,25) increased with R at 25\xa0°C. We conclude that N addition may have small, short-lived effects on photosynthetic processes, but sustained effects on leaf R in N-limited mangrove ecosystems.']"
ur.016132335454.74,"Sturchio, Matthew A.",ur.011201414666.36,"Chieppa, Jeff",UNF,UNF,['pub.1151125604'],['Contrasting Effects of Nitrogen Addition on Leaf Photosynthesis and Respiration in Black Mangrove in North Florida'],"['Nutrient enrichment is a major driver of environmental change in mangrove ecosystems. Yet, nutrient enrichment impacts on physiological processes that regulate CO2 and water fluxes between mangrove vegetation and the atmosphere remain unclear. We measured peak growing season photosynthesis (A) and respiration (R) in black mangrove (Avicennia germinans) leaves that had been subjected to long-term (8-year) nutrient enrichment (added N, added P, control) in north Florida. Previous results from this site indicated that Avicennia productivity was N-limited, but not P-limited. Thus, we expected that N addition would increase light saturated net photosynthesis at ambient CO2 (Anet), intrinsic water-use efficiency (iWUE), maximum rate of Rubisco carboxylation (Vcmax), and leaf dark respiration (R), while P addition would have little effect on any aspect of photosynthesis or respiration. We expected that increased photosynthesis and respiration would be most apparent immediately after N addition and in newly formed leaves. Indeed, Anet and Vcmax increased just after N addition in the N addition treatment; these increases were limited to leaves formed just after N addition. Nonetheless, over time, photosynthetic parameters and iWUE were similar across treatments. Interestingly, R measured at 25\xa0°C increased with N addition; this effect was consistent across time points. P addition had little effect on R. Across treatments and time points, Vcmax,25 (Vcmax standardized to 25\xa0°C) showed no relationship with R at 25\xa0°C, but the maximum rate of electron transport for RuBP regeneration standardized to 25\xa0°C (Jmax,25) increased with R at 25\xa0°C. We conclude that N addition may have small, short-lived effects on photosynthetic processes, but sustained effects on leaf R in N-limited mangrove ecosystems.']"
ur.013621327420.63,"Galena, Amy E.",ur.01134664434.25,"Jahan-Mihan, Alireza",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.013621327420.63,"Galena, Amy E.",ur.014613704314.11,"Perez, Doreen",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.013621327420.63,"Galena, Amy E.",ur.0636665472.55,"Bednarzyk, Michele",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.013621327420.63,"Galena, Amy E.",ur.01226756046.26,"Ochrietor, Judith D.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.013621327420.63,"Galena, Amy E.",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.014613704314.11,"Perez, Doreen",ur.013621327420.63,"Galena, Amy E.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.014613704314.11,"Perez, Doreen",ur.01134664434.25,"Jahan-Mihan, Alireza",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.014613704314.11,"Perez, Doreen",ur.0636665472.55,"Bednarzyk, Michele",UNF,UNF,"['pub.1151600442', 'pub.1152002766']","['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis."", 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.014613704314.11,"Perez, Doreen",ur.01226756046.26,"Ochrietor, Judith D.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.014613704314.11,"Perez, Doreen",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,"['pub.1151600442', 'pub.1152002766']","['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis."", 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.014613704314.11,"Perez, Doreen",ur.014243412173.03,"Ross, Jenifer M",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.014613704314.11,"Perez, Doreen",ur.010776677330.88,"Harris, Michel",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.0636665472.55,"Bednarzyk, Michele",ur.013621327420.63,"Galena, Amy E.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.0636665472.55,"Bednarzyk, Michele",ur.01134664434.25,"Jahan-Mihan, Alireza",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.0636665472.55,"Bednarzyk, Michele",ur.014613704314.11,"Perez, Doreen",UNF,UNF,"['pub.1151600442', 'pub.1152002766']","['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis."", 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.0636665472.55,"Bednarzyk, Michele",ur.01226756046.26,"Ochrietor, Judith D.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.0636665472.55,"Bednarzyk, Michele",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,"['pub.1151600442', 'pub.1152002766']","['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study', 'Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.']","[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis."", 'The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.0636665472.55,"Bednarzyk, Michele",ur.014243412173.03,"Ross, Jenifer M",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.0636665472.55,"Bednarzyk, Michele",ur.010776677330.88,"Harris, Michel",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.01226756046.26,"Ochrietor, Judith D.",ur.013621327420.63,"Galena, Amy E.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01226756046.26,"Ochrietor, Judith D.",ur.01134664434.25,"Jahan-Mihan, Alireza",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01226756046.26,"Ochrietor, Judith D.",ur.014613704314.11,"Perez, Doreen",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01226756046.26,"Ochrietor, Judith D.",ur.0636665472.55,"Bednarzyk, Michele",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01226756046.26,"Ochrietor, Judith D.",ur.01110613476.83,"Arikawa, Andrea Y.",UNF,UNF,['pub.1151600442'],['The effects of fermented vegetable consumption on the composition of the intestinal microbiota and levels of inflammatory markers in women: A pilot and feasibility study'],"[""The primary objective of this pilot study was to investigate the feasibility of regular consumption of fermented vegetables for six weeks on markers of inflammation and the composition of the gut microflora in women (clinical trials ID: NTC03407794). Thirty-one women were randomized into one of three groups: 100 g/day of fermented vegetables (group A), 100 g/day pickled vegetables (group B), or no vegetables (group C) for six weeks. Dietary intake was assessed by a food frequency questionnaire and blood and stool samples were provided before and after the intervention for measurement of C-reactive protein (CRP), tumor necrosis factor alpha (TNF-α), and lipopolysaccharide binding protein (LBP). Next-generation sequencing of the V4 region of the 16S rRNA gene was performed on the Illumina MiSeq platform. Participants' ages ranged between 18 and 69 years. Both groups A and B had a mean daily consumption of 91g of vegetables for 32 and 36 days, respectively. Serum CRP ranged between 0.9 and 265 ng/mL (SD = 92.4) at baseline, while TNF-α and LBP concentrations ranged between 0 and 9 pg/mL (SD = 2.3), and 7 and 29 μg/mL (SD = 4.4), respectively. There were no significant changes in levels of inflammatory markers among groups. At timepoint 2, group A showed an increase in Faecalibacterium prausnitzii (P = 0.022), a decrease in Ruminococcus torques (P<0.05), and a trend towards greater alpha diversity measured by the Shannon index (P = 0.074). The findings indicate that consumption of ~100 g/day of fermented vegetables for six weeks is feasible and may result in beneficial changes in the composition of the gut microbiota. Future trials should determine whether consumption of fermented vegetables is an effective strategy against gut dysbiosis.""]"
ur.01226756046.26,"Ochrietor, Judith D.",ur.011764604531.41,"Gonzalez, Alicia",UNF,UNF,['pub.1153022117'],['Reduced expression of Basigin gene products in response to chronic inflammation may contribute to vision loss'],"['Chronic inflammation of the retina, like that of diabetic retinopathy, disrupts the blood-retina barrier (BRB). Disruption of the BRB increases vascular permeability and leads to vision loss. Basigin gene products, cell-adhesion molecules and members of the immunoglobulin superfamily, are expressed on endothelial cells, photoreceptor cells and Müller glial cells. Basigin variant-1 on photoreceptors interacts with Basigin variant-2 on Müller glial cells and to rod-derived cone viability factor (RdCVF) to form metabolic support mechanisms necessary for the survival of photoreceptor neurons. The goal of the current study was to determine the gene expression changes of Basigin gene products in ex\xa0vivo neonatal, adolescent, and adult retina when exposed to an inflammatory insult in acute and chronic phases. Retinas extracted from mice at postnatal day (P) 7, 30, and 180 were incubated with either phosphate-buffered saline (PBS), as a control, or lipopolysaccharide (LPS), an endotoxin, for 3, 6, 12, or 24\xa0h. RNA was then extracted and Basigin gene products were quantified by qPCR. Analyses indicate both gene products are influenced by LPS exposure in a time and age dependent manner. Specifically, P180 retinas exposed to LPS showed significant decreases in both Basigin gene products, suggesting older retinas may be susceptible to chronic inflammation and subsequent vision loss.']"
ur.014675651125.51,"Brown, Elizabeth R.",ur.011141050324.18,"Wesely, Jennifer K.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.014675651125.51,"Brown, Elizabeth R.",ur.012473703013.35,"Binder, Michael",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.014675651125.51,"Brown, Elizabeth R.",ur.0776454123.71,"Phills, Curtis E.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.012473703013.35,"Binder, Michael",ur.011141050324.18,"Wesely, Jennifer K.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.012473703013.35,"Binder, Michael",ur.014675651125.51,"Brown, Elizabeth R.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.012473703013.35,"Binder, Michael",ur.0776454123.71,"Phills, Curtis E.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.0776454123.71,"Phills, Curtis E.",ur.011141050324.18,"Wesely, Jennifer K.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.0776454123.71,"Phills, Curtis E.",ur.014675651125.51,"Brown, Elizabeth R.",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.0776454123.71,"Phills, Curtis E.",ur.012473703013.35,"Binder, Michael",UNF,UNF,['pub.1151819651'],['Survivor‐focused timely warnings increase negative stereotyping of survivors but make readers feel safer'],"[""Abstract  Universities in the United States issue “timely warnings” when a sexual assault occurs on campus. Timely warnings should make campuses safer by communicating information that allows community members to make informed personal safety decisions. Unfortunately, these warnings often undermine that safety goal by centering the survivor's actions (i.e., walking alone at night) and characteristics (i.e., wearing revealing clothes), rather than or in addition to the perpetrator's actions (i.e., followed the survivor to their house) and characteristics (i.e., wearing a red shirt). The present research investigated whether including survivor‐focused details in timely warnings causes readers to view survivors as less intelligent (i.e., why else would they walk alone at night?) and more promiscuous (i.e., why else would they wear revealing clothes?). Among three distinct samples (registered voters, US\xa0undergraduates, US\xa0based MTurk participants), we manipulated whether timely warnings included survivor‐focused details. Survivor‐focused, versus not survivor‐focused, warnings caused participants to view survivors as less intelligent (all three samples) and more promiscuous (US\xa0undergraduates and US\xa0based MTurk participants). Participants also felt safer when timely warnings focused on the survivor. Notably, feeling safer related to believing survivors were less intelligent (US\xa0based MTurk participants). We discuss how survivor‐focused timely warnings may ironically make campuses less safe by discouraging survivors from reporting crimes. Materials, data, and analysis code are available online ( https://osf.io/acnqb/?view_only=4c9dd1c7abcd4a6bbd2c68e8412fcbdd ). ""]"
ur.01276621531.69,"Gilg, MR",ur.010207033721.05,"Heng, F",UNF,UNF,['pub.1151970373'],['Acclimation to elevated temperatures in Acropora cervicornis: effects of host genotype and symbiont shuffling'],"[' Climate change is increasing the average surface temperatures of tropical waters, creating unfavorable conditions for corals. Some species of coral can physiologically acclimate to elevated temperatures, but the degree to which genetic variation underlies differences in this ability is currently unknown. Acclimation to elevated temperatures in coral has been hypothesized to be due to either alterations in the symbiont community or to changes in gene expression. The present study investigated the ability of Acropora cervicornis to acclimate to elevated temperatures, estimated the heritability of plasticity in upper thermal tolerance, and tested whether observed acclimation patterns could be explained by symbiont shuffling. Coral fragments from a nursery in the Florida Keys (USA) were acclimated at either ambient (27 ± 1°C) or elevated (30 ± 1°C) temperatures and then exposed to a second heat stress (32 ± 1°C) and monitored for mortality. Fragments acclimated to elevated temperatures showed significantly longer lifespans in the subsequent heat stress than did those acclimated at ambient temperature. The ability to acclimate to elevated temperatures differed significantly among coral genets, yielding low, but significant, estimates of broad-sense heritability. A subsequent experiment revealed no changes in either bacterial or dinoflagellate communities of symbionts as a result of acclimation, suggesting that symbiont shuffling did not account for the differences in lifespan between treatments. While estimates of heritability were low, the results suggest that plasticity in upper thermal tolerance significantly differs among coral genets, and that acclimation is likely a result of alterations in gene expression as opposed to symbiont shuffling. ']"
ur.010776677330.88,"Harris, Michel",ur.01110613476.83,"Arikawa, Andrea Y",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.010776677330.88,"Harris, Michel",ur.014613704314.11,"Perez, Doreen",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.010776677330.88,"Harris, Michel",ur.014243412173.03,"Ross, Jenifer M",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.010776677330.88,"Harris, Michel",ur.0636665472.55,"Bednarzyk, Michele",UNF,UNF,['pub.1152002766'],['Dietary Supplement Intake is Associated with Healthier Lifestyle Behaviors in College Students Attending a Regional University in the Southeast: A Cross-Sectional Study.'],"['The relationship between intake of dietary supplements and biomarkers such as insulin and insulin-like growth factor has not been well explored. The primary aim of this cross-sectional study was to investigate the associations between supplement intake and biological and lifestyle factors. We hypothesized that dietary supplement intake was associated with healthier lifestyle behaviors. College students attending a Southeast university were recruited between January 2018 and April 2019. Blood samples were collected to measure insulin, insulin-like growth factor 1 (IGF-1) and alanine aminotransferase (ALT). Statistical tests employed were linear regression and analysis of variance. Ninety-eight participants completed the study and 91% reported taking at least one supplement, while 5.1% reported taking 9+ supplements once per week. There were no differences in levels of insulin, IGF-1 and ALT by levels of dietary supplement intake. Although there were no differences in HEI-2015 score among the groups, those who consumed five or more supplements met a higher percentage of the recommended intake for fruits, performed aerobic exercise for longer duration, and had lower body fat percentage compared to participants who consumed two or less supplements at least once per week. These findings are consistent with previous studies and suggest that dietary supplement intake is highly prevalent in college students, and it may be related to healthy lifestyle behaviors. Future studies should employ mixed methodology to examine reasons by which college students consume dietary supplements and to assess perceived and direct health benefits associated with consumption.']"
ur.012776250411.35,"Bian, Jiang",ur.01310165033.13,"Jo, Ara",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012776250411.35,"Bian, Jiang",ur.012450366366.47,"O'Neal, LaToya J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012776250411.35,"Bian, Jiang",ur.01120345541.16,"George, Thomas J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012776250411.35,"Bian, Jiang",ur.01166236407.18,"Scarton, Lisa",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012776250411.35,"Bian, Jiang",ur.01013553156.09,"Munoz Pena, Juan M",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012776250411.35,"Bian, Jiang",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01310165033.13,"Jo, Ara",ur.012776250411.35,"Bian, Jiang",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01310165033.13,"Jo, Ara",ur.012450366366.47,"O'Neal, LaToya J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01310165033.13,"Jo, Ara",ur.01120345541.16,"George, Thomas J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01310165033.13,"Jo, Ara",ur.01166236407.18,"Scarton, Lisa",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01310165033.13,"Jo, Ara",ur.01013553156.09,"Munoz Pena, Juan M",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01310165033.13,"Jo, Ara",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012450366366.47,"O'Neal, LaToya J",ur.012776250411.35,"Bian, Jiang",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012450366366.47,"O'Neal, LaToya J",ur.01310165033.13,"Jo, Ara",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012450366366.47,"O'Neal, LaToya J",ur.01120345541.16,"George, Thomas J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012450366366.47,"O'Neal, LaToya J",ur.01166236407.18,"Scarton, Lisa",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012450366366.47,"O'Neal, LaToya J",ur.01013553156.09,"Munoz Pena, Juan M",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.012450366366.47,"O'Neal, LaToya J",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01120345541.16,"George, Thomas J",ur.012776250411.35,"Bian, Jiang",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01120345541.16,"George, Thomas J",ur.01310165033.13,"Jo, Ara",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01120345541.16,"George, Thomas J",ur.012450366366.47,"O'Neal, LaToya J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01120345541.16,"George, Thomas J",ur.01166236407.18,"Scarton, Lisa",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01120345541.16,"George, Thomas J",ur.01013553156.09,"Munoz Pena, Juan M",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01120345541.16,"George, Thomas J",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01166236407.18,"Scarton, Lisa",ur.012776250411.35,"Bian, Jiang",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01166236407.18,"Scarton, Lisa",ur.01310165033.13,"Jo, Ara",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01166236407.18,"Scarton, Lisa",ur.012450366366.47,"O'Neal, LaToya J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01166236407.18,"Scarton, Lisa",ur.01120345541.16,"George, Thomas J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01166236407.18,"Scarton, Lisa",ur.01013553156.09,"Munoz Pena, Juan M",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01166236407.18,"Scarton, Lisa",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01013553156.09,"Munoz Pena, Juan M",ur.012776250411.35,"Bian, Jiang",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01013553156.09,"Munoz Pena, Juan M",ur.01310165033.13,"Jo, Ara",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01013553156.09,"Munoz Pena, Juan M",ur.012450366366.47,"O'Neal, LaToya J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01013553156.09,"Munoz Pena, Juan M",ur.01120345541.16,"George, Thomas J",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01013553156.09,"Munoz Pena, Juan M",ur.01166236407.18,"Scarton, Lisa",UF,UF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.01013553156.09,"Munoz Pena, Juan M",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1152004382'],['Examining the relationship between metformin dose and cancer survival: A SEER-Medicare analysis'],"['Cancer is a major health problem in the U.S and type 2 diabetes mellitus (T2DM) is known to increase the risk for the development of many cancers. Metformin, a first-line therapy for treating T2DM, is increasingly being used for its anticancer effects; however, the literature is limited on the effect of metformin dose on overall survival in patients with stage IV cancer. Overall survival was defined as the time interval from the date of diagnosis to the last known follow-up or death from any cause. Subjects who were alive on December 31, 2016 were censored. In this cohort study we examined the relationship between metformin dose and overall survival in persons with both T2DM and stage IV lung, breast, colorectal, prostate, or pancreas cancers. We used a retrospective study design with Cox proportional hazards regression analysis of the 2007-2016 of the Surveillance Epidemiology and End Results-Medicare (SEER) dataset. Of the 7,725 patients, 2,981(38.5%) had been prescribed metformin. Patients who used metformin had significantly better overall survival in both unadjusted (Unadjusted HR, 0.73; 95% CI, 0.69-0.76; p < 0.001) and adjusted models (adjusted HR, 0.77; 95% CI, 0.73-0.81; p < 0.001). The overall survival between patients who took metformin with average daily dose ≥ 1000mg or < 1000mg were not statistically significant (aHR, 1.00; 95% CI, 0.93-1.08; p = 0.90). Metformin use regardless of dose is associated with increased overall survival in older adults with stage IV cancer.']"
ur.014211702264.47,"Ehrlich, Suzanne",ur.011252000536.58,"Hicks‐Roof, Kristen",UNF,UNF,['pub.1152156533'],['Faculty fathers: Understanding the experiences of faculty men with children during the COVID‐19 pandemic'],"['Abstract To effectively cultivate systems of equity, it is important to not only understand the experiences of those who are most affected by inequity but also of those who are in positions of power and privilege. This study explores how faculty men with children navigated the pandemic experience. Two major thematic findings emerged including ‘Not (as) Interrupted’ and ‘“I” feel supported’. These findings suggest that academic fathers may be emerging from the pandemic positioned differently than academic mothers. This research has implications for higher education administrators and faculty in the shaping of practice and policies aimed at equitably supporting faculty post‐pandemic.']"
ur.014211702264.47,"Ehrlich, Suzanne",ur.010676073760.64,"Pascale, Amanda Blakewood",UNF,UNF,['pub.1152156533'],['Faculty fathers: Understanding the experiences of faculty men with children during the COVID‐19 pandemic'],"['Abstract To effectively cultivate systems of equity, it is important to not only understand the experiences of those who are most affected by inequity but also of those who are in positions of power and privilege. This study explores how faculty men with children navigated the pandemic experience. Two major thematic findings emerged including ‘Not (as) Interrupted’ and ‘“I” feel supported’. These findings suggest that academic fathers may be emerging from the pandemic positioned differently than academic mothers. This research has implications for higher education administrators and faculty in the shaping of practice and policies aimed at equitably supporting faculty post‐pandemic.']"
ur.010605654236.42,"Marques, Mariana Fernandes",ur.011403234636.91,"Nunez, Julissa Villegas",UNF,UNF,['pub.1152238101'],['Novel Rickettsia spp. in two common overwintering North American songbirds'],"['American robins and dark-eyed juncos migrate across North America and have been found to be competent hosts for some bacterial and viral pathogens, but their contributions to arthropod-borne diseases more broadly remain poorly characterized. Here, we sampled robins and juncos in multiple sites across North America for arthropod-borne bacterial pathogens of public health significance. We identified two novel <i>Rickettsia</i> spp. in one wintering migrant per bird species related to bellii, transitional, and spotted rickettsiae fever groups. Stable isotope analyses of feathers suggested spring migration of these common songbirds could disperse these novel rickettsiae hundreds-to-thousands of kilometers to host breeding grounds. Further work is needed to characterize zoonotic potential of these rickettsiae and host reservoir competence.']"
ur.010605654236.42,"Marques, Mariana Fernandes",ur.01312414645.39,"Clark, Kerry L.",UNF,UNF,['pub.1152238101'],['Novel Rickettsia spp. in two common overwintering North American songbirds'],"['American robins and dark-eyed juncos migrate across North America and have been found to be competent hosts for some bacterial and viral pathogens, but their contributions to arthropod-borne diseases more broadly remain poorly characterized. Here, we sampled robins and juncos in multiple sites across North America for arthropod-borne bacterial pathogens of public health significance. We identified two novel <i>Rickettsia</i> spp. in one wintering migrant per bird species related to bellii, transitional, and spotted rickettsiae fever groups. Stable isotope analyses of feathers suggested spring migration of these common songbirds could disperse these novel rickettsiae hundreds-to-thousands of kilometers to host breeding grounds. Further work is needed to characterize zoonotic potential of these rickettsiae and host reservoir competence.']"
ur.011403234636.91,"Nunez, Julissa Villegas",ur.010605654236.42,"Marques, Mariana Fernandes",UNF,UNF,['pub.1152238101'],['Novel Rickettsia spp. in two common overwintering North American songbirds'],"['American robins and dark-eyed juncos migrate across North America and have been found to be competent hosts for some bacterial and viral pathogens, but their contributions to arthropod-borne diseases more broadly remain poorly characterized. Here, we sampled robins and juncos in multiple sites across North America for arthropod-borne bacterial pathogens of public health significance. We identified two novel <i>Rickettsia</i> spp. in one wintering migrant per bird species related to bellii, transitional, and spotted rickettsiae fever groups. Stable isotope analyses of feathers suggested spring migration of these common songbirds could disperse these novel rickettsiae hundreds-to-thousands of kilometers to host breeding grounds. Further work is needed to characterize zoonotic potential of these rickettsiae and host reservoir competence.']"
ur.011403234636.91,"Nunez, Julissa Villegas",ur.01312414645.39,"Clark, Kerry L.",UNF,UNF,['pub.1152238101'],['Novel Rickettsia spp. in two common overwintering North American songbirds'],"['American robins and dark-eyed juncos migrate across North America and have been found to be competent hosts for some bacterial and viral pathogens, but their contributions to arthropod-borne diseases more broadly remain poorly characterized. Here, we sampled robins and juncos in multiple sites across North America for arthropod-borne bacterial pathogens of public health significance. We identified two novel <i>Rickettsia</i> spp. in one wintering migrant per bird species related to bellii, transitional, and spotted rickettsiae fever groups. Stable isotope analyses of feathers suggested spring migration of these common songbirds could disperse these novel rickettsiae hundreds-to-thousands of kilometers to host breeding grounds. Further work is needed to characterize zoonotic potential of these rickettsiae and host reservoir competence.']"
ur.016202377225.53,"Roy, Swapnoneel",ur.014101722457.76,"Dutta, Ayan",UNF,UNF,['pub.1152489356'],['Toward a Green Blockchain: Engineering Merkle Tree and Proof of Work for Energy Optimization'],"['Blockchain-powered smart systems deployed in different industrial applications promise operational efficiencies and improved yields, while significantly mitigating cybersecurity risks. Tradeoffs between availability and security arise at implementation, however, triggered by the additional resources (e.g., memory and computation) required by blockchain-enabled hosts. This paper applies an energy-reducing algorithmic engineering technique for Merkle Tree (MT) root calculations and the Proof of Work (PoW) algorithm, two principal elements of blockchain computations, as a means to preserve the promised security benefits but with less compromise to system availability. Using pyRAPL, a python library to measure the energy consumption of a computation, we experiment with both the standard and energy-reduced implementations of both algorithms for different input sizes. Our results show that up to 98% reduction in energy consumption is possible within the blockchain’s MT construction module, with the benefits typically increasing with larger input sizes. For the PoW algorithm, our results show up to 20% reduction in energy consumption, with the benefits being lower for higher difficulty levels. The proposed energy-reducing technique is also applicable to other key elements of blockchain computations, potentially affording even “greener” blockchain-powered systems than implied by only the results obtained thus far on the MT and PoW algorithms.']"
ur.016202377225.53,"Roy, Swapnoneel",ur.013456614565.90,"Kreidl, O. Patrick",UNF,UNF,['pub.1152489356'],['Toward a Green Blockchain: Engineering Merkle Tree and Proof of Work for Energy Optimization'],"['Blockchain-powered smart systems deployed in different industrial applications promise operational efficiencies and improved yields, while significantly mitigating cybersecurity risks. Tradeoffs between availability and security arise at implementation, however, triggered by the additional resources (e.g., memory and computation) required by blockchain-enabled hosts. This paper applies an energy-reducing algorithmic engineering technique for Merkle Tree (MT) root calculations and the Proof of Work (PoW) algorithm, two principal elements of blockchain computations, as a means to preserve the promised security benefits but with less compromise to system availability. Using pyRAPL, a python library to measure the energy consumption of a computation, we experiment with both the standard and energy-reduced implementations of both algorithms for different input sizes. Our results show that up to 98% reduction in energy consumption is possible within the blockchain’s MT construction module, with the benefits typically increasing with larger input sizes. For the PoW algorithm, our results show up to 20% reduction in energy consumption, with the benefits being lower for higher difficulty levels. The proposed energy-reducing technique is also applicable to other key elements of blockchain computations, potentially affording even “greener” blockchain-powered systems than implied by only the results obtained thus far on the MT and PoW algorithms.']"
ur.014101722457.76,"Dutta, Ayan",ur.016202377225.53,"Roy, Swapnoneel",UNF,UNF,['pub.1152489356'],['Toward a Green Blockchain: Engineering Merkle Tree and Proof of Work for Energy Optimization'],"['Blockchain-powered smart systems deployed in different industrial applications promise operational efficiencies and improved yields, while significantly mitigating cybersecurity risks. Tradeoffs between availability and security arise at implementation, however, triggered by the additional resources (e.g., memory and computation) required by blockchain-enabled hosts. This paper applies an energy-reducing algorithmic engineering technique for Merkle Tree (MT) root calculations and the Proof of Work (PoW) algorithm, two principal elements of blockchain computations, as a means to preserve the promised security benefits but with less compromise to system availability. Using pyRAPL, a python library to measure the energy consumption of a computation, we experiment with both the standard and energy-reduced implementations of both algorithms for different input sizes. Our results show that up to 98% reduction in energy consumption is possible within the blockchain’s MT construction module, with the benefits typically increasing with larger input sizes. For the PoW algorithm, our results show up to 20% reduction in energy consumption, with the benefits being lower for higher difficulty levels. The proposed energy-reducing technique is also applicable to other key elements of blockchain computations, potentially affording even “greener” blockchain-powered systems than implied by only the results obtained thus far on the MT and PoW algorithms.']"
ur.014101722457.76,"Dutta, Ayan",ur.013456614565.90,"Kreidl, O. Patrick",UNF,UNF,['pub.1152489356'],['Toward a Green Blockchain: Engineering Merkle Tree and Proof of Work for Energy Optimization'],"['Blockchain-powered smart systems deployed in different industrial applications promise operational efficiencies and improved yields, while significantly mitigating cybersecurity risks. Tradeoffs between availability and security arise at implementation, however, triggered by the additional resources (e.g., memory and computation) required by blockchain-enabled hosts. This paper applies an energy-reducing algorithmic engineering technique for Merkle Tree (MT) root calculations and the Proof of Work (PoW) algorithm, two principal elements of blockchain computations, as a means to preserve the promised security benefits but with less compromise to system availability. Using pyRAPL, a python library to measure the energy consumption of a computation, we experiment with both the standard and energy-reduced implementations of both algorithms for different input sizes. Our results show that up to 98% reduction in energy consumption is possible within the blockchain’s MT construction module, with the benefits typically increasing with larger input sizes. For the PoW algorithm, our results show up to 20% reduction in energy consumption, with the benefits being lower for higher difficulty levels. The proposed energy-reducing technique is also applicable to other key elements of blockchain computations, potentially affording even “greener” blockchain-powered systems than implied by only the results obtained thus far on the MT and PoW algorithms.']"
ur.013456614565.90,"Kreidl, O. Patrick",ur.016202377225.53,"Roy, Swapnoneel",UNF,UNF,['pub.1152489356'],['Toward a Green Blockchain: Engineering Merkle Tree and Proof of Work for Energy Optimization'],"['Blockchain-powered smart systems deployed in different industrial applications promise operational efficiencies and improved yields, while significantly mitigating cybersecurity risks. Tradeoffs between availability and security arise at implementation, however, triggered by the additional resources (e.g., memory and computation) required by blockchain-enabled hosts. This paper applies an energy-reducing algorithmic engineering technique for Merkle Tree (MT) root calculations and the Proof of Work (PoW) algorithm, two principal elements of blockchain computations, as a means to preserve the promised security benefits but with less compromise to system availability. Using pyRAPL, a python library to measure the energy consumption of a computation, we experiment with both the standard and energy-reduced implementations of both algorithms for different input sizes. Our results show that up to 98% reduction in energy consumption is possible within the blockchain’s MT construction module, with the benefits typically increasing with larger input sizes. For the PoW algorithm, our results show up to 20% reduction in energy consumption, with the benefits being lower for higher difficulty levels. The proposed energy-reducing technique is also applicable to other key elements of blockchain computations, potentially affording even “greener” blockchain-powered systems than implied by only the results obtained thus far on the MT and PoW algorithms.']"
ur.013456614565.90,"Kreidl, O. Patrick",ur.014101722457.76,"Dutta, Ayan",UNF,UNF,['pub.1152489356'],['Toward a Green Blockchain: Engineering Merkle Tree and Proof of Work for Energy Optimization'],"['Blockchain-powered smart systems deployed in different industrial applications promise operational efficiencies and improved yields, while significantly mitigating cybersecurity risks. Tradeoffs between availability and security arise at implementation, however, triggered by the additional resources (e.g., memory and computation) required by blockchain-enabled hosts. This paper applies an energy-reducing algorithmic engineering technique for Merkle Tree (MT) root calculations and the Proof of Work (PoW) algorithm, two principal elements of blockchain computations, as a means to preserve the promised security benefits but with less compromise to system availability. Using pyRAPL, a python library to measure the energy consumption of a computation, we experiment with both the standard and energy-reduced implementations of both algorithms for different input sizes. Our results show that up to 98% reduction in energy consumption is possible within the blockchain’s MT construction module, with the benefits typically increasing with larger input sizes. For the PoW algorithm, our results show up to 20% reduction in energy consumption, with the benefits being lower for higher difficulty levels. The proposed energy-reducing technique is also applicable to other key elements of blockchain computations, potentially affording even “greener” blockchain-powered systems than implied by only the results obtained thus far on the MT and PoW algorithms.']"
ur.011646271766.59,"Miller, Holly Ventura",ur.010466517521.57,"Miller, J. Mitchell",UNF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.011646271766.59,"Miller, Holly Ventura",ur.016213755653.16,"Vose, Brenda",UNF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.011646271766.59,"Miller, Holly Ventura",ur.012266061425.16,"Koskinen, Stephanie M.",UNF,UF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.011646271766.59,"Miller, Holly Ventura",ur.012123446530.06,"Jossie, McKenzie L.",UNF,UF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.016213755653.16,"Vose, Brenda",ur.010466517521.57,"Miller, J. Mitchell",UNF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.016213755653.16,"Vose, Brenda",ur.011646271766.59,"Miller, Holly Ventura",UNF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.016213755653.16,"Vose, Brenda",ur.012266061425.16,"Koskinen, Stephanie M.",UNF,UF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.016213755653.16,"Vose, Brenda",ur.012123446530.06,"Jossie, McKenzie L.",UNF,UF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.012266061425.16,"Koskinen, Stephanie M.",ur.010466517521.57,"Miller, J. Mitchell",UF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.012266061425.16,"Koskinen, Stephanie M.",ur.011646271766.59,"Miller, Holly Ventura",UF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.012266061425.16,"Koskinen, Stephanie M.",ur.016213755653.16,"Vose, Brenda",UF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.012266061425.16,"Koskinen, Stephanie M.",ur.012123446530.06,"Jossie, McKenzie L.",UF,UF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.012123446530.06,"Jossie, McKenzie L.",ur.010466517521.57,"Miller, J. Mitchell",UF,UNF,"['pub.1152787034', 'pub.1153668756']","['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States', 'COVID, Crime & Criminal Justice: Affirming the Call for System Reform Research']","['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.', 'Early into the COVID-19 pandemic, Miller & Blumstein (2020) outlined a theoretical research program (TRP) oriented around themes of contagion control and containment, legal amnesty, system leniency, nonenforcement, and tele-justice. Here, two and a half years later, these lingering themes are revisited to advocate for empirical research informing criminal justice system reform. The pandemic created rare natural experiment research conditions that enable unique and potentially valuable insights on necessitated innovations that may indicate future justice practices and policies. Given the sweeping effects of the shutdown, examples are numerous ranging from staffing analyses to estimate agencies’ personnel needs to ensure that basic public safety functions can be met after early retirements and resignations from virus risk and anti-police sentiment, the use of virtual communication in various legal proceedings at arrest, incarceration, and release junctures, and, especially, the risks versus benefits of early release. In addition to better identifying who should be jailed pre-trial, prioritization of calls for service, triaging of court cases, and hygiene and sanitation issues within facilities are other important examples central to a COVID and crime TRP. Attending research could demonstrate the utility of normative operations and identify shortfalls to be addressed during anomic conditions prior to another shutdown or similar event and present, through comparison of innovative and traditional derived outcomes, system reform and improvement opportunities. By seizing upon rare data made possible by natural experimental COVID generated conditions, researchers can meaningfully investigate the ongoing applicability of justice system adaptations mandated by the pandemic in terms of effectiveness and efficiency toward the interrelated goals of evidence-based practice discovery and justice reform.']"
ur.012123446530.06,"Jossie, McKenzie L.",ur.011646271766.59,"Miller, Holly Ventura",UF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.012123446530.06,"Jossie, McKenzie L.",ur.016213755653.16,"Vose, Brenda",UF,UNF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.012123446530.06,"Jossie, McKenzie L.",ur.012266061425.16,"Koskinen, Stephanie M.",UF,UF,['pub.1152787034'],['Marijuana Enforcement since Drug Policy Reform: An Exploration of Officer Discretion in Six States'],"['Marijuana enforcement remains a major point of entry to the criminal justice system despite broad state level reforms. The knowledge base on marijuana enforcement, however, is small and predates the current national decriminalization-legalization movement and is comprised almost entirely of survey data on officer attitudes regarding drug law, policy, and strategy, generally, rather than marijuana specifically. Focusing instead on officer self-reported behavior and expressed enforcement intent, multiple US COPS Office and US Bureau of Justice Assistance grants enabled focus group interview exploration of how marijuana enforcement has been impacted since policy reform. Data were obtained from 148 sworn officers in seven agencies across six states that indicated marijuana incident outcomes are heavily dependent on officer discretion. Incident resolution was observed primarily as a function of situational dynamics (suspect demeanor, quantity of marijuana, and impaired driving concern), suspect background, agency prioritization of marijuana enforcement, and whether jurisdictions were prosecuting marijuana cases. Decision to arrest was correlated with officers’ age, years on force, gender, and ideological outlook regarding drug enforcement priorities and tended toward informal resolution in general and more so with younger and female officers. Despite national arrest statistics indicating otherwise, these findings suggest drug policy reform is affecting some discretionary leniency and nonenforcement of marijuana in favor of reported shiftwork focus on harder drugs and violence. Implications for drug enforcement and criminal justice more broadly center discussion and signal future research steps.']"
ur.011764604531.41,"Gonzalez, Alicia",ur.01226756046.26,"Ochrietor, Judith D",UNF,UNF,['pub.1153022117'],['Reduced expression of Basigin gene products in response to chronic inflammation may contribute to vision loss'],"['Chronic inflammation of the retina, like that of diabetic retinopathy, disrupts the blood-retina barrier (BRB). Disruption of the BRB increases vascular permeability and leads to vision loss. Basigin gene products, cell-adhesion molecules and members of the immunoglobulin superfamily, are expressed on endothelial cells, photoreceptor cells and Müller glial cells. Basigin variant-1 on photoreceptors interacts with Basigin variant-2 on Müller glial cells and to rod-derived cone viability factor (RdCVF) to form metabolic support mechanisms necessary for the survival of photoreceptor neurons. The goal of the current study was to determine the gene expression changes of Basigin gene products in ex\xa0vivo neonatal, adolescent, and adult retina when exposed to an inflammatory insult in acute and chronic phases. Retinas extracted from mice at postnatal day (P) 7, 30, and 180 were incubated with either phosphate-buffered saline (PBS), as a control, or lipopolysaccharide (LPS), an endotoxin, for 3, 6, 12, or 24\xa0h. RNA was then extracted and Basigin gene products were quantified by qPCR. Analyses indicate both gene products are influenced by LPS exposure in a time and age dependent manner. Specifically, P180 retinas exposed to LPS showed significant decreases in both Basigin gene products, suggesting older retinas may be susceptible to chronic inflammation and subsequent vision loss.']"
ur.0675707375.80,"PARRY, Selina M.",ur.0730275122.15,"OSTERBRINK, Jürgen",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0675707375.80,"PARRY, Selina M.",ur.016105651565.67,"EGGMANN, Sabrina",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0675707375.80,"PARRY, Selina M.",ur.01016254223.10,"SCHALLER, Stefan J.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0675707375.80,"PARRY, Selina M.",ur.01050046744.47,"NYDAHL, Peter",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.0675707375.80,"PARRY, Selina M.",ur.01263513533.79,"NEEDHAM, Dale M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.016105651565.67,"EGGMANN, Sabrina",ur.0675707375.80,"PARRY, Selina M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.016105651565.67,"EGGMANN, Sabrina",ur.0730275122.15,"OSTERBRINK, Jürgen",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.016105651565.67,"EGGMANN, Sabrina",ur.01016254223.10,"SCHALLER, Stefan J.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.016105651565.67,"EGGMANN, Sabrina",ur.01050046744.47,"NYDAHL, Peter",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.016105651565.67,"EGGMANN, Sabrina",ur.01263513533.79,"NEEDHAM, Dale M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01016254223.10,"SCHALLER, Stefan J.",ur.0675707375.80,"PARRY, Selina M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01016254223.10,"SCHALLER, Stefan J.",ur.0730275122.15,"OSTERBRINK, Jürgen",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01016254223.10,"SCHALLER, Stefan J.",ur.016105651565.67,"EGGMANN, Sabrina",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01016254223.10,"SCHALLER, Stefan J.",ur.01050046744.47,"NYDAHL, Peter",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01016254223.10,"SCHALLER, Stefan J.",ur.01263513533.79,"NEEDHAM, Dale M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01050046744.47,"NYDAHL, Peter",ur.0675707375.80,"PARRY, Selina M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01050046744.47,"NYDAHL, Peter",ur.0730275122.15,"OSTERBRINK, Jürgen",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01050046744.47,"NYDAHL, Peter",ur.016105651565.67,"EGGMANN, Sabrina",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01050046744.47,"NYDAHL, Peter",ur.01016254223.10,"SCHALLER, Stefan J.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01050046744.47,"NYDAHL, Peter",ur.01263513533.79,"NEEDHAM, Dale M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01263513533.79,"NEEDHAM, Dale M.",ur.0675707375.80,"PARRY, Selina M.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01263513533.79,"NEEDHAM, Dale M.",ur.0730275122.15,"OSTERBRINK, Jürgen",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01263513533.79,"NEEDHAM, Dale M.",ur.016105651565.67,"EGGMANN, Sabrina",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01263513533.79,"NEEDHAM, Dale M.",ur.01016254223.10,"SCHALLER, Stefan J.",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.01263513533.79,"NEEDHAM, Dale M.",ur.01050046744.47,"NYDAHL, Peter",UNF,UNF,['pub.1153677024'],['Infection control practices and device management when mobilizing critically ill patients'],[nan]
ur.015444027474.43,"Villamor, Monique",ur.0741150741.82,"Nicholson, Jody",UNF,UNF,['pub.1153822461'],['WHAT MOTIVATES PARTICIPATION IN ALZHEIMER’S PREVENTION RESEARCH: A MIXED-METHODS STUDY'],"['Abstract\r\n        <p>6.5 million Americans aged 65 and older are currently living with Alzheimer’s disease or related dementias (ADRD; Alzheimer’s Association, 2022). By 2060, it is estimated that this number will increase to 13.9 million (Matthews et al., 2018). Therefore, it is imperative to gain insight into participants’ personal motivations and expectations of research to advance community participation. The Preventing Alzheimer’s with Cognitive Training (PACT) study is a National Institute of Health, National Institute on Aging-funded, multi-site clinical trial examining the prevention of mild cognitive impairment and ADRD through computer-based cognitive training. Across 5 locations, data were collected from 2,360 cognitively normal participants (M=73.03 years, range=65–97, SD=5.04). The current project explores individuals’ motivations and expectations of cognitive training (CT) utilizing a mixed-method approach by coding qualitative open-ended questions about motivation to participate and comparing how motivational themes aligned with expectations about CT from the Expectations Assessment Scale (EAS; Rabipour et al., 2018). Six themes for participant motivation emerged: direct experience with the disease (26.9%), concern about brain health and aging (23.9%), general personal interest (17.9%), general interest in research (20.1%), referral to the study (5.8%), and altruism (5.4%). After completing the initial training session, motivation themes did not differentiate satisfaction with (p=.06) or perceived success of (p=.11) the CT program. Understanding participants’ motivations can further expand and optimize recruitment and retention strategies in AD prevention research. Future research will focus on how these themes influence adherence and retention and relate to participant demographic characteristics (i.e., education, gender, race, and ethnicity).</p>']"
ur.01273574321.58,"Mainous, Arch G",ur.01302000265.19,"Hamadi, Hanadi Y",UF,UNF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.01273574321.58,"Mainous, Arch G",ur.016254065343.32,"Xie, Zhigang",UF,UNF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
ur.01273574321.58,"Mainous, Arch G",ur.012241605737.25,"Hong, Young-Rock",UF,UF,['pub.1153940974'],['Association of dual COVID-19 and seasonal influenza vaccination with COVID-19 infection and disease severity'],"['The clinical guideline states that COVID-19 vaccination can be administered concurrently with Influenza (flu) vaccination (dual vaccination). Using data from the 2021 National Health Interview Survey, we conducted descriptive analysis and multivariate logistic regressions to examine the association between dual vaccination status and self-reported COVID-19 infection and severity. Among 21,387 (weighted 185,251,310) U.S. adults, about 22% did not receive either the flu or COVID-19 vaccine, 6.0% received the flu vaccine only, 29.1% received the COVID-19 vaccine only, and 42.5% received both vaccines. In the multivariate analysis, individuals with dual vaccination (OR, 0.65, 95% CI, 0.56-0.75) and COVID-19 vaccine only (OR, 0.71, 95% CI, 0.61-0.82) were significantly less likely to report COVID-19 infection when compared with those unvaccinated. There was no significant difference in self-reported COVID-19 symptom severity by vaccination status. The results suggest that dual vaccination may be an effective strategy to reduce the contagious respiratory disease burden.']"
